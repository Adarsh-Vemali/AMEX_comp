{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMEX_productionised_code",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adarsh-Vemali/AMEX_comp/blob/main/AMEX_productionised_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIJ44SzMOG6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee3b703-b4fb-44ba-9601-361526198608"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4av8BkuLOcXt"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk7XzKqNOlr6"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Amex_comp/Training_Data.csv')\n",
        "dftest = pd.read_csv('/content/drive/My Drive/Amex_comp/Evaluation_Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHqGyhEzO4QD"
      },
      "source": [
        "df = df[0:19595]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI-c8wet5Pk_"
      },
      "source": [
        "df[\"acq_sub_chn\"].fillna('Unknown', inplace = True) \n",
        "df[\"acq_type_grp\"].fillna('Unknown', inplace = True) \n",
        "dftest[\"acq_sub_chn\"].fillna('Unknown', inplace = True) \n",
        "dftest[\"acq_type_grp\"].fillna('Unknown', inplace = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIRyAyIkA8jG"
      },
      "source": [
        "l=list(df.columns)\n",
        "for col in l:\n",
        "  df[col].fillna(0, inplace = True)\n",
        "\n",
        "l=list(dftest.columns)\n",
        "for col in l:\n",
        "  dftest[col].fillna(0, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksMxxBNKV2Kk"
      },
      "source": [
        "col='acq_sub_chn'\n",
        "df[col] = df[col].astype('category')\n",
        "dftest[col] = dftest[col].astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p53Bm0aWWkJo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "f8d3311c-3e54-4312-dc42-644a3cede87a"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder() \n",
        "df['fee_type_grp'] = labelencoder.fit_transform(df['fee_type_grp'])\n",
        "dftest['fee_type_grp'] = labelencoder.fit_transform(dftest['fee_type_grp'])\n",
        "labelencoder = LabelEncoder() \n",
        "df['acq_type_grp'] = labelencoder.fit_transform(df['acq_type_grp'])\n",
        "dftest['acq_type_grp'] = labelencoder.fit_transform(dftest['acq_type_grp'])\n",
        "labelencoder = LabelEncoder() \n",
        "df['acq_sub_chn'] = labelencoder.fit_transform(df['acq_sub_chn'])\n",
        "dftest['acq_sub_chn'] = labelencoder.fit_transform(dftest['acq_sub_chn'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cust_id</th>\n",
              "      <th>profitable_flag</th>\n",
              "      <th>count_accts</th>\n",
              "      <th>cm_age</th>\n",
              "      <th>flag_top_ed_spender</th>\n",
              "      <th>flag_cust_fee_paid_6m</th>\n",
              "      <th>pre6m_cust_spend</th>\n",
              "      <th>pre6m_cust_roc_cnt</th>\n",
              "      <th>pre6m_cust_non_disc_amt</th>\n",
              "      <th>pre6m_cust_non_disc_cnt</th>\n",
              "      <th>pre6m_cust_disc_amt</th>\n",
              "      <th>pre6m_cust_disc_cnt</th>\n",
              "      <th>pre6m_cust_outbound_amt</th>\n",
              "      <th>pre6m_cust_online_amt</th>\n",
              "      <th>pre6m_cust_online_cnt</th>\n",
              "      <th>pre6m_cust_travel_amt</th>\n",
              "      <th>pre6m_cust_travel_cnt</th>\n",
              "      <th>pre6m_cust_retail_amt</th>\n",
              "      <th>pre6m_cust_retail_cnt</th>\n",
              "      <th>pre6m_cust_myca_active</th>\n",
              "      <th>pre6m_cust_mob_logins</th>\n",
              "      <th>pre6m_total_mc_trs</th>\n",
              "      <th>acq_sub_chn</th>\n",
              "      <th>acq_type_grp</th>\n",
              "      <th>fee_type_grp</th>\n",
              "      <th>Cust_tenure</th>\n",
              "      <th>pre6m_spend_active_ind</th>\n",
              "      <th>highly_utilized_ind</th>\n",
              "      <th>min_pay_ind</th>\n",
              "      <th>paid_in_full_ind</th>\n",
              "      <th>sum_total_line_amt</th>\n",
              "      <th>direct_debit_ind</th>\n",
              "      <th>referrals</th>\n",
              "      <th>spillover</th>\n",
              "      <th>self_accts</th>\n",
              "      <th>Customer Low Quality indicator</th>\n",
              "      <th>cdss_most_rcnt_prob</th>\n",
              "      <th>cust_max_credit_12m_amt</th>\n",
              "      <th>cust_max_remit_12m_amt</th>\n",
              "      <th>cbr_3_score</th>\n",
              "      <th>cnsumr_chrg_actv_cust_cnt</th>\n",
              "      <th>cnsumr_chrg_avg_credit_12m_amt</th>\n",
              "      <th>cnsumr_lend_actv_cust_cnt</th>\n",
              "      <th>cnsumr_lend_tot_util_ratio</th>\n",
              "      <th>sow_revol_avg_paydown_pct</th>\n",
              "      <th>sow_tot_annual_ext_pmt_amt</th>\n",
              "      <th>sow_tot_revol_bal_amt</th>\n",
              "      <th>sow_tot_revol_cnt</th>\n",
              "      <th>sow_tot_trans_bal_amt</th>\n",
              "      <th>sow_tot_trans_cnt</th>\n",
              "      <th>pos_neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>555488</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3696.00</td>\n",
              "      <td>260.0</td>\n",
              "      <td>2236.00</td>\n",
              "      <td>202.0</td>\n",
              "      <td>1461.00</td>\n",
              "      <td>58.0</td>\n",
              "      <td>149.00</td>\n",
              "      <td>253.00</td>\n",
              "      <td>36.0</td>\n",
              "      <td>70.00</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2975.00</td>\n",
              "      <td>224.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6000.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>859.00</td>\n",
              "      <td>792.00</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>550513</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>89</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4440.00</td>\n",
              "      <td>676.0</td>\n",
              "      <td>346.00</td>\n",
              "      <td>158.0</td>\n",
              "      <td>4095.00</td>\n",
              "      <td>518.0</td>\n",
              "      <td>57.00</td>\n",
              "      <td>2166.00</td>\n",
              "      <td>56.0</td>\n",
              "      <td>2690.00</td>\n",
              "      <td>272.0</td>\n",
              "      <td>580.00</td>\n",
              "      <td>184.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>213.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39600.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>814.00</td>\n",
              "      <td>800.00</td>\n",
              "      <td>947.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.04</td>\n",
              "      <td>5269.00</td>\n",
              "      <td>4044.00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>-</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>551684</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7879.00</td>\n",
              "      <td>382.0</td>\n",
              "      <td>1848.00</td>\n",
              "      <td>184.0</td>\n",
              "      <td>6032.00</td>\n",
              "      <td>198.0</td>\n",
              "      <td>2852.00</td>\n",
              "      <td>2658.00</td>\n",
              "      <td>304.0</td>\n",
              "      <td>4232.00</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1958.00</td>\n",
              "      <td>198.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>92400.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2478.00</td>\n",
              "      <td>2467.00</td>\n",
              "      <td>1357.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12783.00</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>409.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>550421</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>86</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11988.00</td>\n",
              "      <td>1296.0</td>\n",
              "      <td>4431.00</td>\n",
              "      <td>220.0</td>\n",
              "      <td>7558.00</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>997.00</td>\n",
              "      <td>5254.00</td>\n",
              "      <td>340.0</td>\n",
              "      <td>1504.00</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5853.00</td>\n",
              "      <td>248.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>969.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>25</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38667.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>2620.00</td>\n",
              "      <td>2523.00</td>\n",
              "      <td>1239.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>883.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>563825</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5712.00</td>\n",
              "      <td>214.0</td>\n",
              "      <td>1231.00</td>\n",
              "      <td>86.0</td>\n",
              "      <td>4481.00</td>\n",
              "      <td>128.0</td>\n",
              "      <td>798.00</td>\n",
              "      <td>4154.00</td>\n",
              "      <td>216.0</td>\n",
              "      <td>2751.00</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1598.00</td>\n",
              "      <td>142.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50800.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1690.00</td>\n",
              "      <td>1690.00</td>\n",
              "      <td>1255.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cust_id  profitable_flag  ...  sow_tot_trans_cnt  pos_neg\n",
              "0   555488                1  ...                0.0        1\n",
              "1   550513                1  ...                2.0        1\n",
              "2   551684                1  ...                2.0        1\n",
              "3   550421                1  ...                0.0        1\n",
              "4   563825                1  ...                0.0        1\n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDgp96aaYvPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "2ac4e495-aa35-4713-96bc-85aad0d5e514"
      },
      "source": [
        "from string import ascii_letters\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"white\")\n",
        "\n",
        "corr = df.corr()['profitable_flag']\n",
        "print(corr)\n",
        "plt.plot(corr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cust_id                           0.002040\n",
            "profitable_flag                   1.000000\n",
            "count_accts                      -0.018371\n",
            "cm_age                           -0.009669\n",
            "flag_top_ed_spender               0.045787\n",
            "flag_cust_fee_paid_6m            -0.014153\n",
            "pre6m_cust_roc_cnt                0.096348\n",
            "pre6m_cust_non_disc_cnt           0.077625\n",
            "pre6m_cust_disc_cnt               0.087323\n",
            "pre6m_cust_online_cnt             0.043151\n",
            "pre6m_cust_travel_cnt             0.112781\n",
            "pre6m_cust_retail_cnt             0.045425\n",
            "pre6m_cust_myca_active            0.017608\n",
            "pre6m_cust_mob_logins            -0.005772\n",
            "pre6m_total_mc_trs                0.078756\n",
            "acq_sub_chn                       0.005866\n",
            "acq_type_grp                     -0.009071\n",
            "fee_type_grp                      0.036341\n",
            "Cust_tenure                       0.027501\n",
            "pre6m_spend_active_ind            0.043024\n",
            "highly_utilized_ind               0.002430\n",
            "min_pay_ind                      -0.026498\n",
            "paid_in_full_ind                  0.016358\n",
            "direct_debit_ind                  0.031757\n",
            "referrals                        -0.024680\n",
            "spillover                        -0.037482\n",
            "self_accts                       -0.032718\n",
            "Customer Low Quality indicator   -0.027741\n",
            "cdss_most_rcnt_prob              -0.032234\n",
            "cbr_3_score                       0.031733\n",
            "cnsumr_chrg_actv_cust_cnt         0.017287\n",
            "cnsumr_lend_actv_cust_cnt        -0.032286\n",
            "cnsumr_lend_tot_util_ratio        0.013452\n",
            "sow_revol_avg_paydown_pct        -0.017190\n",
            "sow_tot_revol_cnt                -0.014663\n",
            "sow_tot_trans_cnt                -0.003677\n",
            "pos_neg                           0.009734\n",
            "Name: profitable_flag, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f73f1b51cf8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sua4uCPkpbXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea115d1-4608-47e6-f336-614831285071"
      },
      "source": [
        "for i in range(len(df)):\n",
        "  for col in df.columns:\n",
        "    if str(df[col][i]).strip()=='-':\n",
        "      df[col][i]=0 \n",
        "columns_for_all = list(df.columns)\n",
        "for i in range(len(dftest)):\n",
        "  for col in dftest.columns:\n",
        "    if str(dftest[col][i]).strip()=='-':\n",
        "      dftest[col][i]=0 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "_p8Ms9kIDUiK",
        "outputId": "76c6e9f6-7442-4b07-b5e5-f8e8aa517821"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cust_id</th>\n",
              "      <th>profitable_flag</th>\n",
              "      <th>count_accts</th>\n",
              "      <th>cm_age</th>\n",
              "      <th>flag_top_ed_spender</th>\n",
              "      <th>flag_cust_fee_paid_6m</th>\n",
              "      <th>pre6m_cust_spend</th>\n",
              "      <th>pre6m_cust_roc_cnt</th>\n",
              "      <th>pre6m_cust_non_disc_amt</th>\n",
              "      <th>pre6m_cust_non_disc_cnt</th>\n",
              "      <th>pre6m_cust_disc_amt</th>\n",
              "      <th>pre6m_cust_disc_cnt</th>\n",
              "      <th>pre6m_cust_outbound_amt</th>\n",
              "      <th>pre6m_cust_online_amt</th>\n",
              "      <th>pre6m_cust_online_cnt</th>\n",
              "      <th>pre6m_cust_travel_amt</th>\n",
              "      <th>pre6m_cust_travel_cnt</th>\n",
              "      <th>pre6m_cust_retail_amt</th>\n",
              "      <th>pre6m_cust_retail_cnt</th>\n",
              "      <th>pre6m_cust_myca_active</th>\n",
              "      <th>pre6m_cust_mob_logins</th>\n",
              "      <th>pre6m_total_mc_trs</th>\n",
              "      <th>acq_sub_chn</th>\n",
              "      <th>acq_type_grp</th>\n",
              "      <th>fee_type_grp</th>\n",
              "      <th>Cust_tenure</th>\n",
              "      <th>pre6m_spend_active_ind</th>\n",
              "      <th>highly_utilized_ind</th>\n",
              "      <th>min_pay_ind</th>\n",
              "      <th>paid_in_full_ind</th>\n",
              "      <th>sum_total_line_amt</th>\n",
              "      <th>direct_debit_ind</th>\n",
              "      <th>referrals</th>\n",
              "      <th>spillover</th>\n",
              "      <th>self_accts</th>\n",
              "      <th>Customer Low Quality indicator</th>\n",
              "      <th>cdss_most_rcnt_prob</th>\n",
              "      <th>cust_max_credit_12m_amt</th>\n",
              "      <th>cust_max_remit_12m_amt</th>\n",
              "      <th>cbr_3_score</th>\n",
              "      <th>cnsumr_chrg_actv_cust_cnt</th>\n",
              "      <th>cnsumr_chrg_avg_credit_12m_amt</th>\n",
              "      <th>cnsumr_lend_actv_cust_cnt</th>\n",
              "      <th>cnsumr_lend_tot_util_ratio</th>\n",
              "      <th>sow_revol_avg_paydown_pct</th>\n",
              "      <th>sow_tot_annual_ext_pmt_amt</th>\n",
              "      <th>sow_tot_revol_bal_amt</th>\n",
              "      <th>sow_tot_revol_cnt</th>\n",
              "      <th>sow_tot_trans_bal_amt</th>\n",
              "      <th>sow_tot_trans_cnt</th>\n",
              "      <th>pos_neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>555488</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3696.00</td>\n",
              "      <td>260.0</td>\n",
              "      <td>2236.00</td>\n",
              "      <td>202.0</td>\n",
              "      <td>1461.00</td>\n",
              "      <td>58.0</td>\n",
              "      <td>149.00</td>\n",
              "      <td>253.00</td>\n",
              "      <td>36.0</td>\n",
              "      <td>70.00</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2975.00</td>\n",
              "      <td>224.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6000.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>859.00</td>\n",
              "      <td>792.00</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>550513</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>89</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4440.00</td>\n",
              "      <td>676.0</td>\n",
              "      <td>346.00</td>\n",
              "      <td>158.0</td>\n",
              "      <td>4095.00</td>\n",
              "      <td>518.0</td>\n",
              "      <td>57.00</td>\n",
              "      <td>2166.00</td>\n",
              "      <td>56.0</td>\n",
              "      <td>2690.00</td>\n",
              "      <td>272.0</td>\n",
              "      <td>580.00</td>\n",
              "      <td>184.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>213.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39600.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>814.00</td>\n",
              "      <td>800.00</td>\n",
              "      <td>947.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.04</td>\n",
              "      <td>5269.00</td>\n",
              "      <td>4044.00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>551684</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7879.00</td>\n",
              "      <td>382.0</td>\n",
              "      <td>1848.00</td>\n",
              "      <td>184.0</td>\n",
              "      <td>6032.00</td>\n",
              "      <td>198.0</td>\n",
              "      <td>2852.00</td>\n",
              "      <td>2658.00</td>\n",
              "      <td>304.0</td>\n",
              "      <td>4232.00</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1958.00</td>\n",
              "      <td>198.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>92400.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2478.00</td>\n",
              "      <td>2467.00</td>\n",
              "      <td>1357.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12783.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>409.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>550421</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>86</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11988.00</td>\n",
              "      <td>1296.0</td>\n",
              "      <td>4431.00</td>\n",
              "      <td>220.0</td>\n",
              "      <td>7558.00</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>997.00</td>\n",
              "      <td>5254.00</td>\n",
              "      <td>340.0</td>\n",
              "      <td>1504.00</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5853.00</td>\n",
              "      <td>248.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>969.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>25</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38667.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>2620.00</td>\n",
              "      <td>2523.00</td>\n",
              "      <td>1239.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>883.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>563825</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5712.00</td>\n",
              "      <td>214.0</td>\n",
              "      <td>1231.00</td>\n",
              "      <td>86.0</td>\n",
              "      <td>4481.00</td>\n",
              "      <td>128.0</td>\n",
              "      <td>798.00</td>\n",
              "      <td>4154.00</td>\n",
              "      <td>216.0</td>\n",
              "      <td>2751.00</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1598.00</td>\n",
              "      <td>142.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50800.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1690.00</td>\n",
              "      <td>1690.00</td>\n",
              "      <td>1255.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cust_id  profitable_flag  ...  sow_tot_trans_cnt  pos_neg\n",
              "0   555488                1  ...                0.0        1\n",
              "1   550513                1  ...                2.0        1\n",
              "2   551684                1  ...                2.0        1\n",
              "3   550421                1  ...                0.0        1\n",
              "4   563825                1  ...                0.0        1\n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0szO1IbcVdPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c13bf8c-813f-4a3b-ea0e-7d761eae75dc"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "y = df.iloc[:, 1]\n",
        "X = sklearn.preprocessing.MinMaxScaler().fit_transform(df)\n",
        "df1 = sklearn.preprocessing.MinMaxScaler().fit_transform(df)\n",
        "df1 = pd.DataFrame(df1)\n",
        "X = X[:, 2:]\n",
        "\n",
        "bestfeatures = SelectKBest(score_func=chi2, k=49)\n",
        "fit = bestfeatures.fit(X,y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(df.iloc[:,2:].columns)\n",
        "\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "featureScores.columns = ['Feature column','Score']  \n",
        "pdf=(featureScores.nlargest(49,'Score'))\n",
        "pdf = pdf.reset_index()\n",
        "pdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Feature column</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>pre6m_cust_travel_cnt</td>\n",
              "      <td>22.629876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>flag_top_ed_spender</td>\n",
              "      <td>19.324681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>Customer Low Quality indicator</td>\n",
              "      <td>14.949132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>min_pay_ind</td>\n",
              "      <td>13.462586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>pre6m_cust_non_disc_cnt</td>\n",
              "      <td>12.236993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>35</td>\n",
              "      <td>cust_max_credit_12m_amt</td>\n",
              "      <td>10.866517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>36</td>\n",
              "      <td>cust_max_remit_12m_amt</td>\n",
              "      <td>10.757942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>29</td>\n",
              "      <td>direct_debit_ind</td>\n",
              "      <td>10.444271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>39</td>\n",
              "      <td>cnsumr_chrg_avg_credit_12m_amt</td>\n",
              "      <td>8.132988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6</td>\n",
              "      <td>pre6m_cust_non_disc_amt</td>\n",
              "      <td>5.986160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>22</td>\n",
              "      <td>fee_type_grp</td>\n",
              "      <td>5.607613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>34</td>\n",
              "      <td>cdss_most_rcnt_prob</td>\n",
              "      <td>5.179966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>32</td>\n",
              "      <td>self_accts</td>\n",
              "      <td>4.781328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>28</td>\n",
              "      <td>sum_total_line_amt</td>\n",
              "      <td>3.162304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>flag_cust_fee_paid_6m</td>\n",
              "      <td>2.974905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4</td>\n",
              "      <td>pre6m_cust_spend</td>\n",
              "      <td>2.872452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>24</td>\n",
              "      <td>pre6m_spend_active_ind</td>\n",
              "      <td>2.837710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>38</td>\n",
              "      <td>cnsumr_chrg_actv_cust_cnt</td>\n",
              "      <td>2.574859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>42</td>\n",
              "      <td>sow_revol_avg_paydown_pct</td>\n",
              "      <td>2.438778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>pre6m_cust_roc_cnt</td>\n",
              "      <td>2.328434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>31</td>\n",
              "      <td>spillover</td>\n",
              "      <td>2.188982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>27</td>\n",
              "      <td>paid_in_full_ind</td>\n",
              "      <td>2.183583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>9</td>\n",
              "      <td>pre6m_cust_disc_cnt</td>\n",
              "      <td>2.148104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>pre6m_cust_disc_amt</td>\n",
              "      <td>2.016353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>15</td>\n",
              "      <td>pre6m_cust_retail_amt</td>\n",
              "      <td>1.795288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>23</td>\n",
              "      <td>Cust_tenure</td>\n",
              "      <td>1.697179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>count_accts</td>\n",
              "      <td>1.545433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>pre6m_cust_outbound_amt</td>\n",
              "      <td>1.245051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>45</td>\n",
              "      <td>sow_tot_revol_cnt</td>\n",
              "      <td>1.139936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>40</td>\n",
              "      <td>cnsumr_lend_actv_cust_cnt</td>\n",
              "      <td>1.075274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>37</td>\n",
              "      <td>cbr_3_score</td>\n",
              "      <td>0.883580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>12</td>\n",
              "      <td>pre6m_cust_online_cnt</td>\n",
              "      <td>0.871769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>17</td>\n",
              "      <td>pre6m_cust_myca_active</td>\n",
              "      <td>0.711717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>sow_tot_annual_ext_pmt_amt</td>\n",
              "      <td>0.665804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>16</td>\n",
              "      <td>pre6m_cust_retail_cnt</td>\n",
              "      <td>0.637171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>13</td>\n",
              "      <td>pre6m_cust_travel_amt</td>\n",
              "      <td>0.593243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>30</td>\n",
              "      <td>referrals</td>\n",
              "      <td>0.502457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>11</td>\n",
              "      <td>pre6m_cust_online_amt</td>\n",
              "      <td>0.379710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>21</td>\n",
              "      <td>acq_type_grp</td>\n",
              "      <td>0.270584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>cm_age</td>\n",
              "      <td>0.175412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>20</td>\n",
              "      <td>acq_sub_chn</td>\n",
              "      <td>0.150234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>25</td>\n",
              "      <td>highly_utilized_ind</td>\n",
              "      <td>0.107095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>19</td>\n",
              "      <td>pre6m_total_mc_trs</td>\n",
              "      <td>0.100204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>18</td>\n",
              "      <td>pre6m_cust_mob_logins</td>\n",
              "      <td>0.053920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>47</td>\n",
              "      <td>sow_tot_trans_cnt</td>\n",
              "      <td>0.036739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>46</td>\n",
              "      <td>sow_tot_trans_bal_amt</td>\n",
              "      <td>0.016804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>44</td>\n",
              "      <td>sow_tot_revol_bal_amt</td>\n",
              "      <td>0.009986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>41</td>\n",
              "      <td>cnsumr_lend_tot_util_ratio</td>\n",
              "      <td>0.002863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>48</td>\n",
              "      <td>pos_neg</td>\n",
              "      <td>0.002369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index                  Feature column      Score\n",
              "0      14           pre6m_cust_travel_cnt  22.629876\n",
              "1       2             flag_top_ed_spender  19.324681\n",
              "2      33  Customer Low Quality indicator  14.949132\n",
              "3      26                     min_pay_ind  13.462586\n",
              "4       7         pre6m_cust_non_disc_cnt  12.236993\n",
              "5      35         cust_max_credit_12m_amt  10.866517\n",
              "6      36          cust_max_remit_12m_amt  10.757942\n",
              "7      29                direct_debit_ind  10.444271\n",
              "8      39  cnsumr_chrg_avg_credit_12m_amt   8.132988\n",
              "9       6         pre6m_cust_non_disc_amt   5.986160\n",
              "10     22                    fee_type_grp   5.607613\n",
              "11     34             cdss_most_rcnt_prob   5.179966\n",
              "12     32                      self_accts   4.781328\n",
              "13     28              sum_total_line_amt   3.162304\n",
              "14      3           flag_cust_fee_paid_6m   2.974905\n",
              "15      4                pre6m_cust_spend   2.872452\n",
              "16     24          pre6m_spend_active_ind   2.837710\n",
              "17     38       cnsumr_chrg_actv_cust_cnt   2.574859\n",
              "18     42       sow_revol_avg_paydown_pct   2.438778\n",
              "19      5              pre6m_cust_roc_cnt   2.328434\n",
              "20     31                       spillover   2.188982\n",
              "21     27                paid_in_full_ind   2.183583\n",
              "22      9             pre6m_cust_disc_cnt   2.148104\n",
              "23      8             pre6m_cust_disc_amt   2.016353\n",
              "24     15           pre6m_cust_retail_amt   1.795288\n",
              "25     23                     Cust_tenure   1.697179\n",
              "26      0                     count_accts   1.545433\n",
              "27     10         pre6m_cust_outbound_amt   1.245051\n",
              "28     45               sow_tot_revol_cnt   1.139936\n",
              "29     40       cnsumr_lend_actv_cust_cnt   1.075274\n",
              "30     37                     cbr_3_score   0.883580\n",
              "31     12           pre6m_cust_online_cnt   0.871769\n",
              "32     17          pre6m_cust_myca_active   0.711717\n",
              "33     43      sow_tot_annual_ext_pmt_amt   0.665804\n",
              "34     16           pre6m_cust_retail_cnt   0.637171\n",
              "35     13           pre6m_cust_travel_amt   0.593243\n",
              "36     30                       referrals   0.502457\n",
              "37     11           pre6m_cust_online_amt   0.379710\n",
              "38     21                    acq_type_grp   0.270584\n",
              "39      1                          cm_age   0.175412\n",
              "40     20                     acq_sub_chn   0.150234\n",
              "41     25             highly_utilized_ind   0.107095\n",
              "42     19              pre6m_total_mc_trs   0.100204\n",
              "43     18           pre6m_cust_mob_logins   0.053920\n",
              "44     47               sow_tot_trans_cnt   0.036739\n",
              "45     46           sow_tot_trans_bal_amt   0.016804\n",
              "46     44           sow_tot_revol_bal_amt   0.009986\n",
              "47     41      cnsumr_lend_tot_util_ratio   0.002863\n",
              "48     48                         pos_neg   0.002369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub1bupL5zvSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff98896-0442-4975-ed6c-9d17598d9070"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt \n",
        "plt.rc(\"font\", size=14)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "sns.set(style=\"white\")\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "RANDOM_SEED=42\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn_pandas import DataFrameMapper"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYQVNaBt8Duo"
      },
      "source": [
        "mapper = DataFrameMapper([(df.columns[2:], StandardScaler())])\n",
        "scaled_features = mapper.fit_transform(df.copy(), 4)\n",
        "scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns[2:])\n",
        "scaled_features_df['cust_id']=df['cust_id']\n",
        "scaled_features_df['profitable_flag']=df['profitable_flag']\n",
        "df = scaled_features_df\n",
        "\n",
        "mapper = DataFrameMapper([(dftest.columns[2:], StandardScaler())])\n",
        "scaled_features_dftest = mapper.fit_transform(dftest.copy(), 4)\n",
        "scaled_features_dftest = pd.DataFrame(scaled_features_dftest, index=dftest.index, columns=dftest.columns[2:])\n",
        "#scaled_features_dftest['cust_id']=df['cust_id']\n",
        "#scaled_features_dftest['profitable_flag']=dftest['profitable_flag']\n",
        "dftest = scaled_features_dftest\n",
        "#dftest.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRKYeWst_k93"
      },
      "source": [
        "columns_for_all=df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "SwXRa8x8FC6M",
        "outputId": "062cae12-8e35-4702-f699-11722aa52ce2"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count_accts</th>\n",
              "      <th>cm_age</th>\n",
              "      <th>flag_top_ed_spender</th>\n",
              "      <th>flag_cust_fee_paid_6m</th>\n",
              "      <th>pre6m_cust_spend</th>\n",
              "      <th>pre6m_cust_roc_cnt</th>\n",
              "      <th>pre6m_cust_non_disc_amt</th>\n",
              "      <th>pre6m_cust_non_disc_cnt</th>\n",
              "      <th>pre6m_cust_disc_amt</th>\n",
              "      <th>pre6m_cust_disc_cnt</th>\n",
              "      <th>pre6m_cust_outbound_amt</th>\n",
              "      <th>pre6m_cust_online_amt</th>\n",
              "      <th>pre6m_cust_online_cnt</th>\n",
              "      <th>pre6m_cust_travel_amt</th>\n",
              "      <th>pre6m_cust_travel_cnt</th>\n",
              "      <th>pre6m_cust_retail_amt</th>\n",
              "      <th>pre6m_cust_retail_cnt</th>\n",
              "      <th>pre6m_cust_myca_active</th>\n",
              "      <th>pre6m_cust_mob_logins</th>\n",
              "      <th>pre6m_total_mc_trs</th>\n",
              "      <th>acq_sub_chn</th>\n",
              "      <th>acq_type_grp</th>\n",
              "      <th>fee_type_grp</th>\n",
              "      <th>Cust_tenure</th>\n",
              "      <th>pre6m_spend_active_ind</th>\n",
              "      <th>highly_utilized_ind</th>\n",
              "      <th>min_pay_ind</th>\n",
              "      <th>paid_in_full_ind</th>\n",
              "      <th>sum_total_line_amt</th>\n",
              "      <th>direct_debit_ind</th>\n",
              "      <th>referrals</th>\n",
              "      <th>spillover</th>\n",
              "      <th>self_accts</th>\n",
              "      <th>Customer Low Quality indicator</th>\n",
              "      <th>cdss_most_rcnt_prob</th>\n",
              "      <th>cust_max_credit_12m_amt</th>\n",
              "      <th>cust_max_remit_12m_amt</th>\n",
              "      <th>cbr_3_score</th>\n",
              "      <th>cnsumr_chrg_actv_cust_cnt</th>\n",
              "      <th>cnsumr_chrg_avg_credit_12m_amt</th>\n",
              "      <th>cnsumr_lend_actv_cust_cnt</th>\n",
              "      <th>cnsumr_lend_tot_util_ratio</th>\n",
              "      <th>sow_revol_avg_paydown_pct</th>\n",
              "      <th>sow_tot_annual_ext_pmt_amt</th>\n",
              "      <th>sow_tot_revol_bal_amt</th>\n",
              "      <th>sow_tot_revol_cnt</th>\n",
              "      <th>sow_tot_trans_bal_amt</th>\n",
              "      <th>sow_tot_trans_cnt</th>\n",
              "      <th>pos_neg</th>\n",
              "      <th>cust_id</th>\n",
              "      <th>profitable_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>4.081611</td>\n",
              "      <td>-1.061005</td>\n",
              "      <td>-0.565190</td>\n",
              "      <td>-0.184002</td>\n",
              "      <td>-0.354711</td>\n",
              "      <td>0.556263</td>\n",
              "      <td>0.435253</td>\n",
              "      <td>-0.353575</td>\n",
              "      <td>-0.532819</td>\n",
              "      <td>-0.323505</td>\n",
              "      <td>-0.409053</td>\n",
              "      <td>-0.276921</td>\n",
              "      <td>-0.535388</td>\n",
              "      <td>-0.667825</td>\n",
              "      <td>0.202086</td>\n",
              "      <td>0.055611</td>\n",
              "      <td>0.765707</td>\n",
              "      <td>0.150008</td>\n",
              "      <td>-0.024362</td>\n",
              "      <td>-0.579518</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>-1.123972</td>\n",
              "      <td>-0.447742</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>3.526988</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>-0.911926</td>\n",
              "      <td>-0.944532</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>-0.167388</td>\n",
              "      <td>-0.334036</td>\n",
              "      <td>-0.338774</td>\n",
              "      <td>0.474741</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-0.084299</td>\n",
              "      <td>-0.237260</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>555488</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.497981</td>\n",
              "      <td>3.995318</td>\n",
              "      <td>0.942502</td>\n",
              "      <td>-0.565190</td>\n",
              "      <td>-0.090246</td>\n",
              "      <td>0.427353</td>\n",
              "      <td>-0.435635</td>\n",
              "      <td>0.112821</td>\n",
              "      <td>0.016091</td>\n",
              "      <td>0.455173</td>\n",
              "      <td>-0.365256</td>\n",
              "      <td>-0.015928</td>\n",
              "      <td>-0.250703</td>\n",
              "      <td>0.453284</td>\n",
              "      <td>1.520369</td>\n",
              "      <td>-0.350885</td>\n",
              "      <td>-0.045471</td>\n",
              "      <td>-1.190263</td>\n",
              "      <td>-0.226942</td>\n",
              "      <td>0.325994</td>\n",
              "      <td>0.481571</td>\n",
              "      <td>1.885999</td>\n",
              "      <td>0.244673</td>\n",
              "      <td>4.147114</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>3.526988</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>0.066764</td>\n",
              "      <td>-0.944532</td>\n",
              "      <td>0.57957</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>-0.167388</td>\n",
              "      <td>-0.353566</td>\n",
              "      <td>-0.335163</td>\n",
              "      <td>-0.818309</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>1.823139</td>\n",
              "      <td>1.008334</td>\n",
              "      <td>0.400535</td>\n",
              "      <td>0.426709</td>\n",
              "      <td>2.026664</td>\n",
              "      <td>5.174360</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>1.032738</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>550513</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>3.909026</td>\n",
              "      <td>0.942502</td>\n",
              "      <td>-0.565190</td>\n",
              "      <td>0.343126</td>\n",
              "      <td>-0.125355</td>\n",
              "      <td>0.352635</td>\n",
              "      <td>0.303349</td>\n",
              "      <td>0.287937</td>\n",
              "      <td>-0.232126</td>\n",
              "      <td>0.903159</td>\n",
              "      <td>0.085179</td>\n",
              "      <td>0.074410</td>\n",
              "      <td>1.035166</td>\n",
              "      <td>-0.485475</td>\n",
              "      <td>-0.032725</td>\n",
              "      <td>-0.010092</td>\n",
              "      <td>0.765707</td>\n",
              "      <td>-0.003564</td>\n",
              "      <td>0.130603</td>\n",
              "      <td>1.306862</td>\n",
              "      <td>-1.673867</td>\n",
              "      <td>-1.123972</td>\n",
              "      <td>2.463849</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>-0.283528</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>0.844748</td>\n",
              "      <td>1.604705</td>\n",
              "      <td>1.058726</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>-0.167388</td>\n",
              "      <td>0.368588</td>\n",
              "      <td>0.417281</td>\n",
              "      <td>0.683533</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-0.084299</td>\n",
              "      <td>-0.144994</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>1.492943</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>0.608326</td>\n",
              "      <td>1.032738</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>551684</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.437261</td>\n",
              "      <td>3.736442</td>\n",
              "      <td>0.942502</td>\n",
              "      <td>-0.565190</td>\n",
              "      <td>0.860930</td>\n",
              "      <td>1.592930</td>\n",
              "      <td>1.708229</td>\n",
              "      <td>0.567157</td>\n",
              "      <td>0.502102</td>\n",
              "      <td>1.653649</td>\n",
              "      <td>0.061331</td>\n",
              "      <td>0.618662</td>\n",
              "      <td>0.121603</td>\n",
              "      <td>0.005740</td>\n",
              "      <td>-0.153931</td>\n",
              "      <td>0.866573</td>\n",
              "      <td>0.116259</td>\n",
              "      <td>3.373667</td>\n",
              "      <td>6.537219</td>\n",
              "      <td>0.231667</td>\n",
              "      <td>1.424760</td>\n",
              "      <td>1.885999</td>\n",
              "      <td>0.244673</td>\n",
              "      <td>1.485737</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>3.526988</td>\n",
              "      <td>6.748625</td>\n",
              "      <td>0.844748</td>\n",
              "      <td>0.039588</td>\n",
              "      <td>1.058726</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>0.537072</td>\n",
              "      <td>0.430215</td>\n",
              "      <td>0.442558</td>\n",
              "      <td>0.251296</td>\n",
              "      <td>2.268908</td>\n",
              "      <td>1.412773</td>\n",
              "      <td>1.823139</td>\n",
              "      <td>3.868587</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>550421</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>3.736442</td>\n",
              "      <td>-1.061005</td>\n",
              "      <td>1.769316</td>\n",
              "      <td>0.070048</td>\n",
              "      <td>-0.441189</td>\n",
              "      <td>0.028825</td>\n",
              "      <td>-0.414795</td>\n",
              "      <td>0.070264</td>\n",
              "      <td>-0.382472</td>\n",
              "      <td>-0.028979</td>\n",
              "      <td>0.392610</td>\n",
              "      <td>-0.040953</td>\n",
              "      <td>0.476303</td>\n",
              "      <td>-0.551784</td>\n",
              "      <td>-0.115843</td>\n",
              "      <td>-0.151606</td>\n",
              "      <td>0.765707</td>\n",
              "      <td>0.010397</td>\n",
              "      <td>-0.159115</td>\n",
              "      <td>0.481571</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>-1.123972</td>\n",
              "      <td>0.189168</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>-0.283528</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>0.844748</td>\n",
              "      <td>0.392994</td>\n",
              "      <td>1.058726</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>-0.167388</td>\n",
              "      <td>0.026607</td>\n",
              "      <td>0.066562</td>\n",
              "      <td>0.309904</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-0.084299</td>\n",
              "      <td>1.285133</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>563825</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19590</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>-1.527383</td>\n",
              "      <td>-1.061005</td>\n",
              "      <td>1.769316</td>\n",
              "      <td>-0.649761</td>\n",
              "      <td>-0.843501</td>\n",
              "      <td>-0.617220</td>\n",
              "      <td>-1.045004</td>\n",
              "      <td>-0.558618</td>\n",
              "      <td>-0.657392</td>\n",
              "      <td>-0.391123</td>\n",
              "      <td>-0.461045</td>\n",
              "      <td>-0.324115</td>\n",
              "      <td>-0.561803</td>\n",
              "      <td>-0.734134</td>\n",
              "      <td>-0.484798</td>\n",
              "      <td>-0.510444</td>\n",
              "      <td>-0.320943</td>\n",
              "      <td>0.017377</td>\n",
              "      <td>0.467484</td>\n",
              "      <td>-0.579518</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>-1.123972</td>\n",
              "      <td>-0.675211</td>\n",
              "      <td>-3.432510</td>\n",
              "      <td>-0.283528</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>-1.086692</td>\n",
              "      <td>-0.944532</td>\n",
              "      <td>0.57957</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>-0.167388</td>\n",
              "      <td>-0.706831</td>\n",
              "      <td>-0.696264</td>\n",
              "      <td>-0.726733</td>\n",
              "      <td>2.268908</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-1.991737</td>\n",
              "      <td>-0.606325</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>565294</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19591</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>-1.527383</td>\n",
              "      <td>0.942502</td>\n",
              "      <td>1.769316</td>\n",
              "      <td>-0.620777</td>\n",
              "      <td>-0.730703</td>\n",
              "      <td>-0.596753</td>\n",
              "      <td>-0.898444</td>\n",
              "      <td>-0.531812</td>\n",
              "      <td>-0.571479</td>\n",
              "      <td>-0.345742</td>\n",
              "      <td>-0.423438</td>\n",
              "      <td>-0.245459</td>\n",
              "      <td>-0.539916</td>\n",
              "      <td>-0.618093</td>\n",
              "      <td>-0.450396</td>\n",
              "      <td>-0.429579</td>\n",
              "      <td>-0.103613</td>\n",
              "      <td>-0.185059</td>\n",
              "      <td>0.002588</td>\n",
              "      <td>1.424760</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>-1.123972</td>\n",
              "      <td>-0.675211</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>-0.283528</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>-1.086692</td>\n",
              "      <td>1.058726</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>0.067432</td>\n",
              "      <td>-0.692076</td>\n",
              "      <td>-0.680917</td>\n",
              "      <td>-1.276187</td>\n",
              "      <td>2.268908</td>\n",
              "      <td>-0.181370</td>\n",
              "      <td>-1.991737</td>\n",
              "      <td>-0.606325</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>554293</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19592</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>-1.527383</td>\n",
              "      <td>-1.061005</td>\n",
              "      <td>-0.565190</td>\n",
              "      <td>-0.633253</td>\n",
              "      <td>-0.760783</td>\n",
              "      <td>-0.587831</td>\n",
              "      <td>-0.869132</td>\n",
              "      <td>-0.548092</td>\n",
              "      <td>-0.614435</td>\n",
              "      <td>-0.391123</td>\n",
              "      <td>-0.461045</td>\n",
              "      <td>-0.324115</td>\n",
              "      <td>-0.557275</td>\n",
              "      <td>-0.667825</td>\n",
              "      <td>-0.471176</td>\n",
              "      <td>-0.439687</td>\n",
              "      <td>-0.320943</td>\n",
              "      <td>-0.199020</td>\n",
              "      <td>-0.179328</td>\n",
              "      <td>-0.579518</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>0.244673</td>\n",
              "      <td>-0.675211</td>\n",
              "      <td>-3.432510</td>\n",
              "      <td>-0.283528</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>-1.067264</td>\n",
              "      <td>-0.944532</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>1.006712</td>\n",
              "      <td>-0.706831</td>\n",
              "      <td>-0.696264</td>\n",
              "      <td>-4.287197</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-0.084299</td>\n",
              "      <td>0.224071</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>556473</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19593</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>-1.613675</td>\n",
              "      <td>-1.061005</td>\n",
              "      <td>1.769316</td>\n",
              "      <td>-0.580829</td>\n",
              "      <td>-0.715664</td>\n",
              "      <td>-0.530101</td>\n",
              "      <td>-0.795852</td>\n",
              "      <td>-0.505147</td>\n",
              "      <td>-0.584366</td>\n",
              "      <td>-0.319874</td>\n",
              "      <td>-0.419328</td>\n",
              "      <td>-0.297896</td>\n",
              "      <td>-0.553124</td>\n",
              "      <td>-0.667825</td>\n",
              "      <td>-0.392906</td>\n",
              "      <td>-0.424525</td>\n",
              "      <td>-0.103613</td>\n",
              "      <td>-0.192039</td>\n",
              "      <td>-0.179328</td>\n",
              "      <td>-1.522707</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>-1.123972</td>\n",
              "      <td>-0.652464</td>\n",
              "      <td>0.291332</td>\n",
              "      <td>3.526988</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>-1.047865</td>\n",
              "      <td>1.058726</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>4.294194</td>\n",
              "      <td>-0.706831</td>\n",
              "      <td>-0.696264</td>\n",
              "      <td>-4.287197</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-0.084299</td>\n",
              "      <td>3.914720</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>560898</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19594</th>\n",
              "      <td>-0.441299</td>\n",
              "      <td>-1.613675</td>\n",
              "      <td>-1.061005</td>\n",
              "      <td>-0.565190</td>\n",
              "      <td>-0.649761</td>\n",
              "      <td>-0.843501</td>\n",
              "      <td>-0.617220</td>\n",
              "      <td>-1.045004</td>\n",
              "      <td>-0.558618</td>\n",
              "      <td>-0.657392</td>\n",
              "      <td>-0.391123</td>\n",
              "      <td>-0.461045</td>\n",
              "      <td>-0.324115</td>\n",
              "      <td>-0.561803</td>\n",
              "      <td>-0.734134</td>\n",
              "      <td>-0.484798</td>\n",
              "      <td>-0.510444</td>\n",
              "      <td>-1.190263</td>\n",
              "      <td>-0.226942</td>\n",
              "      <td>-0.192803</td>\n",
              "      <td>-1.522707</td>\n",
              "      <td>-0.487245</td>\n",
              "      <td>0.244673</td>\n",
              "      <td>-0.697957</td>\n",
              "      <td>-3.432510</td>\n",
              "      <td>-0.283528</td>\n",
              "      <td>-0.148178</td>\n",
              "      <td>-1.183785</td>\n",
              "      <td>-1.086692</td>\n",
              "      <td>-0.944532</td>\n",
              "      <td>-0.24514</td>\n",
              "      <td>-0.194958</td>\n",
              "      <td>-0.107235</td>\n",
              "      <td>-0.09355</td>\n",
              "      <td>-0.167388</td>\n",
              "      <td>-0.706831</td>\n",
              "      <td>-0.696264</td>\n",
              "      <td>-4.287197</td>\n",
              "      <td>-0.422493</td>\n",
              "      <td>-0.245211</td>\n",
              "      <td>-1.991737</td>\n",
              "      <td>-0.606325</td>\n",
              "      <td>-0.279356</td>\n",
              "      <td>-0.320960</td>\n",
              "      <td>-0.268601</td>\n",
              "      <td>-0.397933</td>\n",
              "      <td>-0.254567</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>563890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19595 rows  51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       count_accts    cm_age  ...  cust_id  profitable_flag\n",
              "0        -0.441299  4.081611  ...   555488                1\n",
              "1         1.497981  3.995318  ...   550513                1\n",
              "2        -0.441299  3.909026  ...   551684                1\n",
              "3         3.437261  3.736442  ...   550421                1\n",
              "4        -0.441299  3.736442  ...   563825                1\n",
              "...            ...       ...  ...      ...              ...\n",
              "19590    -0.441299 -1.527383  ...   565294                0\n",
              "19591    -0.441299 -1.527383  ...   554293                0\n",
              "19592    -0.441299 -1.527383  ...   556473                0\n",
              "19593    -0.441299 -1.613675  ...   560898                0\n",
              "19594    -0.441299 -1.613675  ...   563890                0\n",
              "\n",
              "[19595 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xgMrW9hH-L"
      },
      "source": [
        "f_scores = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DFZuorcagm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6772764-947e-4969-a846-6b6bafdb061c"
      },
      "source": [
        "%%time\n",
        "for j in range(10,49):\n",
        "  print('______________________________')\n",
        "  print()\n",
        "  print(j)\n",
        "  print('______________________________')\n",
        "  l=[]\n",
        "  for i in range(j):\n",
        "    l.append(pdf['Feature column'][i])\n",
        "  l.append('profitable_flag')\n",
        "  print(len(l),j)\n",
        "  df1=pd.DataFrame()\n",
        "  for col in columns_for_all:\n",
        "    if col in l:\n",
        "      df1[col]=scaled_features_df[col]\n",
        "  df=df1\n",
        "  print(df)\n",
        "  X = df.drop(['profitable_flag'],axis=1)\n",
        "  y = df['profitable_flag']\n",
        "\n",
        "  #Train_test_split\n",
        "  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  #Logistic regression\n",
        "  clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "  y_pred=clf.predict(X_test)\n",
        "  y_pred2=clf.predict_proba(X_test)\n",
        "  clf.score(X_test, y_test)\n",
        "  k = f1_score(y_test, y_pred, average='macro')\n",
        "  f_scores[('Logistic_regression_without_thresholding',j)]=k\n",
        "\n",
        "  #Linear regression\n",
        "  reg = LinearRegression().fit(X_train, y_train)\n",
        "  reg.score(X_train, y_train)\n",
        "  reg.coef_\n",
        "  reg.intercept_\n",
        "  p=reg.predict(X_test)\n",
        "  p=p>0.234\n",
        "  p=p+0\n",
        "  y_test=np.array(y_test)\n",
        "  correct_pred=0\n",
        "  correct_ones=0\n",
        "  for i in range(len(p)):\n",
        "    if y_test[i]==p[i]:\n",
        "      correct_pred += 1\n",
        "      if y_test[i]==1:\n",
        "        correct_ones+=1\n",
        "  print(correct_pred/len(p))\n",
        "  correct_ones\n",
        "  k=f1_score(y_test, y_pred, average='macro')\n",
        "  f_scores[('Linear_regression_with_thresholding=0.234',j)]=k\n",
        "  \n",
        "  #GNB\n",
        "  gnb = GaussianNB()\n",
        "  y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "  print(\"Number of mislabeled points out of a total %d points : %d\"\n",
        "  % (X_test.shape[0], (y_test != y_pred).sum()))\n",
        "  y_test = np.array(y_test)\n",
        "  correct_pred=0\n",
        "  correct_ones=0\n",
        "  for i in range(len(y_pred)):\n",
        "    if y_test[i]==y_pred[i]:\n",
        "      correct_pred += 1\n",
        "      if y_test[i]==1:\n",
        "        correct_ones+=1\n",
        "  print(correct_pred/len(y_pred))\n",
        "  print('correct_ones',correct_ones)\n",
        "  f1_score(y_test, y_pred, average='macro')\n",
        "  k=f1_score(y_test, y_pred, average='macro')\n",
        "  f_scores[('Gnb',j)]=k\n",
        "\n",
        "  #trivial case\n",
        "  y_pred=np.zeros((y_test.shape))\n",
        "  correct_pred=0\n",
        "  correct_ones=0\n",
        "  for i in range(len(y_pred)):\n",
        "    if y_test[i]==y_pred[i]:\n",
        "      correct_pred += 1\n",
        "      if y_test[i]==1:\n",
        "        correct_ones+=1\n",
        "  print(correct_pred/len(y_pred))\n",
        "  print('correct_ones',correct_ones)\n",
        "  f1_score(y_test, y_pred, average='macro')\n",
        "  k=f1_score(y_test, y_pred, average='macro')\n",
        "  f_scores[('Zeros',j)]=k\n",
        "\n",
        "  #Random Forest\n",
        "  from sklearn.ensemble import RandomForestClassifier \n",
        "  #Create a Gaussian Classifier \n",
        "  clf=RandomForestClassifier(n_estimators=100) \n",
        "  #Train the model using the training sets \n",
        "  clf.fit(X_train,y_train)\n",
        "  y_pred=clf.predict(X_test) \n",
        "  np.sum(y_pred)\n",
        "  k=f1_score(y_test, y_pred, average='macro')\n",
        "  f_scores[('Random Forest',j)]=k\n",
        "\n",
        "  #neural nets\n",
        "  X = df.iloc[:, 0:-1]\n",
        "  y = df.iloc[:, -1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.fit_transform(X_test)\n",
        "  EPOCHS = 40\n",
        "  BATCH_SIZE = 64\n",
        "  LEARNING_RATE = 0.001\n",
        "  class trainData(Dataset):\n",
        "      \n",
        "      def __init__(self, X_data, y_data):\n",
        "          self.X_data = X_data\n",
        "          self.y_data = y_data\n",
        "          \n",
        "      def __getitem__(self, index):\n",
        "          return self.X_data[index], self.y_data[index]\n",
        "          \n",
        "      def __len__ (self):\n",
        "          return len(self.X_data)\n",
        "\n",
        "\n",
        "  train_data = trainData(torch.FloatTensor(X_train), \n",
        "                        torch.FloatTensor(y_train))\n",
        "  ## test data    \n",
        "  class testData(Dataset):\n",
        "      \n",
        "      def __init__(self, X_data):\n",
        "          self.X_data = X_data\n",
        "          \n",
        "      def __getitem__(self, index):\n",
        "          return self.X_data[index]\n",
        "          \n",
        "      def __len__ (self):\n",
        "          return len(self.X_data)\n",
        "      \n",
        "\n",
        "  test_data = testData(torch.FloatTensor(X_test))\n",
        "  train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
        "  l=len(X.columns)\n",
        "\n",
        "  class binaryClassification(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(binaryClassification, self).__init__()\n",
        "          # Number of input features is l.\n",
        "          self.layer_1 = nn.Linear(l, 64) \n",
        "          self.layer_2 = nn.Linear(64, 64)\n",
        "          self.layer_3 = nn.Linear(64, 64)\n",
        "          self.layer_4 = nn.Linear(64, 64)\n",
        "          self.layer_out = nn.Linear(64, 1) \n",
        "          \n",
        "          self.relu = nn.ReLU()\n",
        "          self.dropout = nn.Dropout(p=0.3)\n",
        "          self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "          self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "          \n",
        "      def forward(self, inputs):\n",
        "          x = self.relu(self.layer_1(inputs))\n",
        "          x = self.batchnorm1(x)\n",
        "          x = self.relu(self.layer_2(x))\n",
        "          x = self.batchnorm2(x)\n",
        "          x = self.relu(self.layer_3(x))\n",
        "          x = self.dropout(x)\n",
        "          x = self.relu(self.layer_4(x))\n",
        "          x = self.dropout(x)\n",
        "          x = self.layer_out(x)\n",
        "          \n",
        "          return x\n",
        "  model = binaryClassification()\n",
        "  model.to(device)\n",
        "  print(model)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "  def binary_acc(y_pred, y_test):\n",
        "      y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "      correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "      acc = correct_results_sum/y_test.shape[0]\n",
        "      acc = torch.round(acc * 100)\n",
        "      \n",
        "      return acc\n",
        "  model.train()\n",
        "  for e in range(1, EPOCHS+1):\n",
        "      epoch_loss = 0\n",
        "      epoch_acc = 0\n",
        "      for X_batch, y_batch in train_loader:\n",
        "          X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          y_pred = model(X_batch)\n",
        "          \n",
        "          loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "          acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "          \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "          \n",
        "\n",
        "      print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
        "  y_pred_list = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for X_batch in test_loader:\n",
        "          X_batch = X_batch.to(device)\n",
        "          y_test_pred = model(X_batch)\n",
        "          y_test_pred = torch.sigmoid(y_test_pred)\n",
        "          y_pred_tag = torch.round(y_test_pred)\n",
        "          y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "\n",
        "  y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "  confusion_matrix(y_test, y_pred_list)\n",
        "  q = (classification_report(y_test, y_pred_list))\n",
        "  print(q)\n",
        "  k=f1_score(y_test, y_pred_list, average='macro')\n",
        "  f_scores[('Neural Nets',j)] = k \n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred=clf.predict(X_test) \n",
        "  print(np.sum(y_pred))\n",
        "  k=f1_score(y_test, y_pred, average='macro')\n",
        "  f_scores[('SVC',j)]=k\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "______________________________\n",
            "\n",
            "10\n",
            "______________________________\n",
            "11 10\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 11 columns]\n",
            "0.7519775452921663\n",
            "Number of mislabeled points out of a total 3919 points : 864\n",
            "0.779535595815259\n",
            "correct_ones 130\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=10, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48927 | Acc: 81.086\n",
            "Epoch 002: | Loss: 0.47402 | Acc: 81.498\n",
            "Epoch 003: | Loss: 0.47175 | Acc: 81.453\n",
            "Epoch 004: | Loss: 0.46892 | Acc: 81.453\n",
            "Epoch 005: | Loss: 0.46961 | Acc: 81.376\n",
            "Epoch 006: | Loss: 0.46840 | Acc: 81.445\n",
            "Epoch 007: | Loss: 0.46786 | Acc: 81.478\n",
            "Epoch 008: | Loss: 0.46598 | Acc: 81.498\n",
            "Epoch 009: | Loss: 0.46540 | Acc: 81.449\n",
            "Epoch 010: | Loss: 0.46524 | Acc: 81.502\n",
            "Epoch 011: | Loss: 0.46439 | Acc: 81.494\n",
            "Epoch 012: | Loss: 0.46547 | Acc: 81.473\n",
            "Epoch 013: | Loss: 0.46493 | Acc: 81.478\n",
            "Epoch 014: | Loss: 0.46267 | Acc: 81.518\n",
            "Epoch 015: | Loss: 0.46378 | Acc: 81.482\n",
            "Epoch 016: | Loss: 0.46307 | Acc: 81.469\n",
            "Epoch 017: | Loss: 0.46241 | Acc: 81.486\n",
            "Epoch 018: | Loss: 0.46259 | Acc: 81.408\n",
            "Epoch 019: | Loss: 0.46057 | Acc: 81.547\n",
            "Epoch 020: | Loss: 0.46068 | Acc: 81.514\n",
            "Epoch 021: | Loss: 0.46196 | Acc: 81.657\n",
            "Epoch 022: | Loss: 0.46024 | Acc: 81.514\n",
            "Epoch 023: | Loss: 0.46084 | Acc: 81.518\n",
            "Epoch 024: | Loss: 0.45959 | Acc: 81.510\n",
            "Epoch 025: | Loss: 0.45845 | Acc: 81.559\n",
            "Epoch 026: | Loss: 0.46012 | Acc: 81.604\n",
            "Epoch 027: | Loss: 0.46017 | Acc: 81.576\n",
            "Epoch 028: | Loss: 0.45964 | Acc: 81.559\n",
            "Epoch 029: | Loss: 0.45850 | Acc: 81.502\n",
            "Epoch 030: | Loss: 0.45798 | Acc: 81.584\n",
            "Epoch 031: | Loss: 0.45860 | Acc: 81.686\n",
            "Epoch 032: | Loss: 0.45705 | Acc: 81.665\n",
            "Epoch 033: | Loss: 0.45688 | Acc: 81.661\n",
            "Epoch 034: | Loss: 0.45738 | Acc: 81.596\n",
            "Epoch 035: | Loss: 0.45869 | Acc: 81.502\n",
            "Epoch 036: | Loss: 0.45589 | Acc: 81.743\n",
            "Epoch 037: | Loss: 0.45624 | Acc: 81.649\n",
            "Epoch 038: | Loss: 0.45424 | Acc: 81.800\n",
            "Epoch 039: | Loss: 0.45480 | Acc: 81.706\n",
            "Epoch 040: | Loss: 0.45480 | Acc: 81.686\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.99      0.90      3187\n",
            "           1       0.41      0.02      0.04       732\n",
            "\n",
            "    accuracy                           0.81      3919\n",
            "   macro avg       0.61      0.51      0.47      3919\n",
            "weighted avg       0.74      0.81      0.74      3919\n",
            "\n",
            "3\n",
            "______________________________\n",
            "\n",
            "11\n",
            "______________________________\n",
            "12 11\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 12 columns]\n",
            "0.7481500382750702\n",
            "Number of mislabeled points out of a total 3919 points : 855\n",
            "0.7818321000255167\n",
            "correct_ones 130\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48111 | Acc: 81.486\n",
            "Epoch 002: | Loss: 0.46906 | Acc: 81.465\n",
            "Epoch 003: | Loss: 0.46583 | Acc: 81.441\n",
            "Epoch 004: | Loss: 0.46675 | Acc: 81.437\n",
            "Epoch 005: | Loss: 0.46366 | Acc: 81.478\n",
            "Epoch 006: | Loss: 0.46305 | Acc: 81.416\n",
            "Epoch 007: | Loss: 0.46456 | Acc: 81.445\n",
            "Epoch 008: | Loss: 0.46196 | Acc: 81.502\n",
            "Epoch 009: | Loss: 0.46111 | Acc: 81.506\n",
            "Epoch 010: | Loss: 0.46052 | Acc: 81.469\n",
            "Epoch 011: | Loss: 0.46044 | Acc: 81.473\n",
            "Epoch 012: | Loss: 0.45748 | Acc: 81.498\n",
            "Epoch 013: | Loss: 0.45840 | Acc: 81.465\n",
            "Epoch 014: | Loss: 0.45793 | Acc: 81.559\n",
            "Epoch 015: | Loss: 0.45839 | Acc: 81.567\n",
            "Epoch 016: | Loss: 0.45741 | Acc: 81.653\n",
            "Epoch 017: | Loss: 0.45691 | Acc: 81.580\n",
            "Epoch 018: | Loss: 0.45596 | Acc: 81.502\n",
            "Epoch 019: | Loss: 0.45541 | Acc: 81.551\n",
            "Epoch 020: | Loss: 0.45484 | Acc: 81.527\n",
            "Epoch 021: | Loss: 0.45624 | Acc: 81.649\n",
            "Epoch 022: | Loss: 0.45392 | Acc: 81.669\n",
            "Epoch 023: | Loss: 0.45435 | Acc: 81.624\n",
            "Epoch 024: | Loss: 0.45154 | Acc: 81.743\n",
            "Epoch 025: | Loss: 0.45430 | Acc: 81.678\n",
            "Epoch 026: | Loss: 0.45203 | Acc: 81.718\n",
            "Epoch 027: | Loss: 0.45243 | Acc: 81.665\n",
            "Epoch 028: | Loss: 0.45183 | Acc: 81.584\n",
            "Epoch 029: | Loss: 0.45015 | Acc: 81.710\n",
            "Epoch 030: | Loss: 0.45193 | Acc: 81.710\n",
            "Epoch 031: | Loss: 0.45067 | Acc: 81.804\n",
            "Epoch 032: | Loss: 0.44911 | Acc: 81.792\n",
            "Epoch 033: | Loss: 0.45043 | Acc: 81.784\n",
            "Epoch 034: | Loss: 0.44878 | Acc: 81.784\n",
            "Epoch 035: | Loss: 0.44801 | Acc: 81.784\n",
            "Epoch 036: | Loss: 0.44690 | Acc: 81.763\n",
            "Epoch 037: | Loss: 0.44686 | Acc: 81.955\n",
            "Epoch 038: | Loss: 0.44695 | Acc: 82.024\n",
            "Epoch 039: | Loss: 0.44660 | Acc: 81.967\n",
            "Epoch 040: | Loss: 0.44474 | Acc: 81.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.99      0.90      3187\n",
            "           1       0.52      0.02      0.04       732\n",
            "\n",
            "    accuracy                           0.81      3919\n",
            "   macro avg       0.67      0.51      0.47      3919\n",
            "weighted avg       0.76      0.81      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "12\n",
            "______________________________\n",
            "13 12\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 13 columns]\n",
            "0.7486603725440163\n",
            "Number of mislabeled points out of a total 3919 points : 884\n",
            "0.7744322531257974\n",
            "correct_ones 155\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48748 | Acc: 80.457\n",
            "Epoch 002: | Loss: 0.47122 | Acc: 81.469\n",
            "Epoch 003: | Loss: 0.46623 | Acc: 81.465\n",
            "Epoch 004: | Loss: 0.46756 | Acc: 81.490\n",
            "Epoch 005: | Loss: 0.46436 | Acc: 81.510\n",
            "Epoch 006: | Loss: 0.46329 | Acc: 81.482\n",
            "Epoch 007: | Loss: 0.46396 | Acc: 81.465\n",
            "Epoch 008: | Loss: 0.46162 | Acc: 81.482\n",
            "Epoch 009: | Loss: 0.46204 | Acc: 81.465\n",
            "Epoch 010: | Loss: 0.46164 | Acc: 81.490\n",
            "Epoch 011: | Loss: 0.46049 | Acc: 81.498\n",
            "Epoch 012: | Loss: 0.45915 | Acc: 81.531\n",
            "Epoch 013: | Loss: 0.45906 | Acc: 81.596\n",
            "Epoch 014: | Loss: 0.45820 | Acc: 81.465\n",
            "Epoch 015: | Loss: 0.45866 | Acc: 81.612\n",
            "Epoch 016: | Loss: 0.45761 | Acc: 81.522\n",
            "Epoch 017: | Loss: 0.45656 | Acc: 81.588\n",
            "Epoch 018: | Loss: 0.45565 | Acc: 81.527\n",
            "Epoch 019: | Loss: 0.45658 | Acc: 81.616\n",
            "Epoch 020: | Loss: 0.45726 | Acc: 81.547\n",
            "Epoch 021: | Loss: 0.45424 | Acc: 81.616\n",
            "Epoch 022: | Loss: 0.45541 | Acc: 81.584\n",
            "Epoch 023: | Loss: 0.45311 | Acc: 81.702\n",
            "Epoch 024: | Loss: 0.45556 | Acc: 81.543\n",
            "Epoch 025: | Loss: 0.45411 | Acc: 81.567\n",
            "Epoch 026: | Loss: 0.45495 | Acc: 81.669\n",
            "Epoch 027: | Loss: 0.45161 | Acc: 81.784\n",
            "Epoch 028: | Loss: 0.45284 | Acc: 81.600\n",
            "Epoch 029: | Loss: 0.45310 | Acc: 81.869\n",
            "Epoch 030: | Loss: 0.45049 | Acc: 81.710\n",
            "Epoch 031: | Loss: 0.45092 | Acc: 81.780\n",
            "Epoch 032: | Loss: 0.44996 | Acc: 81.771\n",
            "Epoch 033: | Loss: 0.45009 | Acc: 81.841\n",
            "Epoch 034: | Loss: 0.44877 | Acc: 81.808\n",
            "Epoch 035: | Loss: 0.44876 | Acc: 81.812\n",
            "Epoch 036: | Loss: 0.44824 | Acc: 81.751\n",
            "Epoch 037: | Loss: 0.44783 | Acc: 81.902\n",
            "Epoch 038: | Loss: 0.44673 | Acc: 81.955\n",
            "Epoch 039: | Loss: 0.44770 | Acc: 81.943\n",
            "Epoch 040: | Loss: 0.44431 | Acc: 81.963\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.99      0.89      3187\n",
            "           1       0.36      0.03      0.05       732\n",
            "\n",
            "    accuracy                           0.81      3919\n",
            "   macro avg       0.59      0.51      0.47      3919\n",
            "weighted avg       0.73      0.81      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "13\n",
            "______________________________\n",
            "14 13\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 14 columns]\n",
            "0.7499362082163817\n",
            "Number of mislabeled points out of a total 3919 points : 1066\n",
            "0.7279918346516968\n",
            "correct_ones 243\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=13, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48369 | Acc: 81.188\n",
            "Epoch 002: | Loss: 0.46737 | Acc: 81.449\n",
            "Epoch 003: | Loss: 0.46608 | Acc: 81.490\n",
            "Epoch 004: | Loss: 0.46563 | Acc: 81.469\n",
            "Epoch 005: | Loss: 0.46463 | Acc: 81.465\n",
            "Epoch 006: | Loss: 0.46278 | Acc: 81.433\n",
            "Epoch 007: | Loss: 0.46128 | Acc: 81.473\n",
            "Epoch 008: | Loss: 0.46232 | Acc: 81.482\n",
            "Epoch 009: | Loss: 0.46204 | Acc: 81.461\n",
            "Epoch 010: | Loss: 0.46002 | Acc: 81.380\n",
            "Epoch 011: | Loss: 0.45948 | Acc: 81.449\n",
            "Epoch 012: | Loss: 0.45937 | Acc: 81.465\n",
            "Epoch 013: | Loss: 0.45945 | Acc: 81.494\n",
            "Epoch 014: | Loss: 0.45751 | Acc: 81.518\n",
            "Epoch 015: | Loss: 0.45731 | Acc: 81.539\n",
            "Epoch 016: | Loss: 0.45698 | Acc: 81.555\n",
            "Epoch 017: | Loss: 0.45494 | Acc: 81.661\n",
            "Epoch 018: | Loss: 0.45629 | Acc: 81.543\n",
            "Epoch 019: | Loss: 0.45580 | Acc: 81.624\n",
            "Epoch 020: | Loss: 0.45408 | Acc: 81.588\n",
            "Epoch 021: | Loss: 0.45423 | Acc: 81.706\n",
            "Epoch 022: | Loss: 0.45227 | Acc: 81.698\n",
            "Epoch 023: | Loss: 0.45218 | Acc: 81.645\n",
            "Epoch 024: | Loss: 0.45047 | Acc: 81.735\n",
            "Epoch 025: | Loss: 0.45438 | Acc: 81.653\n",
            "Epoch 026: | Loss: 0.45113 | Acc: 81.657\n",
            "Epoch 027: | Loss: 0.45187 | Acc: 81.576\n",
            "Epoch 028: | Loss: 0.45027 | Acc: 81.755\n",
            "Epoch 029: | Loss: 0.44919 | Acc: 81.829\n",
            "Epoch 030: | Loss: 0.44894 | Acc: 81.837\n",
            "Epoch 031: | Loss: 0.44788 | Acc: 81.886\n",
            "Epoch 032: | Loss: 0.44770 | Acc: 81.808\n",
            "Epoch 033: | Loss: 0.44713 | Acc: 81.955\n",
            "Epoch 034: | Loss: 0.44762 | Acc: 81.788\n",
            "Epoch 035: | Loss: 0.44794 | Acc: 81.788\n",
            "Epoch 036: | Loss: 0.44706 | Acc: 81.971\n",
            "Epoch 037: | Loss: 0.44557 | Acc: 82.029\n",
            "Epoch 038: | Loss: 0.44636 | Acc: 81.882\n",
            "Epoch 039: | Loss: 0.44339 | Acc: 82.053\n",
            "Epoch 040: | Loss: 0.44442 | Acc: 82.057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.99      0.89      3187\n",
            "           1       0.35      0.03      0.06       732\n",
            "\n",
            "    accuracy                           0.81      3919\n",
            "   macro avg       0.58      0.51      0.48      3919\n",
            "weighted avg       0.73      0.81      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "14\n",
            "______________________________\n",
            "15 14\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 15 columns]\n",
            "0.7486603725440163\n",
            "Number of mislabeled points out of a total 3919 points : 1080\n",
            "0.7244194947690737\n",
            "correct_ones 249\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=14, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48230 | Acc: 81.318\n",
            "Epoch 002: | Loss: 0.46740 | Acc: 81.461\n",
            "Epoch 003: | Loss: 0.46593 | Acc: 81.412\n",
            "Epoch 004: | Loss: 0.46399 | Acc: 81.469\n",
            "Epoch 005: | Loss: 0.46271 | Acc: 81.494\n",
            "Epoch 006: | Loss: 0.46074 | Acc: 81.465\n",
            "Epoch 007: | Loss: 0.45969 | Acc: 81.441\n",
            "Epoch 008: | Loss: 0.46073 | Acc: 81.465\n",
            "Epoch 009: | Loss: 0.45829 | Acc: 81.457\n",
            "Epoch 010: | Loss: 0.45749 | Acc: 81.478\n",
            "Epoch 011: | Loss: 0.45641 | Acc: 81.469\n",
            "Epoch 012: | Loss: 0.45580 | Acc: 81.551\n",
            "Epoch 013: | Loss: 0.45617 | Acc: 81.616\n",
            "Epoch 014: | Loss: 0.45609 | Acc: 81.502\n",
            "Epoch 015: | Loss: 0.45365 | Acc: 81.551\n",
            "Epoch 016: | Loss: 0.45253 | Acc: 81.559\n",
            "Epoch 017: | Loss: 0.45384 | Acc: 81.539\n",
            "Epoch 018: | Loss: 0.45147 | Acc: 81.694\n",
            "Epoch 019: | Loss: 0.44980 | Acc: 81.796\n",
            "Epoch 020: | Loss: 0.44956 | Acc: 81.669\n",
            "Epoch 021: | Loss: 0.44990 | Acc: 81.722\n",
            "Epoch 022: | Loss: 0.44778 | Acc: 81.694\n",
            "Epoch 023: | Loss: 0.45036 | Acc: 81.673\n",
            "Epoch 024: | Loss: 0.44873 | Acc: 81.739\n",
            "Epoch 025: | Loss: 0.44678 | Acc: 81.694\n",
            "Epoch 026: | Loss: 0.44803 | Acc: 81.731\n",
            "Epoch 027: | Loss: 0.44626 | Acc: 81.776\n",
            "Epoch 028: | Loss: 0.44388 | Acc: 81.918\n",
            "Epoch 029: | Loss: 0.44710 | Acc: 81.902\n",
            "Epoch 030: | Loss: 0.44464 | Acc: 81.849\n",
            "Epoch 031: | Loss: 0.44430 | Acc: 81.882\n",
            "Epoch 032: | Loss: 0.44126 | Acc: 81.943\n",
            "Epoch 033: | Loss: 0.44343 | Acc: 81.947\n",
            "Epoch 034: | Loss: 0.44216 | Acc: 81.935\n",
            "Epoch 035: | Loss: 0.44145 | Acc: 82.029\n",
            "Epoch 036: | Loss: 0.44124 | Acc: 81.882\n",
            "Epoch 037: | Loss: 0.44029 | Acc: 81.886\n",
            "Epoch 038: | Loss: 0.44057 | Acc: 81.959\n",
            "Epoch 039: | Loss: 0.43822 | Acc: 82.082\n",
            "Epoch 040: | Loss: 0.43785 | Acc: 82.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.98      0.89      3187\n",
            "           1       0.31      0.05      0.08       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.57      0.51      0.49      3919\n",
            "weighted avg       0.72      0.80      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "15\n",
            "______________________________\n",
            "16 15\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 16 columns]\n",
            "0.7504465424853279\n",
            "Number of mislabeled points out of a total 3919 points : 1076\n",
            "0.725440163306966\n",
            "correct_ones 247\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=15, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48078 | Acc: 81.408\n",
            "Epoch 002: | Loss: 0.46887 | Acc: 81.478\n",
            "Epoch 003: | Loss: 0.46430 | Acc: 81.445\n",
            "Epoch 004: | Loss: 0.46246 | Acc: 81.441\n",
            "Epoch 005: | Loss: 0.46266 | Acc: 81.445\n",
            "Epoch 006: | Loss: 0.46106 | Acc: 81.437\n",
            "Epoch 007: | Loss: 0.45870 | Acc: 81.416\n",
            "Epoch 008: | Loss: 0.45773 | Acc: 81.437\n",
            "Epoch 009: | Loss: 0.45755 | Acc: 81.449\n",
            "Epoch 010: | Loss: 0.45639 | Acc: 81.449\n",
            "Epoch 011: | Loss: 0.45543 | Acc: 81.424\n",
            "Epoch 012: | Loss: 0.45416 | Acc: 81.522\n",
            "Epoch 013: | Loss: 0.45291 | Acc: 81.596\n",
            "Epoch 014: | Loss: 0.45302 | Acc: 81.588\n",
            "Epoch 015: | Loss: 0.45351 | Acc: 81.376\n",
            "Epoch 016: | Loss: 0.45142 | Acc: 81.473\n",
            "Epoch 017: | Loss: 0.45025 | Acc: 81.690\n",
            "Epoch 018: | Loss: 0.44928 | Acc: 81.563\n",
            "Epoch 019: | Loss: 0.44873 | Acc: 81.702\n",
            "Epoch 020: | Loss: 0.44646 | Acc: 81.829\n",
            "Epoch 021: | Loss: 0.44704 | Acc: 81.735\n",
            "Epoch 022: | Loss: 0.44631 | Acc: 81.702\n",
            "Epoch 023: | Loss: 0.44585 | Acc: 81.776\n",
            "Epoch 024: | Loss: 0.44426 | Acc: 81.943\n",
            "Epoch 025: | Loss: 0.44436 | Acc: 81.816\n",
            "Epoch 026: | Loss: 0.44365 | Acc: 81.824\n",
            "Epoch 027: | Loss: 0.44237 | Acc: 81.980\n",
            "Epoch 028: | Loss: 0.44072 | Acc: 81.922\n",
            "Epoch 029: | Loss: 0.44031 | Acc: 81.837\n",
            "Epoch 030: | Loss: 0.43932 | Acc: 82.147\n",
            "Epoch 031: | Loss: 0.43763 | Acc: 82.012\n",
            "Epoch 032: | Loss: 0.43817 | Acc: 82.102\n",
            "Epoch 033: | Loss: 0.43563 | Acc: 82.053\n",
            "Epoch 034: | Loss: 0.43553 | Acc: 82.294\n",
            "Epoch 035: | Loss: 0.43427 | Acc: 82.522\n",
            "Epoch 036: | Loss: 0.43333 | Acc: 82.339\n",
            "Epoch 037: | Loss: 0.43372 | Acc: 82.233\n",
            "Epoch 038: | Loss: 0.43342 | Acc: 82.457\n",
            "Epoch 039: | Loss: 0.43146 | Acc: 82.408\n",
            "Epoch 040: | Loss: 0.42969 | Acc: 82.278\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.98      0.89      3187\n",
            "           1       0.30      0.04      0.07       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.56      0.51      0.48      3919\n",
            "weighted avg       0.72      0.80      0.74      3919\n",
            "\n",
            "4\n",
            "______________________________\n",
            "\n",
            "16\n",
            "______________________________\n",
            "17 16\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 17 columns]\n",
            "0.7558050523092625\n",
            "Number of mislabeled points out of a total 3919 points : 982\n",
            "0.7494258739474355\n",
            "correct_ones 219\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=16, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48551 | Acc: 80.869\n",
            "Epoch 002: | Loss: 0.46577 | Acc: 81.437\n",
            "Epoch 003: | Loss: 0.46197 | Acc: 81.506\n",
            "Epoch 004: | Loss: 0.45968 | Acc: 81.445\n",
            "Epoch 005: | Loss: 0.45791 | Acc: 81.424\n",
            "Epoch 006: | Loss: 0.45553 | Acc: 81.449\n",
            "Epoch 007: | Loss: 0.45580 | Acc: 81.453\n",
            "Epoch 008: | Loss: 0.45422 | Acc: 81.445\n",
            "Epoch 009: | Loss: 0.45201 | Acc: 81.404\n",
            "Epoch 010: | Loss: 0.45212 | Acc: 81.498\n",
            "Epoch 011: | Loss: 0.45274 | Acc: 81.408\n",
            "Epoch 012: | Loss: 0.44879 | Acc: 81.461\n",
            "Epoch 013: | Loss: 0.45107 | Acc: 81.555\n",
            "Epoch 014: | Loss: 0.44785 | Acc: 81.555\n",
            "Epoch 015: | Loss: 0.45000 | Acc: 81.437\n",
            "Epoch 016: | Loss: 0.44752 | Acc: 81.588\n",
            "Epoch 017: | Loss: 0.44462 | Acc: 81.637\n",
            "Epoch 018: | Loss: 0.44520 | Acc: 81.629\n",
            "Epoch 019: | Loss: 0.44455 | Acc: 81.527\n",
            "Epoch 020: | Loss: 0.44379 | Acc: 81.743\n",
            "Epoch 021: | Loss: 0.44304 | Acc: 81.682\n",
            "Epoch 022: | Loss: 0.44107 | Acc: 81.816\n",
            "Epoch 023: | Loss: 0.44167 | Acc: 82.114\n",
            "Epoch 024: | Loss: 0.44051 | Acc: 81.906\n",
            "Epoch 025: | Loss: 0.43995 | Acc: 81.898\n",
            "Epoch 026: | Loss: 0.43934 | Acc: 81.853\n",
            "Epoch 027: | Loss: 0.43865 | Acc: 82.053\n",
            "Epoch 028: | Loss: 0.43787 | Acc: 82.069\n",
            "Epoch 029: | Loss: 0.43485 | Acc: 82.082\n",
            "Epoch 030: | Loss: 0.43649 | Acc: 82.139\n",
            "Epoch 031: | Loss: 0.43398 | Acc: 82.041\n",
            "Epoch 032: | Loss: 0.43173 | Acc: 82.249\n",
            "Epoch 033: | Loss: 0.43146 | Acc: 82.151\n",
            "Epoch 034: | Loss: 0.43231 | Acc: 82.290\n",
            "Epoch 035: | Loss: 0.42952 | Acc: 82.233\n",
            "Epoch 036: | Loss: 0.43031 | Acc: 82.335\n",
            "Epoch 037: | Loss: 0.42935 | Acc: 82.310\n",
            "Epoch 038: | Loss: 0.42459 | Acc: 82.722\n",
            "Epoch 039: | Loss: 0.42677 | Acc: 82.584\n",
            "Epoch 040: | Loss: 0.42512 | Acc: 82.482\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88      3187\n",
            "           1       0.32      0.11      0.16       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.57      0.53      0.52      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "3\n",
            "______________________________\n",
            "\n",
            "17\n",
            "______________________________\n",
            "18 17\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 18 columns]\n",
            "0.754274049502424\n",
            "Number of mislabeled points out of a total 3919 points : 1040\n",
            "0.734626180147997\n",
            "correct_ones 245\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=17, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48706 | Acc: 80.131\n",
            "Epoch 002: | Loss: 0.46671 | Acc: 81.408\n",
            "Epoch 003: | Loss: 0.46275 | Acc: 81.469\n",
            "Epoch 004: | Loss: 0.45890 | Acc: 81.429\n",
            "Epoch 005: | Loss: 0.45888 | Acc: 81.420\n",
            "Epoch 006: | Loss: 0.45862 | Acc: 81.457\n",
            "Epoch 007: | Loss: 0.45602 | Acc: 81.465\n",
            "Epoch 008: | Loss: 0.45519 | Acc: 81.482\n",
            "Epoch 009: | Loss: 0.45581 | Acc: 81.498\n",
            "Epoch 010: | Loss: 0.45402 | Acc: 81.527\n",
            "Epoch 011: | Loss: 0.45403 | Acc: 81.506\n",
            "Epoch 012: | Loss: 0.45130 | Acc: 81.490\n",
            "Epoch 013: | Loss: 0.45096 | Acc: 81.502\n",
            "Epoch 014: | Loss: 0.44931 | Acc: 81.580\n",
            "Epoch 015: | Loss: 0.44977 | Acc: 81.547\n",
            "Epoch 016: | Loss: 0.44897 | Acc: 81.624\n",
            "Epoch 017: | Loss: 0.44756 | Acc: 81.673\n",
            "Epoch 018: | Loss: 0.44783 | Acc: 81.600\n",
            "Epoch 019: | Loss: 0.44420 | Acc: 81.620\n",
            "Epoch 020: | Loss: 0.44588 | Acc: 81.641\n",
            "Epoch 021: | Loss: 0.44334 | Acc: 81.780\n",
            "Epoch 022: | Loss: 0.44432 | Acc: 81.686\n",
            "Epoch 023: | Loss: 0.44301 | Acc: 81.767\n",
            "Epoch 024: | Loss: 0.44090 | Acc: 81.992\n",
            "Epoch 025: | Loss: 0.44206 | Acc: 81.894\n",
            "Epoch 026: | Loss: 0.44168 | Acc: 81.841\n",
            "Epoch 027: | Loss: 0.44105 | Acc: 81.910\n",
            "Epoch 028: | Loss: 0.43779 | Acc: 82.000\n",
            "Epoch 029: | Loss: 0.43787 | Acc: 81.971\n",
            "Epoch 030: | Loss: 0.43694 | Acc: 81.853\n",
            "Epoch 031: | Loss: 0.43523 | Acc: 82.053\n",
            "Epoch 032: | Loss: 0.43439 | Acc: 82.176\n",
            "Epoch 033: | Loss: 0.43375 | Acc: 82.237\n",
            "Epoch 034: | Loss: 0.43076 | Acc: 82.233\n",
            "Epoch 035: | Loss: 0.43252 | Acc: 82.212\n",
            "Epoch 036: | Loss: 0.43053 | Acc: 82.269\n",
            "Epoch 037: | Loss: 0.42850 | Acc: 82.588\n",
            "Epoch 038: | Loss: 0.42848 | Acc: 82.404\n",
            "Epoch 039: | Loss: 0.42884 | Acc: 82.543\n",
            "Epoch 040: | Loss: 0.42836 | Acc: 82.531\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.89      3187\n",
            "           1       0.32      0.08      0.12       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.57      0.52      0.50      3919\n",
            "weighted avg       0.73      0.80      0.74      3919\n",
            "\n",
            "3\n",
            "______________________________\n",
            "\n",
            "18\n",
            "______________________________\n",
            "19 18\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 19 columns]\n",
            "0.7545292166368972\n",
            "Number of mislabeled points out of a total 3919 points : 1051\n",
            "0.731819341668793\n",
            "correct_ones 244\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=18, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48395 | Acc: 80.955\n",
            "Epoch 002: | Loss: 0.46431 | Acc: 81.429\n",
            "Epoch 003: | Loss: 0.46180 | Acc: 81.437\n",
            "Epoch 004: | Loss: 0.45905 | Acc: 81.486\n",
            "Epoch 005: | Loss: 0.45766 | Acc: 81.453\n",
            "Epoch 006: | Loss: 0.45483 | Acc: 81.469\n",
            "Epoch 007: | Loss: 0.45561 | Acc: 81.412\n",
            "Epoch 008: | Loss: 0.45556 | Acc: 81.420\n",
            "Epoch 009: | Loss: 0.45251 | Acc: 81.490\n",
            "Epoch 010: | Loss: 0.45170 | Acc: 81.473\n",
            "Epoch 011: | Loss: 0.45154 | Acc: 81.522\n",
            "Epoch 012: | Loss: 0.44981 | Acc: 81.629\n",
            "Epoch 013: | Loss: 0.44957 | Acc: 81.633\n",
            "Epoch 014: | Loss: 0.44841 | Acc: 81.437\n",
            "Epoch 015: | Loss: 0.44778 | Acc: 81.706\n",
            "Epoch 016: | Loss: 0.44570 | Acc: 81.727\n",
            "Epoch 017: | Loss: 0.44505 | Acc: 81.645\n",
            "Epoch 018: | Loss: 0.44483 | Acc: 81.910\n",
            "Epoch 019: | Loss: 0.44428 | Acc: 81.939\n",
            "Epoch 020: | Loss: 0.44317 | Acc: 81.878\n",
            "Epoch 021: | Loss: 0.44409 | Acc: 81.800\n",
            "Epoch 022: | Loss: 0.44202 | Acc: 81.820\n",
            "Epoch 023: | Loss: 0.44132 | Acc: 81.837\n",
            "Epoch 024: | Loss: 0.43920 | Acc: 81.865\n",
            "Epoch 025: | Loss: 0.43927 | Acc: 81.971\n",
            "Epoch 026: | Loss: 0.43729 | Acc: 82.008\n",
            "Epoch 027: | Loss: 0.43649 | Acc: 82.004\n",
            "Epoch 028: | Loss: 0.43669 | Acc: 82.110\n",
            "Epoch 029: | Loss: 0.43530 | Acc: 82.412\n",
            "Epoch 030: | Loss: 0.43233 | Acc: 82.151\n",
            "Epoch 031: | Loss: 0.43201 | Acc: 82.220\n",
            "Epoch 032: | Loss: 0.43268 | Acc: 82.412\n",
            "Epoch 033: | Loss: 0.43121 | Acc: 82.494\n",
            "Epoch 034: | Loss: 0.43225 | Acc: 82.200\n",
            "Epoch 035: | Loss: 0.42876 | Acc: 82.412\n",
            "Epoch 036: | Loss: 0.42785 | Acc: 82.571\n",
            "Epoch 037: | Loss: 0.42808 | Acc: 82.478\n",
            "Epoch 038: | Loss: 0.42807 | Acc: 82.461\n",
            "Epoch 039: | Loss: 0.42352 | Acc: 82.641\n",
            "Epoch 040: | Loss: 0.42529 | Acc: 82.539\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      3187\n",
            "           1       0.30      0.06      0.10       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.56      0.51      0.49      3919\n",
            "weighted avg       0.72      0.80      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "19\n",
            "______________________________\n",
            "20 19\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 20 columns]\n",
            "0.7575912222505742\n",
            "Number of mislabeled points out of a total 3919 points : 1064\n",
            "0.728502168920643\n",
            "correct_ones 248\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=19, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.47956 | Acc: 81.437\n",
            "Epoch 002: | Loss: 0.46577 | Acc: 81.453\n",
            "Epoch 003: | Loss: 0.45890 | Acc: 81.433\n",
            "Epoch 004: | Loss: 0.45759 | Acc: 81.461\n",
            "Epoch 005: | Loss: 0.45586 | Acc: 81.494\n",
            "Epoch 006: | Loss: 0.45601 | Acc: 81.400\n",
            "Epoch 007: | Loss: 0.45438 | Acc: 81.522\n",
            "Epoch 008: | Loss: 0.45187 | Acc: 81.441\n",
            "Epoch 009: | Loss: 0.45346 | Acc: 81.608\n",
            "Epoch 010: | Loss: 0.44880 | Acc: 81.547\n",
            "Epoch 011: | Loss: 0.44974 | Acc: 81.620\n",
            "Epoch 012: | Loss: 0.44826 | Acc: 81.510\n",
            "Epoch 013: | Loss: 0.44762 | Acc: 81.433\n",
            "Epoch 014: | Loss: 0.44641 | Acc: 81.608\n",
            "Epoch 015: | Loss: 0.44723 | Acc: 81.551\n",
            "Epoch 016: | Loss: 0.44656 | Acc: 81.620\n",
            "Epoch 017: | Loss: 0.44457 | Acc: 81.604\n",
            "Epoch 018: | Loss: 0.44340 | Acc: 81.706\n",
            "Epoch 019: | Loss: 0.44278 | Acc: 81.816\n",
            "Epoch 020: | Loss: 0.44217 | Acc: 81.784\n",
            "Epoch 021: | Loss: 0.43998 | Acc: 81.747\n",
            "Epoch 022: | Loss: 0.44048 | Acc: 81.780\n",
            "Epoch 023: | Loss: 0.43969 | Acc: 81.755\n",
            "Epoch 024: | Loss: 0.43694 | Acc: 81.747\n",
            "Epoch 025: | Loss: 0.43728 | Acc: 81.865\n",
            "Epoch 026: | Loss: 0.43541 | Acc: 82.045\n",
            "Epoch 027: | Loss: 0.43436 | Acc: 82.061\n",
            "Epoch 028: | Loss: 0.43398 | Acc: 81.984\n",
            "Epoch 029: | Loss: 0.43205 | Acc: 81.816\n",
            "Epoch 030: | Loss: 0.43083 | Acc: 82.110\n",
            "Epoch 031: | Loss: 0.42996 | Acc: 82.204\n",
            "Epoch 032: | Loss: 0.42931 | Acc: 82.253\n",
            "Epoch 033: | Loss: 0.42858 | Acc: 82.061\n",
            "Epoch 034: | Loss: 0.42740 | Acc: 82.306\n",
            "Epoch 035: | Loss: 0.42483 | Acc: 82.294\n",
            "Epoch 036: | Loss: 0.42625 | Acc: 82.359\n",
            "Epoch 037: | Loss: 0.42322 | Acc: 82.465\n",
            "Epoch 038: | Loss: 0.42342 | Acc: 82.371\n",
            "Epoch 039: | Loss: 0.42302 | Acc: 82.371\n",
            "Epoch 040: | Loss: 0.42147 | Acc: 82.384\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      3187\n",
            "           1       0.33      0.06      0.10       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.57      0.52      0.49      3919\n",
            "weighted avg       0.73      0.80      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "20\n",
            "______________________________\n",
            "21 20\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 21 columns]\n",
            "0.7565705537126818\n",
            "Number of mislabeled points out of a total 3919 points : 1147\n",
            "0.7073232967593774\n",
            "correct_ones 274\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=20, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48366 | Acc: 81.408\n",
            "Epoch 002: | Loss: 0.46343 | Acc: 81.453\n",
            "Epoch 003: | Loss: 0.46327 | Acc: 81.429\n",
            "Epoch 004: | Loss: 0.45922 | Acc: 81.473\n",
            "Epoch 005: | Loss: 0.45703 | Acc: 81.449\n",
            "Epoch 006: | Loss: 0.45646 | Acc: 81.429\n",
            "Epoch 007: | Loss: 0.45457 | Acc: 81.437\n",
            "Epoch 008: | Loss: 0.45338 | Acc: 81.453\n",
            "Epoch 009: | Loss: 0.45237 | Acc: 81.461\n",
            "Epoch 010: | Loss: 0.45075 | Acc: 81.335\n",
            "Epoch 011: | Loss: 0.44960 | Acc: 81.514\n",
            "Epoch 012: | Loss: 0.45051 | Acc: 81.522\n",
            "Epoch 013: | Loss: 0.44799 | Acc: 81.424\n",
            "Epoch 014: | Loss: 0.44653 | Acc: 81.482\n",
            "Epoch 015: | Loss: 0.44658 | Acc: 81.433\n",
            "Epoch 016: | Loss: 0.44607 | Acc: 81.469\n",
            "Epoch 017: | Loss: 0.44476 | Acc: 81.498\n",
            "Epoch 018: | Loss: 0.44347 | Acc: 81.465\n",
            "Epoch 019: | Loss: 0.44264 | Acc: 81.527\n",
            "Epoch 020: | Loss: 0.44290 | Acc: 81.751\n",
            "Epoch 021: | Loss: 0.43963 | Acc: 81.743\n",
            "Epoch 022: | Loss: 0.44189 | Acc: 81.669\n",
            "Epoch 023: | Loss: 0.43799 | Acc: 81.902\n",
            "Epoch 024: | Loss: 0.43766 | Acc: 81.812\n",
            "Epoch 025: | Loss: 0.43813 | Acc: 81.837\n",
            "Epoch 026: | Loss: 0.43738 | Acc: 81.833\n",
            "Epoch 027: | Loss: 0.43268 | Acc: 82.004\n",
            "Epoch 028: | Loss: 0.43257 | Acc: 81.947\n",
            "Epoch 029: | Loss: 0.43287 | Acc: 81.996\n",
            "Epoch 030: | Loss: 0.43382 | Acc: 81.992\n",
            "Epoch 031: | Loss: 0.42693 | Acc: 82.359\n",
            "Epoch 032: | Loss: 0.42956 | Acc: 82.306\n",
            "Epoch 033: | Loss: 0.42753 | Acc: 82.233\n",
            "Epoch 034: | Loss: 0.42682 | Acc: 82.204\n",
            "Epoch 035: | Loss: 0.42663 | Acc: 82.269\n",
            "Epoch 036: | Loss: 0.42414 | Acc: 82.273\n",
            "Epoch 037: | Loss: 0.42178 | Acc: 82.433\n",
            "Epoch 038: | Loss: 0.42362 | Acc: 82.433\n",
            "Epoch 039: | Loss: 0.42171 | Acc: 82.502\n",
            "Epoch 040: | Loss: 0.41925 | Acc: 82.820\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      3187\n",
            "           1       0.38      0.07      0.11       732\n",
            "\n",
            "    accuracy                           0.81      3919\n",
            "   macro avg       0.60      0.52      0.50      3919\n",
            "weighted avg       0.74      0.81      0.75      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "21\n",
            "______________________________\n",
            "22 21\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 22 columns]\n",
            "0.7560602194437357\n",
            "Number of mislabeled points out of a total 3919 points : 1321\n",
            "0.6629242153610615\n",
            "correct_ones 337\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=21, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48589 | Acc: 80.629\n",
            "Epoch 002: | Loss: 0.46734 | Acc: 81.473\n",
            "Epoch 003: | Loss: 0.45929 | Acc: 81.449\n",
            "Epoch 004: | Loss: 0.45976 | Acc: 81.465\n",
            "Epoch 005: | Loss: 0.45831 | Acc: 81.494\n",
            "Epoch 006: | Loss: 0.45372 | Acc: 81.449\n",
            "Epoch 007: | Loss: 0.45382 | Acc: 81.461\n",
            "Epoch 008: | Loss: 0.45177 | Acc: 81.429\n",
            "Epoch 009: | Loss: 0.45034 | Acc: 81.445\n",
            "Epoch 010: | Loss: 0.45143 | Acc: 81.563\n",
            "Epoch 011: | Loss: 0.44824 | Acc: 81.596\n",
            "Epoch 012: | Loss: 0.44689 | Acc: 81.657\n",
            "Epoch 013: | Loss: 0.44756 | Acc: 81.739\n",
            "Epoch 014: | Loss: 0.44501 | Acc: 81.592\n",
            "Epoch 015: | Loss: 0.44411 | Acc: 81.824\n",
            "Epoch 016: | Loss: 0.44333 | Acc: 81.841\n",
            "Epoch 017: | Loss: 0.44253 | Acc: 81.849\n",
            "Epoch 018: | Loss: 0.43923 | Acc: 81.922\n",
            "Epoch 019: | Loss: 0.43990 | Acc: 81.686\n",
            "Epoch 020: | Loss: 0.43855 | Acc: 81.976\n",
            "Epoch 021: | Loss: 0.43805 | Acc: 81.935\n",
            "Epoch 022: | Loss: 0.43513 | Acc: 81.943\n",
            "Epoch 023: | Loss: 0.43642 | Acc: 81.865\n",
            "Epoch 024: | Loss: 0.43285 | Acc: 81.963\n",
            "Epoch 025: | Loss: 0.43031 | Acc: 81.878\n",
            "Epoch 026: | Loss: 0.43149 | Acc: 81.984\n",
            "Epoch 027: | Loss: 0.42862 | Acc: 82.216\n",
            "Epoch 028: | Loss: 0.42970 | Acc: 82.131\n",
            "Epoch 029: | Loss: 0.42865 | Acc: 82.290\n",
            "Epoch 030: | Loss: 0.42725 | Acc: 82.233\n",
            "Epoch 031: | Loss: 0.42421 | Acc: 82.286\n",
            "Epoch 032: | Loss: 0.42188 | Acc: 82.388\n",
            "Epoch 033: | Loss: 0.42225 | Acc: 82.147\n",
            "Epoch 034: | Loss: 0.42171 | Acc: 82.502\n",
            "Epoch 035: | Loss: 0.41968 | Acc: 82.624\n",
            "Epoch 036: | Loss: 0.41958 | Acc: 82.567\n",
            "Epoch 037: | Loss: 0.41445 | Acc: 82.898\n",
            "Epoch 038: | Loss: 0.41517 | Acc: 82.645\n",
            "Epoch 039: | Loss: 0.41567 | Acc: 82.649\n",
            "Epoch 040: | Loss: 0.41427 | Acc: 82.804\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.89      3187\n",
            "           1       0.30      0.06      0.11       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.56      0.51      0.50      3919\n",
            "weighted avg       0.72      0.80      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "22\n",
            "______________________________\n",
            "23 22\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 23 columns]\n",
            "0.7545292166368972\n",
            "Number of mislabeled points out of a total 3919 points : 1326\n",
            "0.661648379688696\n",
            "correct_ones 342\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=22, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48620 | Acc: 81.073\n",
            "Epoch 002: | Loss: 0.46640 | Acc: 81.437\n",
            "Epoch 003: | Loss: 0.46113 | Acc: 81.449\n",
            "Epoch 004: | Loss: 0.45809 | Acc: 81.478\n",
            "Epoch 005: | Loss: 0.45640 | Acc: 81.478\n",
            "Epoch 006: | Loss: 0.45382 | Acc: 81.482\n",
            "Epoch 007: | Loss: 0.45390 | Acc: 81.433\n",
            "Epoch 008: | Loss: 0.45104 | Acc: 81.408\n",
            "Epoch 009: | Loss: 0.45073 | Acc: 81.437\n",
            "Epoch 010: | Loss: 0.44927 | Acc: 81.437\n",
            "Epoch 011: | Loss: 0.44975 | Acc: 81.457\n",
            "Epoch 012: | Loss: 0.44638 | Acc: 81.506\n",
            "Epoch 013: | Loss: 0.44509 | Acc: 81.449\n",
            "Epoch 014: | Loss: 0.44540 | Acc: 81.396\n",
            "Epoch 015: | Loss: 0.44312 | Acc: 81.584\n",
            "Epoch 016: | Loss: 0.44361 | Acc: 81.596\n",
            "Epoch 017: | Loss: 0.44259 | Acc: 81.620\n",
            "Epoch 018: | Loss: 0.43773 | Acc: 81.641\n",
            "Epoch 019: | Loss: 0.43742 | Acc: 81.714\n",
            "Epoch 020: | Loss: 0.43549 | Acc: 81.735\n",
            "Epoch 021: | Loss: 0.43367 | Acc: 81.853\n",
            "Epoch 022: | Loss: 0.43410 | Acc: 81.792\n",
            "Epoch 023: | Loss: 0.43266 | Acc: 81.833\n",
            "Epoch 024: | Loss: 0.43239 | Acc: 81.988\n",
            "Epoch 025: | Loss: 0.43007 | Acc: 81.914\n",
            "Epoch 026: | Loss: 0.42901 | Acc: 81.927\n",
            "Epoch 027: | Loss: 0.42776 | Acc: 82.155\n",
            "Epoch 028: | Loss: 0.42401 | Acc: 82.237\n",
            "Epoch 029: | Loss: 0.42342 | Acc: 82.159\n",
            "Epoch 030: | Loss: 0.42227 | Acc: 82.090\n",
            "Epoch 031: | Loss: 0.42127 | Acc: 82.200\n",
            "Epoch 032: | Loss: 0.41588 | Acc: 82.539\n",
            "Epoch 033: | Loss: 0.41892 | Acc: 82.367\n",
            "Epoch 034: | Loss: 0.41657 | Acc: 82.371\n",
            "Epoch 035: | Loss: 0.41552 | Acc: 82.800\n",
            "Epoch 036: | Loss: 0.41447 | Acc: 82.518\n",
            "Epoch 037: | Loss: 0.41386 | Acc: 82.661\n",
            "Epoch 038: | Loss: 0.41393 | Acc: 82.710\n",
            "Epoch 039: | Loss: 0.41055 | Acc: 82.714\n",
            "Epoch 040: | Loss: 0.40877 | Acc: 82.747\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      3187\n",
            "           1       0.34      0.07      0.12       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.58      0.52      0.50      3919\n",
            "weighted avg       0.73      0.80      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "23\n",
            "______________________________\n",
            "24 23\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 24 columns]\n",
            "0.7545292166368972\n",
            "Number of mislabeled points out of a total 3919 points : 1531\n",
            "0.6093391171217147\n",
            "correct_ones 397\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=23, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48894 | Acc: 80.559\n",
            "Epoch 002: | Loss: 0.46502 | Acc: 81.473\n",
            "Epoch 003: | Loss: 0.46118 | Acc: 81.469\n",
            "Epoch 004: | Loss: 0.45716 | Acc: 81.461\n",
            "Epoch 005: | Loss: 0.45558 | Acc: 81.486\n",
            "Epoch 006: | Loss: 0.45552 | Acc: 81.461\n",
            "Epoch 007: | Loss: 0.45286 | Acc: 81.490\n",
            "Epoch 008: | Loss: 0.45114 | Acc: 81.539\n",
            "Epoch 009: | Loss: 0.44994 | Acc: 81.486\n",
            "Epoch 010: | Loss: 0.45094 | Acc: 81.518\n",
            "Epoch 011: | Loss: 0.44785 | Acc: 81.596\n",
            "Epoch 012: | Loss: 0.44597 | Acc: 81.624\n",
            "Epoch 013: | Loss: 0.44471 | Acc: 81.527\n",
            "Epoch 014: | Loss: 0.44410 | Acc: 81.555\n",
            "Epoch 015: | Loss: 0.44225 | Acc: 81.747\n",
            "Epoch 016: | Loss: 0.43968 | Acc: 81.710\n",
            "Epoch 017: | Loss: 0.44129 | Acc: 81.690\n",
            "Epoch 018: | Loss: 0.43926 | Acc: 81.788\n",
            "Epoch 019: | Loss: 0.43721 | Acc: 81.845\n",
            "Epoch 020: | Loss: 0.43668 | Acc: 81.784\n",
            "Epoch 021: | Loss: 0.43351 | Acc: 82.078\n",
            "Epoch 022: | Loss: 0.43470 | Acc: 81.906\n",
            "Epoch 023: | Loss: 0.43265 | Acc: 81.939\n",
            "Epoch 024: | Loss: 0.43143 | Acc: 82.102\n",
            "Epoch 025: | Loss: 0.42894 | Acc: 82.069\n",
            "Epoch 026: | Loss: 0.42867 | Acc: 82.220\n",
            "Epoch 027: | Loss: 0.42503 | Acc: 82.216\n",
            "Epoch 028: | Loss: 0.42456 | Acc: 82.412\n",
            "Epoch 029: | Loss: 0.42235 | Acc: 82.371\n",
            "Epoch 030: | Loss: 0.42511 | Acc: 82.098\n",
            "Epoch 031: | Loss: 0.42099 | Acc: 82.335\n",
            "Epoch 032: | Loss: 0.42146 | Acc: 82.449\n",
            "Epoch 033: | Loss: 0.41765 | Acc: 82.498\n",
            "Epoch 034: | Loss: 0.41592 | Acc: 82.478\n",
            "Epoch 035: | Loss: 0.41704 | Acc: 82.506\n",
            "Epoch 036: | Loss: 0.41375 | Acc: 82.649\n",
            "Epoch 037: | Loss: 0.41264 | Acc: 82.714\n",
            "Epoch 038: | Loss: 0.41396 | Acc: 82.686\n",
            "Epoch 039: | Loss: 0.41169 | Acc: 82.747\n",
            "Epoch 040: | Loss: 0.40958 | Acc: 82.976\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.89      3187\n",
            "           1       0.36      0.09      0.14       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.59      0.53      0.51      3919\n",
            "weighted avg       0.73      0.80      0.75      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "24\n",
            "______________________________\n",
            "25 24\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 25 columns]\n",
            "0.7547843837713702\n",
            "Number of mislabeled points out of a total 3919 points : 1291\n",
            "0.6705792293952539\n",
            "correct_ones 331\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=24, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.47921 | Acc: 81.376\n",
            "Epoch 002: | Loss: 0.46382 | Acc: 81.465\n",
            "Epoch 003: | Loss: 0.46117 | Acc: 81.437\n",
            "Epoch 004: | Loss: 0.45777 | Acc: 81.486\n",
            "Epoch 005: | Loss: 0.45775 | Acc: 81.461\n",
            "Epoch 006: | Loss: 0.45569 | Acc: 81.490\n",
            "Epoch 007: | Loss: 0.45486 | Acc: 81.469\n",
            "Epoch 008: | Loss: 0.45268 | Acc: 81.502\n",
            "Epoch 009: | Loss: 0.45244 | Acc: 81.514\n",
            "Epoch 010: | Loss: 0.44793 | Acc: 81.457\n",
            "Epoch 011: | Loss: 0.44756 | Acc: 81.527\n",
            "Epoch 012: | Loss: 0.44762 | Acc: 81.559\n",
            "Epoch 013: | Loss: 0.44569 | Acc: 81.620\n",
            "Epoch 014: | Loss: 0.44529 | Acc: 81.673\n",
            "Epoch 015: | Loss: 0.44424 | Acc: 81.682\n",
            "Epoch 016: | Loss: 0.44424 | Acc: 81.592\n",
            "Epoch 017: | Loss: 0.44226 | Acc: 81.661\n",
            "Epoch 018: | Loss: 0.44264 | Acc: 81.763\n",
            "Epoch 019: | Loss: 0.43950 | Acc: 81.747\n",
            "Epoch 020: | Loss: 0.43927 | Acc: 81.616\n",
            "Epoch 021: | Loss: 0.43724 | Acc: 81.800\n",
            "Epoch 022: | Loss: 0.43428 | Acc: 81.808\n",
            "Epoch 023: | Loss: 0.43511 | Acc: 81.955\n",
            "Epoch 024: | Loss: 0.43429 | Acc: 81.967\n",
            "Epoch 025: | Loss: 0.43276 | Acc: 81.984\n",
            "Epoch 026: | Loss: 0.43189 | Acc: 82.037\n",
            "Epoch 027: | Loss: 0.42788 | Acc: 82.155\n",
            "Epoch 028: | Loss: 0.42523 | Acc: 82.090\n",
            "Epoch 029: | Loss: 0.42763 | Acc: 82.151\n",
            "Epoch 030: | Loss: 0.42550 | Acc: 82.298\n",
            "Epoch 031: | Loss: 0.42227 | Acc: 82.180\n",
            "Epoch 032: | Loss: 0.42240 | Acc: 82.176\n",
            "Epoch 033: | Loss: 0.42019 | Acc: 82.453\n",
            "Epoch 034: | Loss: 0.41944 | Acc: 82.620\n",
            "Epoch 035: | Loss: 0.41646 | Acc: 82.367\n",
            "Epoch 036: | Loss: 0.41706 | Acc: 82.665\n",
            "Epoch 037: | Loss: 0.41351 | Acc: 82.486\n",
            "Epoch 038: | Loss: 0.41525 | Acc: 82.612\n",
            "Epoch 039: | Loss: 0.41507 | Acc: 82.698\n",
            "Epoch 040: | Loss: 0.41157 | Acc: 82.604\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      3187\n",
            "           1       0.39      0.08      0.13       732\n",
            "\n",
            "    accuracy                           0.81      3919\n",
            "   macro avg       0.61      0.53      0.51      3919\n",
            "weighted avg       0.74      0.81      0.75      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "25\n",
            "______________________________\n",
            "26 25\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 26 columns]\n",
            "0.7550395509058433\n",
            "Number of mislabeled points out of a total 3919 points : 1163\n",
            "0.7032406226078081\n",
            "correct_ones 292\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=25, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48423 | Acc: 81.306\n",
            "Epoch 002: | Loss: 0.46375 | Acc: 81.461\n",
            "Epoch 003: | Loss: 0.45991 | Acc: 81.457\n",
            "Epoch 004: | Loss: 0.45709 | Acc: 81.449\n",
            "Epoch 005: | Loss: 0.45736 | Acc: 81.433\n",
            "Epoch 006: | Loss: 0.45602 | Acc: 81.469\n",
            "Epoch 007: | Loss: 0.45340 | Acc: 81.445\n",
            "Epoch 008: | Loss: 0.45271 | Acc: 81.494\n",
            "Epoch 009: | Loss: 0.45201 | Acc: 81.416\n",
            "Epoch 010: | Loss: 0.44998 | Acc: 81.465\n",
            "Epoch 011: | Loss: 0.44940 | Acc: 81.629\n",
            "Epoch 012: | Loss: 0.44617 | Acc: 81.608\n",
            "Epoch 013: | Loss: 0.44635 | Acc: 81.604\n",
            "Epoch 014: | Loss: 0.44408 | Acc: 81.771\n",
            "Epoch 015: | Loss: 0.44316 | Acc: 81.784\n",
            "Epoch 016: | Loss: 0.44001 | Acc: 81.747\n",
            "Epoch 017: | Loss: 0.44030 | Acc: 81.780\n",
            "Epoch 018: | Loss: 0.43886 | Acc: 82.000\n",
            "Epoch 019: | Loss: 0.43942 | Acc: 81.882\n",
            "Epoch 020: | Loss: 0.43661 | Acc: 81.963\n",
            "Epoch 021: | Loss: 0.43461 | Acc: 82.053\n",
            "Epoch 022: | Loss: 0.43241 | Acc: 82.110\n",
            "Epoch 023: | Loss: 0.43317 | Acc: 82.016\n",
            "Epoch 024: | Loss: 0.43321 | Acc: 82.053\n",
            "Epoch 025: | Loss: 0.42871 | Acc: 82.408\n",
            "Epoch 026: | Loss: 0.42605 | Acc: 82.396\n",
            "Epoch 027: | Loss: 0.42848 | Acc: 82.049\n",
            "Epoch 028: | Loss: 0.42368 | Acc: 82.437\n",
            "Epoch 029: | Loss: 0.42172 | Acc: 82.522\n",
            "Epoch 030: | Loss: 0.42380 | Acc: 82.388\n",
            "Epoch 031: | Loss: 0.42134 | Acc: 82.449\n",
            "Epoch 032: | Loss: 0.41858 | Acc: 82.767\n",
            "Epoch 033: | Loss: 0.41858 | Acc: 82.286\n",
            "Epoch 034: | Loss: 0.41682 | Acc: 82.649\n",
            "Epoch 035: | Loss: 0.41290 | Acc: 82.776\n",
            "Epoch 036: | Loss: 0.41304 | Acc: 82.857\n",
            "Epoch 037: | Loss: 0.41119 | Acc: 83.008\n",
            "Epoch 038: | Loss: 0.41065 | Acc: 82.976\n",
            "Epoch 039: | Loss: 0.41170 | Acc: 82.767\n",
            "Epoch 040: | Loss: 0.40800 | Acc: 83.033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88      3187\n",
            "           1       0.28      0.10      0.14       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.55      0.52      0.51      3919\n",
            "weighted avg       0.72      0.79      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "26\n",
            "______________________________\n",
            "27 26\n",
            "       flag_top_ed_spender  ...  profitable_flag\n",
            "0                -1.061005  ...                1\n",
            "1                 0.942502  ...                1\n",
            "2                 0.942502  ...                1\n",
            "3                 0.942502  ...                1\n",
            "4                -1.061005  ...                1\n",
            "...                    ...  ...              ...\n",
            "19590            -1.061005  ...                0\n",
            "19591             0.942502  ...                0\n",
            "19592            -1.061005  ...                0\n",
            "19593            -1.061005  ...                0\n",
            "19594            -1.061005  ...                0\n",
            "\n",
            "[19595 rows x 27 columns]\n",
            "0.7535085480990048\n",
            "Number of mislabeled points out of a total 3919 points : 1167\n",
            "0.7022199540699158\n",
            "correct_ones 292\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=26, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48259 | Acc: 81.302\n",
            "Epoch 002: | Loss: 0.46143 | Acc: 81.453\n",
            "Epoch 003: | Loss: 0.45875 | Acc: 81.473\n",
            "Epoch 004: | Loss: 0.45807 | Acc: 81.478\n",
            "Epoch 005: | Loss: 0.45482 | Acc: 81.457\n",
            "Epoch 006: | Loss: 0.45304 | Acc: 81.461\n",
            "Epoch 007: | Loss: 0.45194 | Acc: 81.473\n",
            "Epoch 008: | Loss: 0.45005 | Acc: 81.486\n",
            "Epoch 009: | Loss: 0.44872 | Acc: 81.441\n",
            "Epoch 010: | Loss: 0.44879 | Acc: 81.465\n",
            "Epoch 011: | Loss: 0.44557 | Acc: 81.518\n",
            "Epoch 012: | Loss: 0.44467 | Acc: 81.490\n",
            "Epoch 013: | Loss: 0.44378 | Acc: 81.510\n",
            "Epoch 014: | Loss: 0.44114 | Acc: 81.469\n",
            "Epoch 015: | Loss: 0.43981 | Acc: 81.673\n",
            "Epoch 016: | Loss: 0.43958 | Acc: 81.686\n",
            "Epoch 017: | Loss: 0.43835 | Acc: 81.673\n",
            "Epoch 018: | Loss: 0.43657 | Acc: 81.727\n",
            "Epoch 019: | Loss: 0.43050 | Acc: 81.894\n",
            "Epoch 020: | Loss: 0.43128 | Acc: 82.000\n",
            "Epoch 021: | Loss: 0.43290 | Acc: 81.820\n",
            "Epoch 022: | Loss: 0.42865 | Acc: 82.122\n",
            "Epoch 023: | Loss: 0.42674 | Acc: 82.073\n",
            "Epoch 024: | Loss: 0.42563 | Acc: 82.127\n",
            "Epoch 025: | Loss: 0.42482 | Acc: 82.012\n",
            "Epoch 026: | Loss: 0.42024 | Acc: 82.155\n",
            "Epoch 027: | Loss: 0.41874 | Acc: 82.176\n",
            "Epoch 028: | Loss: 0.41776 | Acc: 82.347\n",
            "Epoch 029: | Loss: 0.41807 | Acc: 82.339\n",
            "Epoch 030: | Loss: 0.41421 | Acc: 82.412\n",
            "Epoch 031: | Loss: 0.41398 | Acc: 82.310\n",
            "Epoch 032: | Loss: 0.41452 | Acc: 82.510\n",
            "Epoch 033: | Loss: 0.41134 | Acc: 82.559\n",
            "Epoch 034: | Loss: 0.41030 | Acc: 82.596\n",
            "Epoch 035: | Loss: 0.40367 | Acc: 82.947\n",
            "Epoch 036: | Loss: 0.40576 | Acc: 82.771\n",
            "Epoch 037: | Loss: 0.40839 | Acc: 82.735\n",
            "Epoch 038: | Loss: 0.40012 | Acc: 83.004\n",
            "Epoch 039: | Loss: 0.40003 | Acc: 83.265\n",
            "Epoch 040: | Loss: 0.40101 | Acc: 82.971\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88      3187\n",
            "           1       0.32      0.11      0.16       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.57      0.53      0.52      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "27\n",
            "______________________________\n",
            "28 27\n",
            "       count_accts  ...  profitable_flag\n",
            "0        -0.441299  ...                1\n",
            "1         1.497981  ...                1\n",
            "2        -0.441299  ...                1\n",
            "3         3.437261  ...                1\n",
            "4        -0.441299  ...                1\n",
            "...            ...  ...              ...\n",
            "19590    -0.441299  ...                0\n",
            "19591    -0.441299  ...                0\n",
            "19592    -0.441299  ...                0\n",
            "19593    -0.441299  ...                0\n",
            "19594    -0.441299  ...                0\n",
            "\n",
            "[19595 rows x 28 columns]\n",
            "0.7522327124266395\n",
            "Number of mislabeled points out of a total 3919 points : 1184\n",
            "0.6978821127838735\n",
            "correct_ones 289\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=27, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48047 | Acc: 81.318\n",
            "Epoch 002: | Loss: 0.46165 | Acc: 81.473\n",
            "Epoch 003: | Loss: 0.45898 | Acc: 81.457\n",
            "Epoch 004: | Loss: 0.45554 | Acc: 81.457\n",
            "Epoch 005: | Loss: 0.45470 | Acc: 81.453\n",
            "Epoch 006: | Loss: 0.45441 | Acc: 81.433\n",
            "Epoch 007: | Loss: 0.45163 | Acc: 81.510\n",
            "Epoch 008: | Loss: 0.45129 | Acc: 81.571\n",
            "Epoch 009: | Loss: 0.44793 | Acc: 81.457\n",
            "Epoch 010: | Loss: 0.44820 | Acc: 81.420\n",
            "Epoch 011: | Loss: 0.44655 | Acc: 81.441\n",
            "Epoch 012: | Loss: 0.44314 | Acc: 81.604\n",
            "Epoch 013: | Loss: 0.44220 | Acc: 81.612\n",
            "Epoch 014: | Loss: 0.44086 | Acc: 81.686\n",
            "Epoch 015: | Loss: 0.43929 | Acc: 81.657\n",
            "Epoch 016: | Loss: 0.43733 | Acc: 81.657\n",
            "Epoch 017: | Loss: 0.43574 | Acc: 81.984\n",
            "Epoch 018: | Loss: 0.43441 | Acc: 81.747\n",
            "Epoch 019: | Loss: 0.43282 | Acc: 81.841\n",
            "Epoch 020: | Loss: 0.42982 | Acc: 82.078\n",
            "Epoch 021: | Loss: 0.42894 | Acc: 82.000\n",
            "Epoch 022: | Loss: 0.42746 | Acc: 82.090\n",
            "Epoch 023: | Loss: 0.42597 | Acc: 82.224\n",
            "Epoch 024: | Loss: 0.42237 | Acc: 82.184\n",
            "Epoch 025: | Loss: 0.42235 | Acc: 82.294\n",
            "Epoch 026: | Loss: 0.42055 | Acc: 82.371\n",
            "Epoch 027: | Loss: 0.41903 | Acc: 82.490\n",
            "Epoch 028: | Loss: 0.41612 | Acc: 82.641\n",
            "Epoch 029: | Loss: 0.41526 | Acc: 82.514\n",
            "Epoch 030: | Loss: 0.41436 | Acc: 82.408\n",
            "Epoch 031: | Loss: 0.41092 | Acc: 82.710\n",
            "Epoch 032: | Loss: 0.41067 | Acc: 82.759\n",
            "Epoch 033: | Loss: 0.40861 | Acc: 82.882\n",
            "Epoch 034: | Loss: 0.40836 | Acc: 83.016\n",
            "Epoch 035: | Loss: 0.40491 | Acc: 82.886\n",
            "Epoch 036: | Loss: 0.40208 | Acc: 83.041\n",
            "Epoch 037: | Loss: 0.40224 | Acc: 83.090\n",
            "Epoch 038: | Loss: 0.40314 | Acc: 83.045\n",
            "Epoch 039: | Loss: 0.39785 | Acc: 83.196\n",
            "Epoch 040: | Loss: 0.39893 | Acc: 83.135\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88      3187\n",
            "           1       0.33      0.11      0.16       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.58      0.53      0.52      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "28\n",
            "______________________________\n",
            "29 28\n",
            "       count_accts  ...  profitable_flag\n",
            "0        -0.441299  ...                1\n",
            "1         1.497981  ...                1\n",
            "2        -0.441299  ...                1\n",
            "3         3.437261  ...                1\n",
            "4        -0.441299  ...                1\n",
            "...            ...  ...              ...\n",
            "19590    -0.441299  ...                0\n",
            "19591    -0.441299  ...                0\n",
            "19592    -0.441299  ...                0\n",
            "19593    -0.441299  ...                0\n",
            "19594    -0.441299  ...                0\n",
            "\n",
            "[19595 rows x 29 columns]\n",
            "0.7524878795611125\n",
            "Number of mislabeled points out of a total 3919 points : 1078\n",
            "0.72492982903802\n",
            "correct_ones 261\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=28, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48382 | Acc: 80.776\n",
            "Epoch 002: | Loss: 0.46408 | Acc: 81.473\n",
            "Epoch 003: | Loss: 0.46030 | Acc: 81.465\n",
            "Epoch 004: | Loss: 0.45543 | Acc: 81.457\n",
            "Epoch 005: | Loss: 0.45335 | Acc: 81.514\n",
            "Epoch 006: | Loss: 0.45366 | Acc: 81.539\n",
            "Epoch 007: | Loss: 0.44940 | Acc: 81.449\n",
            "Epoch 008: | Loss: 0.45057 | Acc: 81.490\n",
            "Epoch 009: | Loss: 0.44832 | Acc: 81.522\n",
            "Epoch 010: | Loss: 0.44594 | Acc: 81.649\n",
            "Epoch 011: | Loss: 0.44522 | Acc: 81.535\n",
            "Epoch 012: | Loss: 0.44439 | Acc: 81.608\n",
            "Epoch 013: | Loss: 0.44096 | Acc: 81.584\n",
            "Epoch 014: | Loss: 0.43921 | Acc: 81.584\n",
            "Epoch 015: | Loss: 0.43775 | Acc: 81.678\n",
            "Epoch 016: | Loss: 0.43560 | Acc: 81.657\n",
            "Epoch 017: | Loss: 0.43631 | Acc: 81.735\n",
            "Epoch 018: | Loss: 0.43167 | Acc: 82.069\n",
            "Epoch 019: | Loss: 0.43146 | Acc: 81.882\n",
            "Epoch 020: | Loss: 0.42824 | Acc: 82.176\n",
            "Epoch 021: | Loss: 0.42446 | Acc: 82.208\n",
            "Epoch 022: | Loss: 0.42621 | Acc: 82.200\n",
            "Epoch 023: | Loss: 0.42498 | Acc: 82.118\n",
            "Epoch 024: | Loss: 0.42172 | Acc: 82.257\n",
            "Epoch 025: | Loss: 0.41943 | Acc: 82.510\n",
            "Epoch 026: | Loss: 0.41762 | Acc: 82.531\n",
            "Epoch 027: | Loss: 0.41759 | Acc: 82.555\n",
            "Epoch 028: | Loss: 0.41374 | Acc: 82.649\n",
            "Epoch 029: | Loss: 0.41161 | Acc: 82.882\n",
            "Epoch 030: | Loss: 0.41089 | Acc: 82.816\n",
            "Epoch 031: | Loss: 0.40569 | Acc: 83.265\n",
            "Epoch 032: | Loss: 0.40720 | Acc: 82.894\n",
            "Epoch 033: | Loss: 0.40709 | Acc: 83.086\n",
            "Epoch 034: | Loss: 0.40390 | Acc: 83.118\n",
            "Epoch 035: | Loss: 0.40366 | Acc: 83.237\n",
            "Epoch 036: | Loss: 0.40292 | Acc: 83.367\n",
            "Epoch 037: | Loss: 0.39697 | Acc: 83.490\n",
            "Epoch 038: | Loss: 0.39623 | Acc: 83.486\n",
            "Epoch 039: | Loss: 0.39409 | Acc: 83.555\n",
            "Epoch 040: | Loss: 0.39790 | Acc: 83.355\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.88      3187\n",
            "           1       0.29      0.07      0.11       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.55      0.51      0.50      3919\n",
            "weighted avg       0.72      0.80      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "29\n",
            "______________________________\n",
            "30 29\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 30 columns]\n",
            "0.7532533809645318\n",
            "Number of mislabeled points out of a total 3919 points : 1079\n",
            "0.7246746619035468\n",
            "correct_ones 261\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=29, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48125 | Acc: 81.298\n",
            "Epoch 002: | Loss: 0.46411 | Acc: 81.437\n",
            "Epoch 003: | Loss: 0.46030 | Acc: 81.441\n",
            "Epoch 004: | Loss: 0.45638 | Acc: 81.449\n",
            "Epoch 005: | Loss: 0.45310 | Acc: 81.453\n",
            "Epoch 006: | Loss: 0.45350 | Acc: 81.461\n",
            "Epoch 007: | Loss: 0.45118 | Acc: 81.473\n",
            "Epoch 008: | Loss: 0.45064 | Acc: 81.473\n",
            "Epoch 009: | Loss: 0.44792 | Acc: 81.441\n",
            "Epoch 010: | Loss: 0.44529 | Acc: 81.473\n",
            "Epoch 011: | Loss: 0.44549 | Acc: 81.522\n",
            "Epoch 012: | Loss: 0.44273 | Acc: 81.743\n",
            "Epoch 013: | Loss: 0.44101 | Acc: 81.563\n",
            "Epoch 014: | Loss: 0.43960 | Acc: 81.731\n",
            "Epoch 015: | Loss: 0.43777 | Acc: 81.539\n",
            "Epoch 016: | Loss: 0.43619 | Acc: 81.784\n",
            "Epoch 017: | Loss: 0.43679 | Acc: 81.776\n",
            "Epoch 018: | Loss: 0.43273 | Acc: 81.755\n",
            "Epoch 019: | Loss: 0.43022 | Acc: 81.939\n",
            "Epoch 020: | Loss: 0.42806 | Acc: 82.200\n",
            "Epoch 021: | Loss: 0.42840 | Acc: 82.008\n",
            "Epoch 022: | Loss: 0.42658 | Acc: 82.118\n",
            "Epoch 023: | Loss: 0.42471 | Acc: 82.424\n",
            "Epoch 024: | Loss: 0.42148 | Acc: 82.306\n",
            "Epoch 025: | Loss: 0.42137 | Acc: 82.433\n",
            "Epoch 026: | Loss: 0.42037 | Acc: 82.629\n",
            "Epoch 027: | Loss: 0.41452 | Acc: 82.657\n",
            "Epoch 028: | Loss: 0.41351 | Acc: 82.690\n",
            "Epoch 029: | Loss: 0.41200 | Acc: 82.931\n",
            "Epoch 030: | Loss: 0.41148 | Acc: 82.702\n",
            "Epoch 031: | Loss: 0.40933 | Acc: 82.951\n",
            "Epoch 032: | Loss: 0.40767 | Acc: 82.922\n",
            "Epoch 033: | Loss: 0.40168 | Acc: 83.155\n",
            "Epoch 034: | Loss: 0.40232 | Acc: 83.478\n",
            "Epoch 035: | Loss: 0.40019 | Acc: 83.433\n",
            "Epoch 036: | Loss: 0.39872 | Acc: 83.563\n",
            "Epoch 037: | Loss: 0.39965 | Acc: 83.388\n",
            "Epoch 038: | Loss: 0.39850 | Acc: 83.331\n",
            "Epoch 039: | Loss: 0.39346 | Acc: 83.649\n",
            "Epoch 040: | Loss: 0.39688 | Acc: 83.482\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.88      3187\n",
            "           1       0.34      0.09      0.15       732\n",
            "\n",
            "    accuracy                           0.80      3919\n",
            "   macro avg       0.58      0.53      0.52      3919\n",
            "weighted avg       0.73      0.80      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "30\n",
            "______________________________\n",
            "31 30\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 31 columns]\n",
            "0.7519775452921663\n",
            "Number of mislabeled points out of a total 3919 points : 1090\n",
            "0.721867823424343\n",
            "correct_ones 260\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=30, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48375 | Acc: 81.208\n",
            "Epoch 002: | Loss: 0.46257 | Acc: 81.449\n",
            "Epoch 003: | Loss: 0.45888 | Acc: 81.461\n",
            "Epoch 004: | Loss: 0.45578 | Acc: 81.473\n",
            "Epoch 005: | Loss: 0.45438 | Acc: 81.478\n",
            "Epoch 006: | Loss: 0.45414 | Acc: 81.437\n",
            "Epoch 007: | Loss: 0.45216 | Acc: 81.461\n",
            "Epoch 008: | Loss: 0.44869 | Acc: 81.478\n",
            "Epoch 009: | Loss: 0.44980 | Acc: 81.576\n",
            "Epoch 010: | Loss: 0.44771 | Acc: 81.547\n",
            "Epoch 011: | Loss: 0.44406 | Acc: 81.633\n",
            "Epoch 012: | Loss: 0.44363 | Acc: 81.608\n",
            "Epoch 013: | Loss: 0.44310 | Acc: 81.620\n",
            "Epoch 014: | Loss: 0.44110 | Acc: 81.571\n",
            "Epoch 015: | Loss: 0.43871 | Acc: 81.706\n",
            "Epoch 016: | Loss: 0.43714 | Acc: 81.849\n",
            "Epoch 017: | Loss: 0.43597 | Acc: 81.747\n",
            "Epoch 018: | Loss: 0.43257 | Acc: 81.718\n",
            "Epoch 019: | Loss: 0.43261 | Acc: 81.906\n",
            "Epoch 020: | Loss: 0.42905 | Acc: 82.037\n",
            "Epoch 021: | Loss: 0.42917 | Acc: 82.229\n",
            "Epoch 022: | Loss: 0.42653 | Acc: 82.118\n",
            "Epoch 023: | Loss: 0.42436 | Acc: 82.318\n",
            "Epoch 024: | Loss: 0.42313 | Acc: 82.327\n",
            "Epoch 025: | Loss: 0.42117 | Acc: 82.531\n",
            "Epoch 026: | Loss: 0.42031 | Acc: 82.547\n",
            "Epoch 027: | Loss: 0.41707 | Acc: 82.596\n",
            "Epoch 028: | Loss: 0.41530 | Acc: 82.588\n",
            "Epoch 029: | Loss: 0.41069 | Acc: 82.898\n",
            "Epoch 030: | Loss: 0.41125 | Acc: 82.812\n",
            "Epoch 031: | Loss: 0.40813 | Acc: 83.016\n",
            "Epoch 032: | Loss: 0.40433 | Acc: 83.204\n",
            "Epoch 033: | Loss: 0.40486 | Acc: 83.139\n",
            "Epoch 034: | Loss: 0.40114 | Acc: 83.306\n",
            "Epoch 035: | Loss: 0.40262 | Acc: 83.212\n",
            "Epoch 036: | Loss: 0.39999 | Acc: 83.237\n",
            "Epoch 037: | Loss: 0.39837 | Acc: 83.453\n",
            "Epoch 038: | Loss: 0.39326 | Acc: 83.527\n",
            "Epoch 039: | Loss: 0.39336 | Acc: 83.424\n",
            "Epoch 040: | Loss: 0.39083 | Acc: 83.653\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88      3187\n",
            "           1       0.28      0.09      0.14       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.55      0.52      0.51      3919\n",
            "weighted avg       0.72      0.79      0.74      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "31\n",
            "______________________________\n",
            "32 31\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 32 columns]\n",
            "0.7519775452921663\n",
            "Number of mislabeled points out of a total 3919 points : 1116\n",
            "0.7152334779280428\n",
            "correct_ones 274\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=31, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48238 | Acc: 81.249\n",
            "Epoch 002: | Loss: 0.46328 | Acc: 81.449\n",
            "Epoch 003: | Loss: 0.45844 | Acc: 81.453\n",
            "Epoch 004: | Loss: 0.45679 | Acc: 81.441\n",
            "Epoch 005: | Loss: 0.45449 | Acc: 81.469\n",
            "Epoch 006: | Loss: 0.45407 | Acc: 81.429\n",
            "Epoch 007: | Loss: 0.45190 | Acc: 81.445\n",
            "Epoch 008: | Loss: 0.44615 | Acc: 81.527\n",
            "Epoch 009: | Loss: 0.44740 | Acc: 81.580\n",
            "Epoch 010: | Loss: 0.44529 | Acc: 81.584\n",
            "Epoch 011: | Loss: 0.44472 | Acc: 81.686\n",
            "Epoch 012: | Loss: 0.43963 | Acc: 81.739\n",
            "Epoch 013: | Loss: 0.43875 | Acc: 81.731\n",
            "Epoch 014: | Loss: 0.43614 | Acc: 81.792\n",
            "Epoch 015: | Loss: 0.43509 | Acc: 81.833\n",
            "Epoch 016: | Loss: 0.43111 | Acc: 81.841\n",
            "Epoch 017: | Loss: 0.42944 | Acc: 82.143\n",
            "Epoch 018: | Loss: 0.42910 | Acc: 81.886\n",
            "Epoch 019: | Loss: 0.42352 | Acc: 82.286\n",
            "Epoch 020: | Loss: 0.42337 | Acc: 82.139\n",
            "Epoch 021: | Loss: 0.42043 | Acc: 82.363\n",
            "Epoch 022: | Loss: 0.41721 | Acc: 82.502\n",
            "Epoch 023: | Loss: 0.41755 | Acc: 82.469\n",
            "Epoch 024: | Loss: 0.41399 | Acc: 82.567\n",
            "Epoch 025: | Loss: 0.41261 | Acc: 82.510\n",
            "Epoch 026: | Loss: 0.41097 | Acc: 82.829\n",
            "Epoch 027: | Loss: 0.40767 | Acc: 82.988\n",
            "Epoch 028: | Loss: 0.40569 | Acc: 82.739\n",
            "Epoch 029: | Loss: 0.40475 | Acc: 82.906\n",
            "Epoch 030: | Loss: 0.40179 | Acc: 83.024\n",
            "Epoch 031: | Loss: 0.40033 | Acc: 83.261\n",
            "Epoch 032: | Loss: 0.40078 | Acc: 83.306\n",
            "Epoch 033: | Loss: 0.39612 | Acc: 83.180\n",
            "Epoch 034: | Loss: 0.39163 | Acc: 83.286\n",
            "Epoch 035: | Loss: 0.39325 | Acc: 83.282\n",
            "Epoch 036: | Loss: 0.39303 | Acc: 83.506\n",
            "Epoch 037: | Loss: 0.39182 | Acc: 83.531\n",
            "Epoch 038: | Loss: 0.38562 | Acc: 83.992\n",
            "Epoch 039: | Loss: 0.38597 | Acc: 83.710\n",
            "Epoch 040: | Loss: 0.38154 | Acc: 84.045\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88      3187\n",
            "           1       0.33      0.12      0.18       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.58      0.53      0.53      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "32\n",
            "______________________________\n",
            "33 32\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 33 columns]\n",
            "0.7491707068129625\n",
            "Number of mislabeled points out of a total 3919 points : 1485\n",
            "0.6210768053074764\n",
            "correct_ones 385\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48436 | Acc: 81.098\n",
            "Epoch 002: | Loss: 0.46258 | Acc: 81.437\n",
            "Epoch 003: | Loss: 0.45812 | Acc: 81.449\n",
            "Epoch 004: | Loss: 0.45579 | Acc: 81.396\n",
            "Epoch 005: | Loss: 0.45403 | Acc: 81.486\n",
            "Epoch 006: | Loss: 0.45082 | Acc: 81.543\n",
            "Epoch 007: | Loss: 0.44959 | Acc: 81.527\n",
            "Epoch 008: | Loss: 0.44913 | Acc: 81.588\n",
            "Epoch 009: | Loss: 0.44655 | Acc: 81.600\n",
            "Epoch 010: | Loss: 0.44415 | Acc: 81.576\n",
            "Epoch 011: | Loss: 0.44297 | Acc: 81.620\n",
            "Epoch 012: | Loss: 0.44115 | Acc: 81.563\n",
            "Epoch 013: | Loss: 0.43907 | Acc: 81.661\n",
            "Epoch 014: | Loss: 0.43663 | Acc: 81.763\n",
            "Epoch 015: | Loss: 0.43317 | Acc: 81.984\n",
            "Epoch 016: | Loss: 0.43193 | Acc: 81.804\n",
            "Epoch 017: | Loss: 0.43059 | Acc: 82.082\n",
            "Epoch 018: | Loss: 0.42635 | Acc: 82.176\n",
            "Epoch 019: | Loss: 0.42566 | Acc: 82.163\n",
            "Epoch 020: | Loss: 0.42441 | Acc: 82.327\n",
            "Epoch 021: | Loss: 0.42368 | Acc: 82.273\n",
            "Epoch 022: | Loss: 0.41983 | Acc: 82.506\n",
            "Epoch 023: | Loss: 0.41820 | Acc: 82.433\n",
            "Epoch 024: | Loss: 0.41701 | Acc: 82.576\n",
            "Epoch 025: | Loss: 0.41222 | Acc: 82.739\n",
            "Epoch 026: | Loss: 0.41206 | Acc: 82.751\n",
            "Epoch 027: | Loss: 0.41115 | Acc: 82.963\n",
            "Epoch 028: | Loss: 0.40728 | Acc: 82.906\n",
            "Epoch 029: | Loss: 0.40739 | Acc: 82.914\n",
            "Epoch 030: | Loss: 0.40464 | Acc: 83.106\n",
            "Epoch 031: | Loss: 0.40153 | Acc: 83.310\n",
            "Epoch 032: | Loss: 0.40107 | Acc: 83.180\n",
            "Epoch 033: | Loss: 0.40110 | Acc: 83.343\n",
            "Epoch 034: | Loss: 0.39646 | Acc: 83.424\n",
            "Epoch 035: | Loss: 0.39636 | Acc: 83.555\n",
            "Epoch 036: | Loss: 0.39419 | Acc: 83.404\n",
            "Epoch 037: | Loss: 0.39077 | Acc: 83.739\n",
            "Epoch 038: | Loss: 0.39229 | Acc: 83.759\n",
            "Epoch 039: | Loss: 0.38945 | Acc: 83.837\n",
            "Epoch 040: | Loss: 0.38651 | Acc: 83.776\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88      3187\n",
            "           1       0.32      0.10      0.15       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.57      0.53      0.52      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "33\n",
            "______________________________\n",
            "34 33\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 34 columns]\n",
            "0.7494258739474355\n",
            "Number of mislabeled points out of a total 3919 points : 1486\n",
            "0.6208216381730033\n",
            "correct_ones 389\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=33, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48536 | Acc: 80.722\n",
            "Epoch 002: | Loss: 0.46401 | Acc: 81.482\n",
            "Epoch 003: | Loss: 0.45882 | Acc: 81.441\n",
            "Epoch 004: | Loss: 0.45447 | Acc: 81.449\n",
            "Epoch 005: | Loss: 0.45371 | Acc: 81.478\n",
            "Epoch 006: | Loss: 0.44981 | Acc: 81.445\n",
            "Epoch 007: | Loss: 0.45049 | Acc: 81.510\n",
            "Epoch 008: | Loss: 0.44731 | Acc: 81.633\n",
            "Epoch 009: | Loss: 0.44536 | Acc: 81.624\n",
            "Epoch 010: | Loss: 0.44553 | Acc: 81.641\n",
            "Epoch 011: | Loss: 0.43845 | Acc: 81.490\n",
            "Epoch 012: | Loss: 0.44076 | Acc: 81.690\n",
            "Epoch 013: | Loss: 0.43847 | Acc: 81.776\n",
            "Epoch 014: | Loss: 0.43331 | Acc: 81.865\n",
            "Epoch 015: | Loss: 0.43382 | Acc: 81.943\n",
            "Epoch 016: | Loss: 0.43122 | Acc: 81.914\n",
            "Epoch 017: | Loss: 0.42752 | Acc: 81.971\n",
            "Epoch 018: | Loss: 0.42698 | Acc: 82.196\n",
            "Epoch 019: | Loss: 0.42326 | Acc: 82.188\n",
            "Epoch 020: | Loss: 0.42195 | Acc: 82.478\n",
            "Epoch 021: | Loss: 0.41965 | Acc: 82.453\n",
            "Epoch 022: | Loss: 0.41893 | Acc: 82.318\n",
            "Epoch 023: | Loss: 0.41570 | Acc: 82.714\n",
            "Epoch 024: | Loss: 0.41209 | Acc: 82.722\n",
            "Epoch 025: | Loss: 0.40976 | Acc: 82.841\n",
            "Epoch 026: | Loss: 0.40760 | Acc: 82.857\n",
            "Epoch 027: | Loss: 0.40329 | Acc: 82.739\n",
            "Epoch 028: | Loss: 0.40266 | Acc: 83.298\n",
            "Epoch 029: | Loss: 0.40187 | Acc: 82.902\n",
            "Epoch 030: | Loss: 0.39930 | Acc: 83.380\n",
            "Epoch 031: | Loss: 0.39696 | Acc: 83.282\n",
            "Epoch 032: | Loss: 0.39567 | Acc: 83.576\n",
            "Epoch 033: | Loss: 0.39177 | Acc: 83.771\n",
            "Epoch 034: | Loss: 0.39233 | Acc: 83.771\n",
            "Epoch 035: | Loss: 0.38806 | Acc: 83.816\n",
            "Epoch 036: | Loss: 0.38541 | Acc: 84.016\n",
            "Epoch 037: | Loss: 0.38206 | Acc: 83.837\n",
            "Epoch 038: | Loss: 0.38499 | Acc: 84.114\n",
            "Epoch 039: | Loss: 0.38333 | Acc: 84.139\n",
            "Epoch 040: | Loss: 0.38244 | Acc: 84.106\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88      3187\n",
            "           1       0.32      0.13      0.18       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.57      0.53      0.53      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "34\n",
            "______________________________\n",
            "35 34\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 35 columns]\n",
            "0.7491707068129625\n",
            "Number of mislabeled points out of a total 3919 points : 1431\n",
            "0.6348558305690227\n",
            "correct_ones 369\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=34, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.47940 | Acc: 81.371\n",
            "Epoch 002: | Loss: 0.46223 | Acc: 81.445\n",
            "Epoch 003: | Loss: 0.45841 | Acc: 81.494\n",
            "Epoch 004: | Loss: 0.45614 | Acc: 81.453\n",
            "Epoch 005: | Loss: 0.45294 | Acc: 81.445\n",
            "Epoch 006: | Loss: 0.45128 | Acc: 81.465\n",
            "Epoch 007: | Loss: 0.44842 | Acc: 81.469\n",
            "Epoch 008: | Loss: 0.44686 | Acc: 81.453\n",
            "Epoch 009: | Loss: 0.44389 | Acc: 81.510\n",
            "Epoch 010: | Loss: 0.44117 | Acc: 81.514\n",
            "Epoch 011: | Loss: 0.43978 | Acc: 81.420\n",
            "Epoch 012: | Loss: 0.43830 | Acc: 81.559\n",
            "Epoch 013: | Loss: 0.43387 | Acc: 81.563\n",
            "Epoch 014: | Loss: 0.43218 | Acc: 81.694\n",
            "Epoch 015: | Loss: 0.43035 | Acc: 81.935\n",
            "Epoch 016: | Loss: 0.42692 | Acc: 81.943\n",
            "Epoch 017: | Loss: 0.42620 | Acc: 81.996\n",
            "Epoch 018: | Loss: 0.42428 | Acc: 81.910\n",
            "Epoch 019: | Loss: 0.42086 | Acc: 81.984\n",
            "Epoch 020: | Loss: 0.41693 | Acc: 82.143\n",
            "Epoch 021: | Loss: 0.41659 | Acc: 82.278\n",
            "Epoch 022: | Loss: 0.41189 | Acc: 82.522\n",
            "Epoch 023: | Loss: 0.41100 | Acc: 82.657\n",
            "Epoch 024: | Loss: 0.40636 | Acc: 82.857\n",
            "Epoch 025: | Loss: 0.40406 | Acc: 82.816\n",
            "Epoch 026: | Loss: 0.40425 | Acc: 83.073\n",
            "Epoch 027: | Loss: 0.40292 | Acc: 82.955\n",
            "Epoch 028: | Loss: 0.39726 | Acc: 83.294\n",
            "Epoch 029: | Loss: 0.39692 | Acc: 83.408\n",
            "Epoch 030: | Loss: 0.39306 | Acc: 83.339\n",
            "Epoch 031: | Loss: 0.39120 | Acc: 83.412\n",
            "Epoch 032: | Loss: 0.39082 | Acc: 83.482\n",
            "Epoch 033: | Loss: 0.38591 | Acc: 83.771\n",
            "Epoch 034: | Loss: 0.38669 | Acc: 83.767\n",
            "Epoch 035: | Loss: 0.38356 | Acc: 83.951\n",
            "Epoch 036: | Loss: 0.38397 | Acc: 83.665\n",
            "Epoch 037: | Loss: 0.38075 | Acc: 84.155\n",
            "Epoch 038: | Loss: 0.38017 | Acc: 84.000\n",
            "Epoch 039: | Loss: 0.37554 | Acc: 84.571\n",
            "Epoch 040: | Loss: 0.37390 | Acc: 84.559\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87      3187\n",
            "           1       0.33      0.17      0.23       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.58      0.55      0.55      3919\n",
            "weighted avg       0.74      0.78      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "35\n",
            "______________________________\n",
            "36 35\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 36 columns]\n",
            "0.7514672110232202\n",
            "Number of mislabeled points out of a total 3919 points : 1809\n",
            "0.5384026537381985\n",
            "correct_ones 457\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=35, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48852 | Acc: 79.596\n",
            "Epoch 002: | Loss: 0.46270 | Acc: 81.461\n",
            "Epoch 003: | Loss: 0.46245 | Acc: 81.445\n",
            "Epoch 004: | Loss: 0.45491 | Acc: 81.441\n",
            "Epoch 005: | Loss: 0.45526 | Acc: 81.449\n",
            "Epoch 006: | Loss: 0.45267 | Acc: 81.420\n",
            "Epoch 007: | Loss: 0.44534 | Acc: 81.539\n",
            "Epoch 008: | Loss: 0.44737 | Acc: 81.604\n",
            "Epoch 009: | Loss: 0.44410 | Acc: 81.498\n",
            "Epoch 010: | Loss: 0.44117 | Acc: 81.669\n",
            "Epoch 011: | Loss: 0.44131 | Acc: 81.629\n",
            "Epoch 012: | Loss: 0.43691 | Acc: 81.645\n",
            "Epoch 013: | Loss: 0.43619 | Acc: 81.698\n",
            "Epoch 014: | Loss: 0.43150 | Acc: 81.759\n",
            "Epoch 015: | Loss: 0.42950 | Acc: 81.976\n",
            "Epoch 016: | Loss: 0.42878 | Acc: 82.004\n",
            "Epoch 017: | Loss: 0.42172 | Acc: 82.241\n",
            "Epoch 018: | Loss: 0.42180 | Acc: 82.351\n",
            "Epoch 019: | Loss: 0.42045 | Acc: 82.155\n",
            "Epoch 020: | Loss: 0.41647 | Acc: 82.502\n",
            "Epoch 021: | Loss: 0.41495 | Acc: 82.588\n",
            "Epoch 022: | Loss: 0.41071 | Acc: 82.869\n",
            "Epoch 023: | Loss: 0.40690 | Acc: 83.041\n",
            "Epoch 024: | Loss: 0.40846 | Acc: 82.857\n",
            "Epoch 025: | Loss: 0.40419 | Acc: 82.988\n",
            "Epoch 026: | Loss: 0.40029 | Acc: 83.245\n",
            "Epoch 027: | Loss: 0.39901 | Acc: 83.331\n",
            "Epoch 028: | Loss: 0.39787 | Acc: 83.376\n",
            "Epoch 029: | Loss: 0.39412 | Acc: 83.514\n",
            "Epoch 030: | Loss: 0.39535 | Acc: 83.482\n",
            "Epoch 031: | Loss: 0.38785 | Acc: 83.641\n",
            "Epoch 032: | Loss: 0.38561 | Acc: 84.131\n",
            "Epoch 033: | Loss: 0.38372 | Acc: 83.869\n",
            "Epoch 034: | Loss: 0.38149 | Acc: 84.082\n",
            "Epoch 035: | Loss: 0.38246 | Acc: 84.159\n",
            "Epoch 036: | Loss: 0.37683 | Acc: 84.135\n",
            "Epoch 037: | Loss: 0.37677 | Acc: 84.449\n",
            "Epoch 038: | Loss: 0.37515 | Acc: 84.363\n",
            "Epoch 039: | Loss: 0.37069 | Acc: 84.596\n",
            "Epoch 040: | Loss: 0.37229 | Acc: 84.608\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88      3187\n",
            "           1       0.30      0.11      0.16       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.56      0.53      0.52      3919\n",
            "weighted avg       0.72      0.79      0.74      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "36\n",
            "______________________________\n",
            "37 36\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 37 columns]\n",
            "0.7532533809645318\n",
            "Number of mislabeled points out of a total 3919 points : 1656\n",
            "0.5774432253125797\n",
            "correct_ones 424\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=36, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48334 | Acc: 81.265\n",
            "Epoch 002: | Loss: 0.46330 | Acc: 81.461\n",
            "Epoch 003: | Loss: 0.45891 | Acc: 81.453\n",
            "Epoch 004: | Loss: 0.45785 | Acc: 81.441\n",
            "Epoch 005: | Loss: 0.45430 | Acc: 81.469\n",
            "Epoch 006: | Loss: 0.45155 | Acc: 81.424\n",
            "Epoch 007: | Loss: 0.44807 | Acc: 81.469\n",
            "Epoch 008: | Loss: 0.44677 | Acc: 81.465\n",
            "Epoch 009: | Loss: 0.44678 | Acc: 81.478\n",
            "Epoch 010: | Loss: 0.44399 | Acc: 81.429\n",
            "Epoch 011: | Loss: 0.44266 | Acc: 81.498\n",
            "Epoch 012: | Loss: 0.43838 | Acc: 81.335\n",
            "Epoch 013: | Loss: 0.43798 | Acc: 81.522\n",
            "Epoch 014: | Loss: 0.43498 | Acc: 81.453\n",
            "Epoch 015: | Loss: 0.43410 | Acc: 81.592\n",
            "Epoch 016: | Loss: 0.43300 | Acc: 81.559\n",
            "Epoch 017: | Loss: 0.42771 | Acc: 81.727\n",
            "Epoch 018: | Loss: 0.42857 | Acc: 81.743\n",
            "Epoch 019: | Loss: 0.42336 | Acc: 81.882\n",
            "Epoch 020: | Loss: 0.42156 | Acc: 81.890\n",
            "Epoch 021: | Loss: 0.41982 | Acc: 82.029\n",
            "Epoch 022: | Loss: 0.41437 | Acc: 82.404\n",
            "Epoch 023: | Loss: 0.41438 | Acc: 82.359\n",
            "Epoch 024: | Loss: 0.41100 | Acc: 82.702\n",
            "Epoch 025: | Loss: 0.40986 | Acc: 82.710\n",
            "Epoch 026: | Loss: 0.40524 | Acc: 82.600\n",
            "Epoch 027: | Loss: 0.40731 | Acc: 82.649\n",
            "Epoch 028: | Loss: 0.40237 | Acc: 82.735\n",
            "Epoch 029: | Loss: 0.39855 | Acc: 83.073\n",
            "Epoch 030: | Loss: 0.39853 | Acc: 82.988\n",
            "Epoch 031: | Loss: 0.39723 | Acc: 82.992\n",
            "Epoch 032: | Loss: 0.39280 | Acc: 83.094\n",
            "Epoch 033: | Loss: 0.39188 | Acc: 83.665\n",
            "Epoch 034: | Loss: 0.38939 | Acc: 83.335\n",
            "Epoch 035: | Loss: 0.38727 | Acc: 83.371\n",
            "Epoch 036: | Loss: 0.38415 | Acc: 83.780\n",
            "Epoch 037: | Loss: 0.38262 | Acc: 83.784\n",
            "Epoch 038: | Loss: 0.37722 | Acc: 84.408\n",
            "Epoch 039: | Loss: 0.37861 | Acc: 84.192\n",
            "Epoch 040: | Loss: 0.37494 | Acc: 84.404\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87      3187\n",
            "           1       0.31      0.15      0.20       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.57      0.54      0.54      3919\n",
            "weighted avg       0.73      0.78      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "37\n",
            "______________________________\n",
            "38 37\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 38 columns]\n",
            "0.7532533809645318\n",
            "Number of mislabeled points out of a total 3919 points : 1944\n",
            "0.5039550905843327\n",
            "correct_ones 497\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=37, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48343 | Acc: 81.278\n",
            "Epoch 002: | Loss: 0.46433 | Acc: 81.457\n",
            "Epoch 003: | Loss: 0.45947 | Acc: 81.494\n",
            "Epoch 004: | Loss: 0.45537 | Acc: 81.457\n",
            "Epoch 005: | Loss: 0.45440 | Acc: 81.486\n",
            "Epoch 006: | Loss: 0.45174 | Acc: 81.469\n",
            "Epoch 007: | Loss: 0.44850 | Acc: 81.453\n",
            "Epoch 008: | Loss: 0.44550 | Acc: 81.412\n",
            "Epoch 009: | Loss: 0.44511 | Acc: 81.469\n",
            "Epoch 010: | Loss: 0.44350 | Acc: 81.494\n",
            "Epoch 011: | Loss: 0.43785 | Acc: 81.506\n",
            "Epoch 012: | Loss: 0.43783 | Acc: 81.665\n",
            "Epoch 013: | Loss: 0.43308 | Acc: 81.824\n",
            "Epoch 014: | Loss: 0.43256 | Acc: 81.947\n",
            "Epoch 015: | Loss: 0.43073 | Acc: 82.053\n",
            "Epoch 016: | Loss: 0.42663 | Acc: 82.159\n",
            "Epoch 017: | Loss: 0.42558 | Acc: 82.118\n",
            "Epoch 018: | Loss: 0.41957 | Acc: 82.424\n",
            "Epoch 019: | Loss: 0.42123 | Acc: 82.200\n",
            "Epoch 020: | Loss: 0.41509 | Acc: 82.837\n",
            "Epoch 021: | Loss: 0.41529 | Acc: 82.678\n",
            "Epoch 022: | Loss: 0.41007 | Acc: 83.106\n",
            "Epoch 023: | Loss: 0.40616 | Acc: 83.192\n",
            "Epoch 024: | Loss: 0.40596 | Acc: 83.278\n",
            "Epoch 025: | Loss: 0.40386 | Acc: 83.359\n",
            "Epoch 026: | Loss: 0.40031 | Acc: 83.469\n",
            "Epoch 027: | Loss: 0.39532 | Acc: 83.743\n",
            "Epoch 028: | Loss: 0.39839 | Acc: 83.739\n",
            "Epoch 029: | Loss: 0.39559 | Acc: 83.694\n",
            "Epoch 030: | Loss: 0.39327 | Acc: 83.898\n",
            "Epoch 031: | Loss: 0.38911 | Acc: 84.065\n",
            "Epoch 032: | Loss: 0.38784 | Acc: 84.024\n",
            "Epoch 033: | Loss: 0.38726 | Acc: 84.147\n",
            "Epoch 034: | Loss: 0.38212 | Acc: 84.065\n",
            "Epoch 035: | Loss: 0.38117 | Acc: 84.527\n",
            "Epoch 036: | Loss: 0.38015 | Acc: 84.335\n",
            "Epoch 037: | Loss: 0.37993 | Acc: 84.367\n",
            "Epoch 038: | Loss: 0.37503 | Acc: 84.739\n",
            "Epoch 039: | Loss: 0.37635 | Acc: 84.547\n",
            "Epoch 040: | Loss: 0.37573 | Acc: 84.824\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86      3187\n",
            "           1       0.29      0.17      0.22       732\n",
            "\n",
            "    accuracy                           0.77      3919\n",
            "   macro avg       0.56      0.54      0.54      3919\n",
            "weighted avg       0.73      0.77      0.74      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "38\n",
            "______________________________\n",
            "39 38\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 39 columns]\n",
            "0.7519775452921663\n",
            "Number of mislabeled points out of a total 3919 points : 1708\n",
            "0.5641745343199795\n",
            "correct_ones 449\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=38, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.49030 | Acc: 80.404\n",
            "Epoch 002: | Loss: 0.46367 | Acc: 81.478\n",
            "Epoch 003: | Loss: 0.45920 | Acc: 81.461\n",
            "Epoch 004: | Loss: 0.45516 | Acc: 81.445\n",
            "Epoch 005: | Loss: 0.45266 | Acc: 81.429\n",
            "Epoch 006: | Loss: 0.45322 | Acc: 81.461\n",
            "Epoch 007: | Loss: 0.44809 | Acc: 81.510\n",
            "Epoch 008: | Loss: 0.44650 | Acc: 81.498\n",
            "Epoch 009: | Loss: 0.44358 | Acc: 81.510\n",
            "Epoch 010: | Loss: 0.44124 | Acc: 81.747\n",
            "Epoch 011: | Loss: 0.43974 | Acc: 81.620\n",
            "Epoch 012: | Loss: 0.43684 | Acc: 81.755\n",
            "Epoch 013: | Loss: 0.43550 | Acc: 81.816\n",
            "Epoch 014: | Loss: 0.43126 | Acc: 82.004\n",
            "Epoch 015: | Loss: 0.43017 | Acc: 82.102\n",
            "Epoch 016: | Loss: 0.42491 | Acc: 82.155\n",
            "Epoch 017: | Loss: 0.42344 | Acc: 82.522\n",
            "Epoch 018: | Loss: 0.42256 | Acc: 82.420\n",
            "Epoch 019: | Loss: 0.41708 | Acc: 82.531\n",
            "Epoch 020: | Loss: 0.41244 | Acc: 82.698\n",
            "Epoch 021: | Loss: 0.41229 | Acc: 82.547\n",
            "Epoch 022: | Loss: 0.40826 | Acc: 82.963\n",
            "Epoch 023: | Loss: 0.40718 | Acc: 82.951\n",
            "Epoch 024: | Loss: 0.40515 | Acc: 83.118\n",
            "Epoch 025: | Loss: 0.40208 | Acc: 83.155\n",
            "Epoch 026: | Loss: 0.39613 | Acc: 83.249\n",
            "Epoch 027: | Loss: 0.39282 | Acc: 83.576\n",
            "Epoch 028: | Loss: 0.39034 | Acc: 83.682\n",
            "Epoch 029: | Loss: 0.39173 | Acc: 83.535\n",
            "Epoch 030: | Loss: 0.38379 | Acc: 84.147\n",
            "Epoch 031: | Loss: 0.38512 | Acc: 83.682\n",
            "Epoch 032: | Loss: 0.38313 | Acc: 83.824\n",
            "Epoch 033: | Loss: 0.38078 | Acc: 84.184\n",
            "Epoch 034: | Loss: 0.37856 | Acc: 84.241\n",
            "Epoch 035: | Loss: 0.37837 | Acc: 84.416\n",
            "Epoch 036: | Loss: 0.37607 | Acc: 84.310\n",
            "Epoch 037: | Loss: 0.37306 | Acc: 84.506\n",
            "Epoch 038: | Loss: 0.37184 | Acc: 84.229\n",
            "Epoch 039: | Loss: 0.36714 | Acc: 84.727\n",
            "Epoch 040: | Loss: 0.36408 | Acc: 84.865\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88      3187\n",
            "           1       0.31      0.12      0.17       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.57      0.53      0.52      3919\n",
            "weighted avg       0.73      0.79      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "39\n",
            "______________________________\n",
            "40 39\n",
            "       count_accts  flag_top_ed_spender  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299            -1.061005  ...          -0.397933                1\n",
            "1         1.497981             0.942502  ...           5.174360                1\n",
            "2        -0.441299             0.942502  ...          -0.397933                1\n",
            "3         3.437261             0.942502  ...          -0.397933                1\n",
            "4        -0.441299            -1.061005  ...          -0.397933                1\n",
            "...            ...                  ...  ...                ...              ...\n",
            "19590    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19591    -0.441299             0.942502  ...          -0.397933                0\n",
            "19592    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19593    -0.441299            -1.061005  ...          -0.397933                0\n",
            "19594    -0.441299            -1.061005  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 40 columns]\n",
            "0.7519775452921663\n",
            "Number of mislabeled points out of a total 3919 points : 1710\n",
            "0.5636642000510335\n",
            "correct_ones 446\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=39, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48682 | Acc: 80.886\n",
            "Epoch 002: | Loss: 0.46293 | Acc: 81.494\n",
            "Epoch 003: | Loss: 0.45894 | Acc: 81.392\n",
            "Epoch 004: | Loss: 0.45491 | Acc: 81.392\n",
            "Epoch 005: | Loss: 0.45276 | Acc: 81.437\n",
            "Epoch 006: | Loss: 0.44857 | Acc: 81.465\n",
            "Epoch 007: | Loss: 0.44904 | Acc: 81.469\n",
            "Epoch 008: | Loss: 0.44748 | Acc: 81.486\n",
            "Epoch 009: | Loss: 0.44429 | Acc: 81.612\n",
            "Epoch 010: | Loss: 0.44230 | Acc: 81.514\n",
            "Epoch 011: | Loss: 0.43701 | Acc: 81.531\n",
            "Epoch 012: | Loss: 0.43650 | Acc: 81.592\n",
            "Epoch 013: | Loss: 0.43497 | Acc: 81.702\n",
            "Epoch 014: | Loss: 0.43374 | Acc: 81.600\n",
            "Epoch 015: | Loss: 0.42670 | Acc: 81.763\n",
            "Epoch 016: | Loss: 0.42653 | Acc: 81.914\n",
            "Epoch 017: | Loss: 0.42462 | Acc: 81.947\n",
            "Epoch 018: | Loss: 0.42187 | Acc: 82.143\n",
            "Epoch 019: | Loss: 0.41877 | Acc: 82.139\n",
            "Epoch 020: | Loss: 0.41542 | Acc: 82.180\n",
            "Epoch 021: | Loss: 0.41441 | Acc: 82.298\n",
            "Epoch 022: | Loss: 0.41053 | Acc: 82.543\n",
            "Epoch 023: | Loss: 0.40630 | Acc: 82.710\n",
            "Epoch 024: | Loss: 0.40795 | Acc: 82.624\n",
            "Epoch 025: | Loss: 0.40181 | Acc: 82.833\n",
            "Epoch 026: | Loss: 0.39791 | Acc: 83.029\n",
            "Epoch 027: | Loss: 0.39649 | Acc: 83.029\n",
            "Epoch 028: | Loss: 0.39105 | Acc: 83.294\n",
            "Epoch 029: | Loss: 0.39311 | Acc: 83.273\n",
            "Epoch 030: | Loss: 0.39075 | Acc: 83.282\n",
            "Epoch 031: | Loss: 0.38771 | Acc: 83.580\n",
            "Epoch 032: | Loss: 0.38591 | Acc: 83.620\n",
            "Epoch 033: | Loss: 0.38075 | Acc: 83.824\n",
            "Epoch 034: | Loss: 0.38363 | Acc: 83.641\n",
            "Epoch 035: | Loss: 0.37399 | Acc: 83.927\n",
            "Epoch 036: | Loss: 0.37831 | Acc: 83.849\n",
            "Epoch 037: | Loss: 0.37197 | Acc: 83.992\n",
            "Epoch 038: | Loss: 0.37275 | Acc: 83.947\n",
            "Epoch 039: | Loss: 0.37041 | Acc: 84.155\n",
            "Epoch 040: | Loss: 0.36924 | Acc: 84.331\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87      3187\n",
            "           1       0.28      0.12      0.17       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.55      0.53      0.52      3919\n",
            "weighted avg       0.72      0.78      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "40\n",
            "______________________________\n",
            "41 40\n",
            "       count_accts    cm_age  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.397933                1\n",
            "1         1.497981  3.995318  ...           5.174360                1\n",
            "2        -0.441299  3.909026  ...          -0.397933                1\n",
            "3         3.437261  3.736442  ...          -0.397933                1\n",
            "4        -0.441299  3.736442  ...          -0.397933                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19591    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19592    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19593    -0.441299 -1.613675  ...          -0.397933                0\n",
            "19594    -0.441299 -1.613675  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 41 columns]\n",
            "0.7512120438887472\n",
            "Number of mislabeled points out of a total 3919 points : 1708\n",
            "0.5641745343199795\n",
            "correct_ones 451\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=40, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48257 | Acc: 81.331\n",
            "Epoch 002: | Loss: 0.46422 | Acc: 81.445\n",
            "Epoch 003: | Loss: 0.45673 | Acc: 81.449\n",
            "Epoch 004: | Loss: 0.45551 | Acc: 81.441\n",
            "Epoch 005: | Loss: 0.45332 | Acc: 81.469\n",
            "Epoch 006: | Loss: 0.45093 | Acc: 81.461\n",
            "Epoch 007: | Loss: 0.44822 | Acc: 81.420\n",
            "Epoch 008: | Loss: 0.44613 | Acc: 81.404\n",
            "Epoch 009: | Loss: 0.44171 | Acc: 81.482\n",
            "Epoch 010: | Loss: 0.44072 | Acc: 81.461\n",
            "Epoch 011: | Loss: 0.43837 | Acc: 81.555\n",
            "Epoch 012: | Loss: 0.43542 | Acc: 81.563\n",
            "Epoch 013: | Loss: 0.43276 | Acc: 81.563\n",
            "Epoch 014: | Loss: 0.43246 | Acc: 81.600\n",
            "Epoch 015: | Loss: 0.42830 | Acc: 81.751\n",
            "Epoch 016: | Loss: 0.42340 | Acc: 81.820\n",
            "Epoch 017: | Loss: 0.42133 | Acc: 81.665\n",
            "Epoch 018: | Loss: 0.41841 | Acc: 82.200\n",
            "Epoch 019: | Loss: 0.41480 | Acc: 82.245\n",
            "Epoch 020: | Loss: 0.41275 | Acc: 82.135\n",
            "Epoch 021: | Loss: 0.40688 | Acc: 82.220\n",
            "Epoch 022: | Loss: 0.40798 | Acc: 82.322\n",
            "Epoch 023: | Loss: 0.40515 | Acc: 82.694\n",
            "Epoch 024: | Loss: 0.39855 | Acc: 82.755\n",
            "Epoch 025: | Loss: 0.39771 | Acc: 82.747\n",
            "Epoch 026: | Loss: 0.39128 | Acc: 83.347\n",
            "Epoch 027: | Loss: 0.39208 | Acc: 82.918\n",
            "Epoch 028: | Loss: 0.39148 | Acc: 83.294\n",
            "Epoch 029: | Loss: 0.38392 | Acc: 83.812\n",
            "Epoch 030: | Loss: 0.38334 | Acc: 83.539\n",
            "Epoch 031: | Loss: 0.37900 | Acc: 83.776\n",
            "Epoch 032: | Loss: 0.37935 | Acc: 83.918\n",
            "Epoch 033: | Loss: 0.37467 | Acc: 83.967\n",
            "Epoch 034: | Loss: 0.37108 | Acc: 84.220\n",
            "Epoch 035: | Loss: 0.36740 | Acc: 84.527\n",
            "Epoch 036: | Loss: 0.36865 | Acc: 84.510\n",
            "Epoch 037: | Loss: 0.36524 | Acc: 84.490\n",
            "Epoch 038: | Loss: 0.36591 | Acc: 84.649\n",
            "Epoch 039: | Loss: 0.35963 | Acc: 84.722\n",
            "Epoch 040: | Loss: 0.35790 | Acc: 84.645\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87      3187\n",
            "           1       0.32      0.16      0.21       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.57      0.54      0.54      3919\n",
            "weighted avg       0.73      0.78      0.75      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "41\n",
            "______________________________\n",
            "42 41\n",
            "       count_accts    cm_age  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.397933                1\n",
            "1         1.497981  3.995318  ...           5.174360                1\n",
            "2        -0.441299  3.909026  ...          -0.397933                1\n",
            "3         3.437261  3.736442  ...          -0.397933                1\n",
            "4        -0.441299  3.736442  ...          -0.397933                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19591    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19592    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19593    -0.441299 -1.613675  ...          -0.397933                0\n",
            "19594    -0.441299 -1.613675  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 42 columns]\n",
            "0.7547843837713702\n",
            "Number of mislabeled points out of a total 3919 points : 1707\n",
            "0.5644297014544527\n",
            "correct_ones 450\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=41, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48244 | Acc: 81.176\n",
            "Epoch 002: | Loss: 0.46155 | Acc: 81.502\n",
            "Epoch 003: | Loss: 0.45895 | Acc: 81.445\n",
            "Epoch 004: | Loss: 0.45459 | Acc: 81.461\n",
            "Epoch 005: | Loss: 0.45191 | Acc: 81.461\n",
            "Epoch 006: | Loss: 0.44916 | Acc: 81.453\n",
            "Epoch 007: | Loss: 0.44513 | Acc: 81.473\n",
            "Epoch 008: | Loss: 0.44491 | Acc: 81.453\n",
            "Epoch 009: | Loss: 0.44141 | Acc: 81.527\n",
            "Epoch 010: | Loss: 0.43711 | Acc: 81.559\n",
            "Epoch 011: | Loss: 0.43826 | Acc: 81.661\n",
            "Epoch 012: | Loss: 0.43095 | Acc: 81.722\n",
            "Epoch 013: | Loss: 0.43088 | Acc: 81.955\n",
            "Epoch 014: | Loss: 0.42673 | Acc: 81.959\n",
            "Epoch 015: | Loss: 0.42255 | Acc: 81.996\n",
            "Epoch 016: | Loss: 0.41810 | Acc: 82.114\n",
            "Epoch 017: | Loss: 0.41847 | Acc: 82.273\n",
            "Epoch 018: | Loss: 0.41183 | Acc: 82.461\n",
            "Epoch 019: | Loss: 0.40927 | Acc: 82.367\n",
            "Epoch 020: | Loss: 0.40518 | Acc: 82.596\n",
            "Epoch 021: | Loss: 0.40227 | Acc: 82.943\n",
            "Epoch 022: | Loss: 0.39883 | Acc: 82.980\n",
            "Epoch 023: | Loss: 0.39161 | Acc: 83.253\n",
            "Epoch 024: | Loss: 0.39091 | Acc: 83.380\n",
            "Epoch 025: | Loss: 0.38658 | Acc: 83.176\n",
            "Epoch 026: | Loss: 0.38561 | Acc: 83.624\n",
            "Epoch 027: | Loss: 0.38427 | Acc: 83.784\n",
            "Epoch 028: | Loss: 0.38003 | Acc: 83.796\n",
            "Epoch 029: | Loss: 0.37695 | Acc: 83.812\n",
            "Epoch 030: | Loss: 0.37620 | Acc: 83.931\n",
            "Epoch 031: | Loss: 0.37012 | Acc: 84.314\n",
            "Epoch 032: | Loss: 0.36911 | Acc: 84.539\n",
            "Epoch 033: | Loss: 0.36598 | Acc: 84.265\n",
            "Epoch 034: | Loss: 0.36085 | Acc: 84.522\n",
            "Epoch 035: | Loss: 0.36370 | Acc: 84.727\n",
            "Epoch 036: | Loss: 0.35858 | Acc: 84.743\n",
            "Epoch 037: | Loss: 0.35367 | Acc: 85.143\n",
            "Epoch 038: | Loss: 0.35563 | Acc: 85.041\n",
            "Epoch 039: | Loss: 0.35563 | Acc: 84.878\n",
            "Epoch 040: | Loss: 0.35016 | Acc: 85.204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88      3187\n",
            "           1       0.27      0.08      0.13       732\n",
            "\n",
            "    accuracy                           0.79      3919\n",
            "   macro avg       0.54      0.52      0.50      3919\n",
            "weighted avg       0.72      0.79      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "42\n",
            "______________________________\n",
            "43 42\n",
            "       count_accts    cm_age  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.397933                1\n",
            "1         1.497981  3.995318  ...           5.174360                1\n",
            "2        -0.441299  3.909026  ...          -0.397933                1\n",
            "3         3.437261  3.736442  ...          -0.397933                1\n",
            "4        -0.441299  3.736442  ...          -0.397933                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19591    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19592    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19593    -0.441299 -1.613675  ...          -0.397933                0\n",
            "19594    -0.441299 -1.613675  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 43 columns]\n",
            "0.7535085480990048\n",
            "Number of mislabeled points out of a total 3919 points : 1709\n",
            "0.5639193671855065\n",
            "correct_ones 450\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=42, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48211 | Acc: 81.208\n",
            "Epoch 002: | Loss: 0.46297 | Acc: 81.433\n",
            "Epoch 003: | Loss: 0.45927 | Acc: 81.424\n",
            "Epoch 004: | Loss: 0.45513 | Acc: 81.445\n",
            "Epoch 005: | Loss: 0.44989 | Acc: 81.420\n",
            "Epoch 006: | Loss: 0.45100 | Acc: 81.396\n",
            "Epoch 007: | Loss: 0.44722 | Acc: 81.465\n",
            "Epoch 008: | Loss: 0.44489 | Acc: 81.494\n",
            "Epoch 009: | Loss: 0.44226 | Acc: 81.555\n",
            "Epoch 010: | Loss: 0.43816 | Acc: 81.612\n",
            "Epoch 011: | Loss: 0.43426 | Acc: 81.571\n",
            "Epoch 012: | Loss: 0.43319 | Acc: 81.890\n",
            "Epoch 013: | Loss: 0.42637 | Acc: 81.963\n",
            "Epoch 014: | Loss: 0.42608 | Acc: 82.082\n",
            "Epoch 015: | Loss: 0.42302 | Acc: 82.302\n",
            "Epoch 016: | Loss: 0.42143 | Acc: 82.086\n",
            "Epoch 017: | Loss: 0.41576 | Acc: 82.429\n",
            "Epoch 018: | Loss: 0.41328 | Acc: 82.547\n",
            "Epoch 019: | Loss: 0.40954 | Acc: 82.539\n",
            "Epoch 020: | Loss: 0.40552 | Acc: 82.706\n",
            "Epoch 021: | Loss: 0.40107 | Acc: 82.955\n",
            "Epoch 022: | Loss: 0.40120 | Acc: 83.020\n",
            "Epoch 023: | Loss: 0.39644 | Acc: 83.245\n",
            "Epoch 024: | Loss: 0.39145 | Acc: 83.367\n",
            "Epoch 025: | Loss: 0.39107 | Acc: 83.265\n",
            "Epoch 026: | Loss: 0.38624 | Acc: 83.592\n",
            "Epoch 027: | Loss: 0.38050 | Acc: 83.522\n",
            "Epoch 028: | Loss: 0.38044 | Acc: 83.629\n",
            "Epoch 029: | Loss: 0.38137 | Acc: 83.710\n",
            "Epoch 030: | Loss: 0.37355 | Acc: 84.135\n",
            "Epoch 031: | Loss: 0.37259 | Acc: 84.012\n",
            "Epoch 032: | Loss: 0.36916 | Acc: 84.461\n",
            "Epoch 033: | Loss: 0.36597 | Acc: 84.314\n",
            "Epoch 034: | Loss: 0.36210 | Acc: 84.682\n",
            "Epoch 035: | Loss: 0.36293 | Acc: 84.661\n",
            "Epoch 036: | Loss: 0.35700 | Acc: 84.763\n",
            "Epoch 037: | Loss: 0.35777 | Acc: 84.641\n",
            "Epoch 038: | Loss: 0.35678 | Acc: 84.580\n",
            "Epoch 039: | Loss: 0.35359 | Acc: 85.335\n",
            "Epoch 040: | Loss: 0.35034 | Acc: 85.233\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86      3187\n",
            "           1       0.27      0.15      0.19       732\n",
            "\n",
            "    accuracy                           0.77      3919\n",
            "   macro avg       0.55      0.53      0.53      3919\n",
            "weighted avg       0.72      0.77      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "43\n",
            "______________________________\n",
            "44 43\n",
            "       count_accts    cm_age  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.397933                1\n",
            "1         1.497981  3.995318  ...           5.174360                1\n",
            "2        -0.441299  3.909026  ...          -0.397933                1\n",
            "3         3.437261  3.736442  ...          -0.397933                1\n",
            "4        -0.441299  3.736442  ...          -0.397933                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19591    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19592    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19593    -0.441299 -1.613675  ...          -0.397933                0\n",
            "19594    -0.441299 -1.613675  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 44 columns]\n",
            "0.754018882367951\n",
            "Number of mislabeled points out of a total 3919 points : 1635\n",
            "0.5828017351365145\n",
            "correct_ones 429\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=43, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48621 | Acc: 80.988\n",
            "Epoch 002: | Loss: 0.46299 | Acc: 81.453\n",
            "Epoch 003: | Loss: 0.45778 | Acc: 81.445\n",
            "Epoch 004: | Loss: 0.45362 | Acc: 81.490\n",
            "Epoch 005: | Loss: 0.45356 | Acc: 81.437\n",
            "Epoch 006: | Loss: 0.44890 | Acc: 81.433\n",
            "Epoch 007: | Loss: 0.44553 | Acc: 81.473\n",
            "Epoch 008: | Loss: 0.44510 | Acc: 81.494\n",
            "Epoch 009: | Loss: 0.44087 | Acc: 81.502\n",
            "Epoch 010: | Loss: 0.43902 | Acc: 81.620\n",
            "Epoch 011: | Loss: 0.43620 | Acc: 81.551\n",
            "Epoch 012: | Loss: 0.43278 | Acc: 81.633\n",
            "Epoch 013: | Loss: 0.42932 | Acc: 81.714\n",
            "Epoch 014: | Loss: 0.42690 | Acc: 82.000\n",
            "Epoch 015: | Loss: 0.42582 | Acc: 81.845\n",
            "Epoch 016: | Loss: 0.41970 | Acc: 82.012\n",
            "Epoch 017: | Loss: 0.41520 | Acc: 82.224\n",
            "Epoch 018: | Loss: 0.41392 | Acc: 82.367\n",
            "Epoch 019: | Loss: 0.41066 | Acc: 82.229\n",
            "Epoch 020: | Loss: 0.40755 | Acc: 82.567\n",
            "Epoch 021: | Loss: 0.40368 | Acc: 82.629\n",
            "Epoch 022: | Loss: 0.40202 | Acc: 82.886\n",
            "Epoch 023: | Loss: 0.39464 | Acc: 82.976\n",
            "Epoch 024: | Loss: 0.38880 | Acc: 83.184\n",
            "Epoch 025: | Loss: 0.38833 | Acc: 83.424\n",
            "Epoch 026: | Loss: 0.38866 | Acc: 83.486\n",
            "Epoch 027: | Loss: 0.38079 | Acc: 83.616\n",
            "Epoch 028: | Loss: 0.38135 | Acc: 83.596\n",
            "Epoch 029: | Loss: 0.37614 | Acc: 83.702\n",
            "Epoch 030: | Loss: 0.37171 | Acc: 84.204\n",
            "Epoch 031: | Loss: 0.37054 | Acc: 84.053\n",
            "Epoch 032: | Loss: 0.36708 | Acc: 84.135\n",
            "Epoch 033: | Loss: 0.36399 | Acc: 84.555\n",
            "Epoch 034: | Loss: 0.36471 | Acc: 84.518\n",
            "Epoch 035: | Loss: 0.35640 | Acc: 84.800\n",
            "Epoch 036: | Loss: 0.35848 | Acc: 84.514\n",
            "Epoch 037: | Loss: 0.35366 | Acc: 84.910\n",
            "Epoch 038: | Loss: 0.35053 | Acc: 85.269\n",
            "Epoch 039: | Loss: 0.34647 | Acc: 85.335\n",
            "Epoch 040: | Loss: 0.34470 | Acc: 85.514\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86      3187\n",
            "           1       0.30      0.21      0.25       732\n",
            "\n",
            "    accuracy                           0.76      3919\n",
            "   macro avg       0.57      0.55      0.55      3919\n",
            "weighted avg       0.73      0.76      0.74      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "44\n",
            "______________________________\n",
            "45 44\n",
            "       count_accts    cm_age  ...  sow_tot_revol_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.397933                1\n",
            "1         1.497981  3.995318  ...           5.174360                1\n",
            "2        -0.441299  3.909026  ...          -0.397933                1\n",
            "3         3.437261  3.736442  ...          -0.397933                1\n",
            "4        -0.441299  3.736442  ...          -0.397933                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19591    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19592    -0.441299 -1.527383  ...          -0.397933                0\n",
            "19593    -0.441299 -1.613675  ...          -0.397933                0\n",
            "19594    -0.441299 -1.613675  ...          -0.397933                0\n",
            "\n",
            "[19595 rows x 45 columns]\n",
            "0.754018882367951\n",
            "Number of mislabeled points out of a total 3919 points : 1656\n",
            "0.5774432253125797\n",
            "correct_ones 435\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=44, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48408 | Acc: 80.788\n",
            "Epoch 002: | Loss: 0.45977 | Acc: 81.457\n",
            "Epoch 003: | Loss: 0.45698 | Acc: 81.461\n",
            "Epoch 004: | Loss: 0.45419 | Acc: 81.461\n",
            "Epoch 005: | Loss: 0.45151 | Acc: 81.453\n",
            "Epoch 006: | Loss: 0.44738 | Acc: 81.461\n",
            "Epoch 007: | Loss: 0.44501 | Acc: 81.482\n",
            "Epoch 008: | Loss: 0.44110 | Acc: 81.567\n",
            "Epoch 009: | Loss: 0.43986 | Acc: 81.469\n",
            "Epoch 010: | Loss: 0.43937 | Acc: 81.690\n",
            "Epoch 011: | Loss: 0.43353 | Acc: 81.767\n",
            "Epoch 012: | Loss: 0.43197 | Acc: 81.886\n",
            "Epoch 013: | Loss: 0.42751 | Acc: 82.020\n",
            "Epoch 014: | Loss: 0.42434 | Acc: 82.008\n",
            "Epoch 015: | Loss: 0.41956 | Acc: 82.314\n",
            "Epoch 016: | Loss: 0.41622 | Acc: 82.327\n",
            "Epoch 017: | Loss: 0.41138 | Acc: 82.580\n",
            "Epoch 018: | Loss: 0.40672 | Acc: 82.710\n",
            "Epoch 019: | Loss: 0.40484 | Acc: 82.951\n",
            "Epoch 020: | Loss: 0.40092 | Acc: 82.959\n",
            "Epoch 021: | Loss: 0.39712 | Acc: 83.143\n",
            "Epoch 022: | Loss: 0.39303 | Acc: 83.314\n",
            "Epoch 023: | Loss: 0.39323 | Acc: 83.351\n",
            "Epoch 024: | Loss: 0.38553 | Acc: 83.824\n",
            "Epoch 025: | Loss: 0.38438 | Acc: 83.596\n",
            "Epoch 026: | Loss: 0.37854 | Acc: 83.947\n",
            "Epoch 027: | Loss: 0.37768 | Acc: 84.233\n",
            "Epoch 028: | Loss: 0.37377 | Acc: 84.171\n",
            "Epoch 029: | Loss: 0.37615 | Acc: 84.098\n",
            "Epoch 030: | Loss: 0.36917 | Acc: 84.478\n",
            "Epoch 031: | Loss: 0.36410 | Acc: 84.841\n",
            "Epoch 032: | Loss: 0.36255 | Acc: 84.971\n",
            "Epoch 033: | Loss: 0.36069 | Acc: 84.747\n",
            "Epoch 034: | Loss: 0.35687 | Acc: 84.816\n",
            "Epoch 035: | Loss: 0.35095 | Acc: 85.302\n",
            "Epoch 036: | Loss: 0.35504 | Acc: 85.188\n",
            "Epoch 037: | Loss: 0.35578 | Acc: 85.220\n",
            "Epoch 038: | Loss: 0.34811 | Acc: 85.302\n",
            "Epoch 039: | Loss: 0.34485 | Acc: 85.371\n",
            "Epoch 040: | Loss: 0.34531 | Acc: 85.567\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87      3187\n",
            "           1       0.30      0.16      0.21       732\n",
            "\n",
            "    accuracy                           0.77      3919\n",
            "   macro avg       0.57      0.54      0.54      3919\n",
            "weighted avg       0.73      0.77      0.75      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "45\n",
            "______________________________\n",
            "46 45\n",
            "       count_accts    cm_age  ...  sow_tot_trans_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.656805                1\n",
            "1         1.497981  3.995318  ...           1.032738                1\n",
            "2        -0.441299  3.909026  ...           1.032738                1\n",
            "3         3.437261  3.736442  ...          -0.656805                1\n",
            "4        -0.441299  3.736442  ...          -0.656805                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19591    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19592    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19593    -0.441299 -1.613675  ...          -0.656805                0\n",
            "19594    -0.441299 -1.613675  ...          -0.656805                0\n",
            "\n",
            "[19595 rows x 46 columns]\n",
            "0.7547843837713702\n",
            "Number of mislabeled points out of a total 3919 points : 1655\n",
            "0.5776983924470528\n",
            "correct_ones 435\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=45, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48587 | Acc: 81.237\n",
            "Epoch 002: | Loss: 0.46262 | Acc: 81.465\n",
            "Epoch 003: | Loss: 0.45924 | Acc: 81.473\n",
            "Epoch 004: | Loss: 0.45252 | Acc: 81.486\n",
            "Epoch 005: | Loss: 0.45113 | Acc: 81.453\n",
            "Epoch 006: | Loss: 0.44714 | Acc: 81.441\n",
            "Epoch 007: | Loss: 0.44421 | Acc: 81.412\n",
            "Epoch 008: | Loss: 0.44061 | Acc: 81.580\n",
            "Epoch 009: | Loss: 0.43916 | Acc: 81.527\n",
            "Epoch 010: | Loss: 0.43712 | Acc: 81.653\n",
            "Epoch 011: | Loss: 0.43490 | Acc: 81.665\n",
            "Epoch 012: | Loss: 0.42981 | Acc: 81.563\n",
            "Epoch 013: | Loss: 0.42278 | Acc: 82.000\n",
            "Epoch 014: | Loss: 0.42331 | Acc: 82.147\n",
            "Epoch 015: | Loss: 0.41869 | Acc: 82.176\n",
            "Epoch 016: | Loss: 0.41580 | Acc: 82.482\n",
            "Epoch 017: | Loss: 0.41480 | Acc: 82.502\n",
            "Epoch 018: | Loss: 0.40951 | Acc: 82.780\n",
            "Epoch 019: | Loss: 0.40435 | Acc: 82.984\n",
            "Epoch 020: | Loss: 0.39931 | Acc: 83.020\n",
            "Epoch 021: | Loss: 0.39790 | Acc: 83.061\n",
            "Epoch 022: | Loss: 0.39688 | Acc: 83.233\n",
            "Epoch 023: | Loss: 0.38838 | Acc: 83.416\n",
            "Epoch 024: | Loss: 0.38732 | Acc: 83.690\n",
            "Epoch 025: | Loss: 0.38590 | Acc: 83.678\n",
            "Epoch 026: | Loss: 0.37853 | Acc: 83.690\n",
            "Epoch 027: | Loss: 0.38114 | Acc: 83.771\n",
            "Epoch 028: | Loss: 0.37606 | Acc: 84.167\n",
            "Epoch 029: | Loss: 0.37633 | Acc: 84.094\n",
            "Epoch 030: | Loss: 0.36995 | Acc: 84.420\n",
            "Epoch 031: | Loss: 0.36505 | Acc: 84.812\n",
            "Epoch 032: | Loss: 0.36752 | Acc: 84.404\n",
            "Epoch 033: | Loss: 0.35876 | Acc: 84.706\n",
            "Epoch 034: | Loss: 0.36114 | Acc: 84.747\n",
            "Epoch 035: | Loss: 0.35895 | Acc: 84.816\n",
            "Epoch 036: | Loss: 0.35861 | Acc: 84.878\n",
            "Epoch 037: | Loss: 0.35391 | Acc: 85.302\n",
            "Epoch 038: | Loss: 0.35170 | Acc: 85.327\n",
            "Epoch 039: | Loss: 0.34935 | Acc: 85.367\n",
            "Epoch 040: | Loss: 0.34680 | Acc: 85.314\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87      3187\n",
            "           1       0.28      0.12      0.17       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.55      0.53      0.52      3919\n",
            "weighted avg       0.72      0.78      0.74      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "46\n",
            "______________________________\n",
            "47 46\n",
            "       count_accts    cm_age  ...  sow_tot_trans_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.656805                1\n",
            "1         1.497981  3.995318  ...           1.032738                1\n",
            "2        -0.441299  3.909026  ...           1.032738                1\n",
            "3         3.437261  3.736442  ...          -0.656805                1\n",
            "4        -0.441299  3.736442  ...          -0.656805                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19591    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19592    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19593    -0.441299 -1.613675  ...          -0.656805                0\n",
            "19594    -0.441299 -1.613675  ...          -0.656805                0\n",
            "\n",
            "[19595 rows x 47 columns]\n",
            "0.7550395509058433\n",
            "Number of mislabeled points out of a total 3919 points : 1600\n",
            "0.5917325848430722\n",
            "correct_ones 420\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=46, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.50141 | Acc: 77.922\n",
            "Epoch 002: | Loss: 0.46113 | Acc: 81.461\n",
            "Epoch 003: | Loss: 0.45681 | Acc: 81.461\n",
            "Epoch 004: | Loss: 0.45249 | Acc: 81.433\n",
            "Epoch 005: | Loss: 0.44979 | Acc: 81.624\n",
            "Epoch 006: | Loss: 0.44805 | Acc: 81.527\n",
            "Epoch 007: | Loss: 0.44180 | Acc: 81.600\n",
            "Epoch 008: | Loss: 0.43720 | Acc: 81.751\n",
            "Epoch 009: | Loss: 0.43455 | Acc: 81.886\n",
            "Epoch 010: | Loss: 0.43126 | Acc: 81.971\n",
            "Epoch 011: | Loss: 0.42637 | Acc: 81.988\n",
            "Epoch 012: | Loss: 0.42527 | Acc: 82.131\n",
            "Epoch 013: | Loss: 0.41996 | Acc: 82.159\n",
            "Epoch 014: | Loss: 0.41987 | Acc: 82.567\n",
            "Epoch 015: | Loss: 0.41199 | Acc: 82.327\n",
            "Epoch 016: | Loss: 0.41069 | Acc: 82.698\n",
            "Epoch 017: | Loss: 0.40606 | Acc: 82.845\n",
            "Epoch 018: | Loss: 0.40231 | Acc: 83.151\n",
            "Epoch 019: | Loss: 0.39895 | Acc: 83.159\n",
            "Epoch 020: | Loss: 0.39288 | Acc: 83.502\n",
            "Epoch 021: | Loss: 0.38997 | Acc: 83.702\n",
            "Epoch 022: | Loss: 0.38608 | Acc: 83.653\n",
            "Epoch 023: | Loss: 0.38428 | Acc: 84.008\n",
            "Epoch 024: | Loss: 0.37960 | Acc: 84.261\n",
            "Epoch 025: | Loss: 0.37404 | Acc: 84.498\n",
            "Epoch 026: | Loss: 0.37524 | Acc: 84.180\n",
            "Epoch 027: | Loss: 0.36856 | Acc: 84.608\n",
            "Epoch 028: | Loss: 0.36585 | Acc: 84.629\n",
            "Epoch 029: | Loss: 0.36238 | Acc: 84.845\n",
            "Epoch 030: | Loss: 0.36024 | Acc: 84.873\n",
            "Epoch 031: | Loss: 0.35561 | Acc: 85.257\n",
            "Epoch 032: | Loss: 0.35273 | Acc: 85.445\n",
            "Epoch 033: | Loss: 0.35340 | Acc: 85.327\n",
            "Epoch 034: | Loss: 0.34987 | Acc: 85.478\n",
            "Epoch 035: | Loss: 0.34830 | Acc: 85.661\n",
            "Epoch 036: | Loss: 0.34134 | Acc: 85.669\n",
            "Epoch 037: | Loss: 0.34160 | Acc: 85.935\n",
            "Epoch 038: | Loss: 0.34168 | Acc: 85.686\n",
            "Epoch 039: | Loss: 0.33756 | Acc: 86.122\n",
            "Epoch 040: | Loss: 0.33613 | Acc: 86.302\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87      3187\n",
            "           1       0.28      0.13      0.17       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.55      0.53      0.52      3919\n",
            "weighted avg       0.72      0.78      0.74      3919\n",
            "\n",
            "1\n",
            "______________________________\n",
            "\n",
            "47\n",
            "______________________________\n",
            "48 47\n",
            "       count_accts    cm_age  ...  sow_tot_trans_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.656805                1\n",
            "1         1.497981  3.995318  ...           1.032738                1\n",
            "2        -0.441299  3.909026  ...           1.032738                1\n",
            "3         3.437261  3.736442  ...          -0.656805                1\n",
            "4        -0.441299  3.736442  ...          -0.656805                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19591    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19592    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19593    -0.441299 -1.613675  ...          -0.656805                0\n",
            "19594    -0.441299 -1.613675  ...          -0.656805                0\n",
            "\n",
            "[19595 rows x 48 columns]\n",
            "0.7547843837713702\n",
            "Number of mislabeled points out of a total 3919 points : 1569\n",
            "0.5996427660117377\n",
            "correct_ones 413\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=47, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48581 | Acc: 80.433\n",
            "Epoch 002: | Loss: 0.46295 | Acc: 81.420\n",
            "Epoch 003: | Loss: 0.45812 | Acc: 81.478\n",
            "Epoch 004: | Loss: 0.45494 | Acc: 81.453\n",
            "Epoch 005: | Loss: 0.45106 | Acc: 81.433\n",
            "Epoch 006: | Loss: 0.44925 | Acc: 81.424\n",
            "Epoch 007: | Loss: 0.44465 | Acc: 81.412\n",
            "Epoch 008: | Loss: 0.44540 | Acc: 81.416\n",
            "Epoch 009: | Loss: 0.44016 | Acc: 81.506\n",
            "Epoch 010: | Loss: 0.43902 | Acc: 81.551\n",
            "Epoch 011: | Loss: 0.43517 | Acc: 81.624\n",
            "Epoch 012: | Loss: 0.43270 | Acc: 81.604\n",
            "Epoch 013: | Loss: 0.42869 | Acc: 81.845\n",
            "Epoch 014: | Loss: 0.42313 | Acc: 81.890\n",
            "Epoch 015: | Loss: 0.41931 | Acc: 82.049\n",
            "Epoch 016: | Loss: 0.41966 | Acc: 82.253\n",
            "Epoch 017: | Loss: 0.41403 | Acc: 82.363\n",
            "Epoch 018: | Loss: 0.41075 | Acc: 82.241\n",
            "Epoch 019: | Loss: 0.40608 | Acc: 82.559\n",
            "Epoch 020: | Loss: 0.40353 | Acc: 82.869\n",
            "Epoch 021: | Loss: 0.39914 | Acc: 82.947\n",
            "Epoch 022: | Loss: 0.39605 | Acc: 82.959\n",
            "Epoch 023: | Loss: 0.39253 | Acc: 82.906\n",
            "Epoch 024: | Loss: 0.39028 | Acc: 83.318\n",
            "Epoch 025: | Loss: 0.38165 | Acc: 83.629\n",
            "Epoch 026: | Loss: 0.38285 | Acc: 83.580\n",
            "Epoch 027: | Loss: 0.38035 | Acc: 83.637\n",
            "Epoch 028: | Loss: 0.37813 | Acc: 83.714\n",
            "Epoch 029: | Loss: 0.37150 | Acc: 84.045\n",
            "Epoch 030: | Loss: 0.36991 | Acc: 84.024\n",
            "Epoch 031: | Loss: 0.36660 | Acc: 84.306\n",
            "Epoch 032: | Loss: 0.36470 | Acc: 84.298\n",
            "Epoch 033: | Loss: 0.36190 | Acc: 84.616\n",
            "Epoch 034: | Loss: 0.35862 | Acc: 84.473\n",
            "Epoch 035: | Loss: 0.35536 | Acc: 84.906\n",
            "Epoch 036: | Loss: 0.35191 | Acc: 85.024\n",
            "Epoch 037: | Loss: 0.35317 | Acc: 84.967\n",
            "Epoch 038: | Loss: 0.34871 | Acc: 85.135\n",
            "Epoch 039: | Loss: 0.34650 | Acc: 85.306\n",
            "Epoch 040: | Loss: 0.34714 | Acc: 85.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87      3187\n",
            "           1       0.29      0.12      0.17       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.55      0.53      0.52      3919\n",
            "weighted avg       0.72      0.78      0.74      3919\n",
            "\n",
            "0\n",
            "______________________________\n",
            "\n",
            "48\n",
            "______________________________\n",
            "49 48\n",
            "       count_accts    cm_age  ...  sow_tot_trans_cnt  profitable_flag\n",
            "0        -0.441299  4.081611  ...          -0.656805                1\n",
            "1         1.497981  3.995318  ...           1.032738                1\n",
            "2        -0.441299  3.909026  ...           1.032738                1\n",
            "3         3.437261  3.736442  ...          -0.656805                1\n",
            "4        -0.441299  3.736442  ...          -0.656805                1\n",
            "...            ...       ...  ...                ...              ...\n",
            "19590    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19591    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19592    -0.441299 -1.527383  ...          -0.656805                0\n",
            "19593    -0.441299 -1.613675  ...          -0.656805                0\n",
            "19594    -0.441299 -1.613675  ...          -0.656805                0\n",
            "\n",
            "[19595 rows x 49 columns]\n",
            "0.7535085480990048\n",
            "Number of mislabeled points out of a total 3919 points : 1580\n",
            "0.5968359275325338\n",
            "correct_ones 416\n",
            "0.8165348303138555\n",
            "correct_ones 0\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=48, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_4): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Loss: 0.48374 | Acc: 81.188\n",
            "Epoch 002: | Loss: 0.46216 | Acc: 81.433\n",
            "Epoch 003: | Loss: 0.45873 | Acc: 81.461\n",
            "Epoch 004: | Loss: 0.45317 | Acc: 81.420\n",
            "Epoch 005: | Loss: 0.45035 | Acc: 81.486\n",
            "Epoch 006: | Loss: 0.44721 | Acc: 81.449\n",
            "Epoch 007: | Loss: 0.44406 | Acc: 81.649\n",
            "Epoch 008: | Loss: 0.44482 | Acc: 81.482\n",
            "Epoch 009: | Loss: 0.43615 | Acc: 81.637\n",
            "Epoch 010: | Loss: 0.43492 | Acc: 81.759\n",
            "Epoch 011: | Loss: 0.42962 | Acc: 82.302\n",
            "Epoch 012: | Loss: 0.42814 | Acc: 81.886\n",
            "Epoch 013: | Loss: 0.42228 | Acc: 82.261\n",
            "Epoch 014: | Loss: 0.41994 | Acc: 82.355\n",
            "Epoch 015: | Loss: 0.41574 | Acc: 82.490\n",
            "Epoch 016: | Loss: 0.41121 | Acc: 82.567\n",
            "Epoch 017: | Loss: 0.40817 | Acc: 82.637\n",
            "Epoch 018: | Loss: 0.40574 | Acc: 82.902\n",
            "Epoch 019: | Loss: 0.39642 | Acc: 83.147\n",
            "Epoch 020: | Loss: 0.39489 | Acc: 83.188\n",
            "Epoch 021: | Loss: 0.39036 | Acc: 83.567\n",
            "Epoch 022: | Loss: 0.38609 | Acc: 83.714\n",
            "Epoch 023: | Loss: 0.37857 | Acc: 84.171\n",
            "Epoch 024: | Loss: 0.37783 | Acc: 84.245\n",
            "Epoch 025: | Loss: 0.37229 | Acc: 84.392\n",
            "Epoch 026: | Loss: 0.37266 | Acc: 84.241\n",
            "Epoch 027: | Loss: 0.37027 | Acc: 84.273\n",
            "Epoch 028: | Loss: 0.36503 | Acc: 84.633\n",
            "Epoch 029: | Loss: 0.35947 | Acc: 85.041\n",
            "Epoch 030: | Loss: 0.36017 | Acc: 84.698\n",
            "Epoch 031: | Loss: 0.36072 | Acc: 84.653\n",
            "Epoch 032: | Loss: 0.35341 | Acc: 84.996\n",
            "Epoch 033: | Loss: 0.34932 | Acc: 85.200\n",
            "Epoch 034: | Loss: 0.35005 | Acc: 85.241\n",
            "Epoch 035: | Loss: 0.34255 | Acc: 85.580\n",
            "Epoch 036: | Loss: 0.34133 | Acc: 85.539\n",
            "Epoch 037: | Loss: 0.33932 | Acc: 85.522\n",
            "Epoch 038: | Loss: 0.33553 | Acc: 85.682\n",
            "Epoch 039: | Loss: 0.33522 | Acc: 85.910\n",
            "Epoch 040: | Loss: 0.33154 | Acc: 86.020\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88      3187\n",
            "           1       0.30      0.12      0.17       732\n",
            "\n",
            "    accuracy                           0.78      3919\n",
            "   macro avg       0.56      0.53      0.52      3919\n",
            "weighted avg       0.72      0.78      0.74      3919\n",
            "\n",
            "0\n",
            "CPU times: user 43min 35s, sys: 28.8 s, total: 44min 4s\n",
            "Wall time: 44min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2N-nQ1_gsUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d4c42b-d268-43a4-d4b3-93b57dc8bfc9"
      },
      "source": [
        "f_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Gnb', 10): 0.5513151994504498,\n",
              " ('Gnb', 11): 0.5530042441815093,\n",
              " ('Gnb', 12): 0.5632895563508895,\n",
              " ('Gnb', 13): 0.5717805645666527,\n",
              " ('Gnb', 14): 0.5715326959754127,\n",
              " ('Gnb', 15): 0.5714920392327423,\n",
              " ('Gnb', 16): 0.5777217684417505,\n",
              " ('Gnb', 17): 0.5776957157836713,\n",
              " ('Gnb', 18): 0.5751185687962063,\n",
              " ('Gnb', 19): 0.5742339958013739,\n",
              " ('Gnb', 20): 0.5682936234109669,\n",
              " ('Gnb', 21): 0.5558810598291039,\n",
              " ('Gnb', 22): 0.5563881006771223,\n",
              " ('Gnb', 23): 0.5318990694526525,\n",
              " ('Gnb', 24): 0.5597972054408593,\n",
              " ('Gnb', 25): 0.5716749505238099,\n",
              " ('Gnb', 26): 0.5709018207999991,\n",
              " ('Gnb', 27): 0.5665856397848217,\n",
              " ('Gnb', 28): 0.5767191006732928,\n",
              " ('Gnb', 29): 0.5765231907226493,\n",
              " ('Gnb', 30): 0.5739826549703403,\n",
              " ('Gnb', 31): 0.5742844527921058,\n",
              " ('Gnb', 32): 0.5377386928087443,\n",
              " ('Gnb', 33): 0.5385223355280258,\n",
              " ('Gnb', 34): 0.5439117453955377,\n",
              " ('Gnb', 35): 0.4909967551314264,\n",
              " ('Gnb', 36): 0.51409847730695,\n",
              " ('Gnb', 37): 0.47079534877259277,\n",
              " ('Gnb', 38): 0.5090684048338227,\n",
              " ('Gnb', 39): 0.5081140205365914,\n",
              " ('Gnb', 40): 0.5094457489439418,\n",
              " ('Gnb', 41): 0.5094502786015651,\n",
              " ('Gnb', 42): 0.5090643103176179,\n",
              " ('Gnb', 43): 0.5191351499821577,\n",
              " ('Gnb', 44): 0.5163355321523624,\n",
              " ('Gnb', 45): 0.5165271806683774,\n",
              " ('Gnb', 46): 0.5239281093787013,\n",
              " ('Gnb', 47): 0.5283125134004066,\n",
              " ('Gnb', 48): 0.526875696465455,\n",
              " ('Linear_regression_with_thresholding=0.234', 10): 0.4589516250589241,\n",
              " ('Linear_regression_with_thresholding=0.234', 11): 0.46177326625427023,\n",
              " ('Linear_regression_with_thresholding=0.234', 12): 0.4618674362373273,\n",
              " ('Linear_regression_with_thresholding=0.234', 13): 0.46036427265139795,\n",
              " ('Linear_regression_with_thresholding=0.234', 14): 0.46036427265139795,\n",
              " ('Linear_regression_with_thresholding=0.234', 15): 0.4616791197825475,\n",
              " ('Linear_regression_with_thresholding=0.234', 16): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 17): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 18): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 19): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 20): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 21): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 22): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 23): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 24): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 25): 0.46458035130641684,\n",
              " ('Linear_regression_with_thresholding=0.234', 26): 0.46458035130641684,\n",
              " ('Linear_regression_with_thresholding=0.234', 27): 0.46458035130641684,\n",
              " ('Linear_regression_with_thresholding=0.234', 28): 0.463178620841649,\n",
              " ('Linear_regression_with_thresholding=0.234', 29): 0.463178620841649,\n",
              " ('Linear_regression_with_thresholding=0.234', 30): 0.463178620841649,\n",
              " ('Linear_regression_with_thresholding=0.234', 31): 0.463178620841649,\n",
              " ('Linear_regression_with_thresholding=0.234', 32): 0.46438490926984516,\n",
              " ('Linear_regression_with_thresholding=0.234', 33): 0.4642872373641604,\n",
              " ('Linear_regression_with_thresholding=0.234', 34): 0.4642872373641604,\n",
              " ('Linear_regression_with_thresholding=0.234', 35): 0.4670692573470647,\n",
              " ('Linear_regression_with_thresholding=0.234', 36): 0.46568003016613624,\n",
              " ('Linear_regression_with_thresholding=0.234', 37): 0.46558062398769895,\n",
              " ('Linear_regression_with_thresholding=0.234', 38): 0.46558062398769895,\n",
              " ('Linear_regression_with_thresholding=0.234', 39): 0.46558062398769895,\n",
              " ('Linear_regression_with_thresholding=0.234', 40): 0.46558062398769895,\n",
              " ('Linear_regression_with_thresholding=0.234', 41): 0.46568003016613624,\n",
              " ('Linear_regression_with_thresholding=0.234', 42): 0.4657794734717811,\n",
              " ('Linear_regression_with_thresholding=0.234', 43): 0.46438490926984516,\n",
              " ('Linear_regression_with_thresholding=0.234', 44): 0.46438490926984516,\n",
              " ('Linear_regression_with_thresholding=0.234', 45): 0.4642872373641604,\n",
              " ('Linear_regression_with_thresholding=0.234', 46): 0.4642872373641604,\n",
              " ('Linear_regression_with_thresholding=0.234', 47): 0.4642872373641604,\n",
              " ('Linear_regression_with_thresholding=0.234', 48): 0.4642872373641604,\n",
              " ('Logistic_regression_without_thresholding', 10): 0.4589516250589241,\n",
              " ('Logistic_regression_without_thresholding', 11): 0.46177326625427023,\n",
              " ('Logistic_regression_without_thresholding', 12): 0.4618674362373273,\n",
              " ('Logistic_regression_without_thresholding', 13): 0.46036427265139795,\n",
              " ('Logistic_regression_without_thresholding', 14): 0.46036427265139795,\n",
              " ('Logistic_regression_without_thresholding', 15): 0.4616791197825475,\n",
              " ('Logistic_regression_without_thresholding', 16): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 17): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 18): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 19): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 20): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 21): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 22): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 23): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 24): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 25): 0.46458035130641684,\n",
              " ('Logistic_regression_without_thresholding', 26): 0.46458035130641684,\n",
              " ('Logistic_regression_without_thresholding', 27): 0.46458035130641684,\n",
              " ('Logistic_regression_without_thresholding', 28): 0.463178620841649,\n",
              " ('Logistic_regression_without_thresholding', 29): 0.463178620841649,\n",
              " ('Logistic_regression_without_thresholding', 30): 0.463178620841649,\n",
              " ('Logistic_regression_without_thresholding', 31): 0.463178620841649,\n",
              " ('Logistic_regression_without_thresholding', 32): 0.46438490926984516,\n",
              " ('Logistic_regression_without_thresholding', 33): 0.4642872373641604,\n",
              " ('Logistic_regression_without_thresholding', 34): 0.4642872373641604,\n",
              " ('Logistic_regression_without_thresholding', 35): 0.4670692573470647,\n",
              " ('Logistic_regression_without_thresholding', 36): 0.46568003016613624,\n",
              " ('Logistic_regression_without_thresholding', 37): 0.46558062398769895,\n",
              " ('Logistic_regression_without_thresholding', 38): 0.46558062398769895,\n",
              " ('Logistic_regression_without_thresholding', 39): 0.46558062398769895,\n",
              " ('Logistic_regression_without_thresholding', 40): 0.46558062398769895,\n",
              " ('Logistic_regression_without_thresholding', 41): 0.46568003016613624,\n",
              " ('Logistic_regression_without_thresholding', 42): 0.4657794734717811,\n",
              " ('Logistic_regression_without_thresholding', 43): 0.46438490926984516,\n",
              " ('Logistic_regression_without_thresholding', 44): 0.46438490926984516,\n",
              " ('Logistic_regression_without_thresholding', 45): 0.4642872373641604,\n",
              " ('Logistic_regression_without_thresholding', 46): 0.4642872373641604,\n",
              " ('Logistic_regression_without_thresholding', 47): 0.4642872373641604,\n",
              " ('Logistic_regression_without_thresholding', 48): 0.4642872373641604,\n",
              " ('Neural Nets', 10): 0.466099160592134,\n",
              " ('Neural Nets', 11): 0.47054683695430194,\n",
              " ('Neural Nets', 12): 0.4712476328002449,\n",
              " ('Neural Nets', 13): 0.47541296422875373,\n",
              " ('Neural Nets', 14): 0.48531757372854833,\n",
              " ('Neural Nets', 15): 0.47923293496225294,\n",
              " ('Neural Nets', 16): 0.5189983683005958,\n",
              " ('Neural Nets', 17): 0.5037343278553185,\n",
              " ('Neural Nets', 18): 0.4926224045992664,\n",
              " ('Neural Nets', 19): 0.4948753751122835,\n",
              " ('Neural Nets', 20): 0.5020853732518785,\n",
              " ('Neural Nets', 21): 0.49538682896007397,\n",
              " ('Neural Nets', 22): 0.5017564242488255,\n",
              " ('Neural Nets', 23): 0.5143431769764047,\n",
              " ('Neural Nets', 24): 0.5101889546859625,\n",
              " ('Neural Nets', 25): 0.5107304855511351,\n",
              " ('Neural Nets', 26): 0.5216918348650965,\n",
              " ('Neural Nets', 27): 0.5211220994996696,\n",
              " ('Neural Nets', 28): 0.4965059724935231,\n",
              " ('Neural Nets', 29): 0.5153303279228488,\n",
              " ('Neural Nets', 30): 0.5090791506786587,\n",
              " ('Neural Nets', 31): 0.5282122898514225,\n",
              " ('Neural Nets', 32): 0.5165467521488464,\n",
              " ('Neural Nets', 33): 0.5294007156163718,\n",
              " ('Neural Nets', 34): 0.5490071738524452,\n",
              " ('Neural Nets', 35): 0.5197247733255338,\n",
              " ('Neural Nets', 36): 0.5351973964805538,\n",
              " ('Neural Nets', 37): 0.5404576394959905,\n",
              " ('Neural Nets', 38): 0.5242513370838738,\n",
              " ('Neural Nets', 39): 0.5207390256659953,\n",
              " ('Neural Nets', 40): 0.5429456691437117,\n",
              " ('Neural Nets', 41): 0.5035124734077613,\n",
              " ('Neural Nets', 42): 0.528493176306421,\n",
              " ('Neural Nets', 43): 0.5543746572500615,\n",
              " ('Neural Nets', 44): 0.5392332351162767,\n",
              " ('Neural Nets', 45): 0.5214659412822157,\n",
              " ('Neural Nets', 46): 0.52243393923262,\n",
              " ('Neural Nets', 47): 0.5201537858752081,\n",
              " ('Neural Nets', 48): 0.522290586496974,\n",
              " ('Random Forest', 10): 0.470012274086977,\n",
              " ('Random Forest', 11): 0.47595135785664816,\n",
              " ('Random Forest', 12): 0.4608875877471421,\n",
              " ('Random Forest', 13): 0.4726725368069233,\n",
              " ('Random Forest', 14): 0.47129749713042046,\n",
              " ('Random Forest', 15): 0.47935548953693713,\n",
              " ('Random Forest', 16): 0.48716775528354966,\n",
              " ('Random Forest', 17): 0.48615765778298853,\n",
              " ('Random Forest', 18): 0.49355269040865984,\n",
              " ('Random Forest', 19): 0.4900447945245957,\n",
              " ('Random Forest', 20): 0.4808057527972799,\n",
              " ('Random Forest', 21): 0.48429071604250373,\n",
              " ('Random Forest', 22): 0.4757916369498297,\n",
              " ('Random Forest', 23): 0.4783681087431775,\n",
              " ('Random Forest', 24): 0.4799428099329216,\n",
              " ('Random Forest', 25): 0.4739968917773382,\n",
              " ('Random Forest', 26): 0.4759043345600815,\n",
              " ('Random Forest', 27): 0.47534154523630345,\n",
              " ('Random Forest', 28): 0.4718353591046306,\n",
              " ('Random Forest', 29): 0.47748278921735005,\n",
              " ('Random Forest', 30): 0.47243081871889037,\n",
              " ('Random Forest', 31): 0.46941456017828187,\n",
              " ('Random Forest', 32): 0.4708682189175194,\n",
              " ('Random Forest', 33): 0.4741093963796565,\n",
              " ('Random Forest', 34): 0.4749962281606745,\n",
              " ('Random Forest', 35): 0.4636986391666514,\n",
              " ('Random Forest', 36): 0.4662011201577572,\n",
              " ('Random Forest', 37): 0.4733335893103978,\n",
              " ('Random Forest', 38): 0.4686886706628033,\n",
              " ('Random Forest', 39): 0.46941456017828187,\n",
              " ('Random Forest', 40): 0.47330452361337993,\n",
              " ('Random Forest', 41): 0.46753037154473015,\n",
              " ('Random Forest', 42): 0.459718301074545,\n",
              " ('Random Forest', 43): 0.46438490926984516,\n",
              " ('Random Forest', 44): 0.4607389060512997,\n",
              " ('Random Forest', 45): 0.47205090545750866,\n",
              " ('Random Forest', 46): 0.47100298906338467,\n",
              " ('Random Forest', 47): 0.4636044294698656,\n",
              " ('Random Forest', 48): 0.4654812546890546,\n",
              " ('SVC', 10): 0.44826129804308035,\n",
              " ('SVC', 11): 0.4484166080225193,\n",
              " ('SVC', 12): 0.4484166080225193,\n",
              " ('SVC', 13): 0.4484166080225193,\n",
              " ('SVC', 14): 0.4484166080225193,\n",
              " ('SVC', 15): 0.4511826123688367,\n",
              " ('SVC', 16): 0.44976262784432364,\n",
              " ('SVC', 17): 0.44976262784432364,\n",
              " ('SVC', 18): 0.4499216104562914,\n",
              " ('SVC', 19): 0.4499216104562914,\n",
              " ('SVC', 20): 0.4499216104562914,\n",
              " ('SVC', 21): 0.4499216104562914,\n",
              " ('SVC', 22): 0.4499216104562914,\n",
              " ('SVC', 23): 0.4499216104562914,\n",
              " ('SVC', 24): 0.4499216104562914,\n",
              " ('SVC', 25): 0.4499216104562914,\n",
              " ('SVC', 26): 0.4499216104562914,\n",
              " ('SVC', 27): 0.4499216104562914,\n",
              " ('SVC', 28): 0.4499216104562914,\n",
              " ('SVC', 29): 0.44849423022797635,\n",
              " ('SVC', 30): 0.44849423022797635,\n",
              " ('SVC', 31): 0.44849423022797635,\n",
              " ('SVC', 32): 0.44849423022797635,\n",
              " ('SVC', 33): 0.44849423022797635,\n",
              " ('SVC', 34): 0.44849423022797635,\n",
              " ('SVC', 35): 0.44849423022797635,\n",
              " ('SVC', 36): 0.44849423022797635,\n",
              " ('SVC', 37): 0.44849423022797635,\n",
              " ('SVC', 38): 0.44849423022797635,\n",
              " ('SVC', 39): 0.4484166080225193,\n",
              " ('SVC', 40): 0.4484166080225193,\n",
              " ('SVC', 41): 0.4484166080225193,\n",
              " ('SVC', 42): 0.4484166080225193,\n",
              " ('SVC', 43): 0.44849423022797635,\n",
              " ('SVC', 44): 0.44849423022797635,\n",
              " ('SVC', 45): 0.44849423022797635,\n",
              " ('SVC', 46): 0.4484166080225193,\n",
              " ('SVC', 47): 0.44849423022797635,\n",
              " ('SVC', 48): 0.44849423022797635,\n",
              " ('Zeros', 10): 0.4495013344570867,\n",
              " ('Zeros', 11): 0.4495013344570867,\n",
              " ('Zeros', 12): 0.4495013344570867,\n",
              " ('Zeros', 13): 0.4495013344570867,\n",
              " ('Zeros', 14): 0.4495013344570867,\n",
              " ('Zeros', 15): 0.4495013344570867,\n",
              " ('Zeros', 16): 0.4495013344570867,\n",
              " ('Zeros', 17): 0.4495013344570867,\n",
              " ('Zeros', 18): 0.4495013344570867,\n",
              " ('Zeros', 19): 0.4495013344570867,\n",
              " ('Zeros', 20): 0.4495013344570867,\n",
              " ('Zeros', 21): 0.4495013344570867,\n",
              " ('Zeros', 22): 0.4495013344570867,\n",
              " ('Zeros', 23): 0.4495013344570867,\n",
              " ('Zeros', 24): 0.4495013344570867,\n",
              " ('Zeros', 25): 0.4495013344570867,\n",
              " ('Zeros', 26): 0.4495013344570867,\n",
              " ('Zeros', 27): 0.4495013344570867,\n",
              " ('Zeros', 28): 0.4495013344570867,\n",
              " ('Zeros', 29): 0.4495013344570867,\n",
              " ('Zeros', 30): 0.4495013344570867,\n",
              " ('Zeros', 31): 0.4495013344570867,\n",
              " ('Zeros', 32): 0.4495013344570867,\n",
              " ('Zeros', 33): 0.4495013344570867,\n",
              " ('Zeros', 34): 0.4495013344570867,\n",
              " ('Zeros', 35): 0.4495013344570867,\n",
              " ('Zeros', 36): 0.4495013344570867,\n",
              " ('Zeros', 37): 0.4495013344570867,\n",
              " ('Zeros', 38): 0.4495013344570867,\n",
              " ('Zeros', 39): 0.4495013344570867,\n",
              " ('Zeros', 40): 0.4495013344570867,\n",
              " ('Zeros', 41): 0.4495013344570867,\n",
              " ('Zeros', 42): 0.4495013344570867,\n",
              " ('Zeros', 43): 0.4495013344570867,\n",
              " ('Zeros', 44): 0.4495013344570867,\n",
              " ('Zeros', 45): 0.4495013344570867,\n",
              " ('Zeros', 46): 0.4495013344570867,\n",
              " ('Zeros', 47): 0.4495013344570867,\n",
              " ('Zeros', 48): 0.4495013344570867}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xgszJOGfCl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}